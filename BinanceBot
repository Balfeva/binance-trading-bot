Schema of the bot:
BinanceBot/
├── binance_api/
│   ├── client.py
│   ├── utils.py
│   └── websocket.py
├── optimizations/
│   ├── vectorized_math.py
│   ├── path_generator.py
│   ├── jit_utils.py
│   ├── adaptive_rate_limiter.py
│   ├── async_executor.py
├── monitoring/
│   ├── aws_watchdog.py
│   ├── free_tier_guard.py
│   ├── security_monitor.py
│   ├── hft_performance.py
│   ├── performance.py
├── app/
│   ├── bootstrap.py
├── core/
│   ├── strategy_engine.py
│   ├── turbo_arbitrage.py
│   ├── quantum_overlays.py
│   ├── pair_manager.pyimport logging
│   ├── portfolio_manager.py
│   ├── type_checker.py
│   ├── emergency.py
│   ├── hedging.py
│   ├── light_predictor.py
│   └── self_evolution.py
├── data/
│   ├── logs/
│   │   ├── errors.log
│   │   └── trades.log
│   ├── reports/
│   │   └── weekly_profit_report.pdf
├── ml/
│   ├── model.py
│   ├── trainer.py
│   ├── rl_wrapper.py
│   ├── light_predictor.py
│   ├── rl_light.py
│   ├── rl_env.py
│   ├── rl_trainer.py
│   └── advanced_model.py
│   └── chaos_module.py
├── alerts/
│   ├── email/
│   │   └── send_email.py
│   ├── email_notifier.py
│   ├── daily_summary
│   └── monitor.py
├── config/
│   ├── config.yaml
│   ├── config_secrets.yaml
│   ├── free_tier.yaml
│   └── presets/
│       └── high_volatility.yaml
├── docker/
│   ├── Dockerfile
│   └── docker-compose.yml
├── utils/
│   ├── config_loader.py
│   └── advanced_logger.py
│   └── cache.py
│   └── smart_sleep.py
│   └── threading_utils.py
│   └── type_checker.py
│   └── fast_math.py
│   └── parallel.py
│   └── queue.py
│   └── log_uploader.py
│   └── watchdog.py
│   └── secure_storage.py
├── sklearn/
│   ├── linear_model.py
├── ws/
│   ├── ws_client.py
├── scripts/
│   ├── free_tier_optimizer.py
│   ├── schedule_manager.py
├── run.py
├── collections.py
├── requirements.txt
├── custom_types.txt
└── main.py
└── custom_types.py
└── README.md
└── run_ws.py
└── run.sh
└── monitor.sh

the code of the bot that containes 67 files that are named strating with ### [FILE]. there are 10220 lines
### [FILE] utils/queue.py 

import threading
import time
import logging
from typing import Any, Optional, Union, Dict, List
from datetime import datetime
import psutil
import logging.handlers
import weakref
import gc
from collections import deque
import hashlib
import re
import types

logger = logging.getLogger('atomic_queue')

class MalformedQueueItemException(Exception):
    """Exception for malformed queue items"""
    pass

class SecurityViolationException(Exception):
    """Exception for security policy violations"""
    pass

class QueueSecurityValidator:
    """🔒 ENHANCED: Security-focused input validation with comprehensive protection"""
    
    # Enhanced patterns for detecting potentially malicious content
    MALICIOUS_PATTERNS = [
        r'__.*__',  # Python internals
        r'\.\./',   # Path traversal
        r'[^\x20-\x7E]',  # Non-printable characters
        r'(exec|eval|compile|__import__|open|file|os\.|subprocess\.|system\.|popen)',
        r'(union|select|insert|update|delete|drop|alter|create)',
        r'(<script|javascript:|onload=|onerror=)',
        r'(base64|fromCharCode|eval\(|setTimeout\(|setInterval\()',
    ]
    
    # File extension blacklist
    DANGEROUS_EXTENSIONS = [
        '.exe', '.dll', '.so', '.sh', '.bat', '.cmd', '.py', '.php', '.js'
    ]
    
    @classmethod
    def validate_item_security(cls, item: Any) -> bool:
        """Comprehensive security validation with enhanced checks"""
        if item is None:
            return True
            
        # Check for dangerous types
        dangerous_types = (type, types.FunctionType, types.ModuleType, types.MethodType)
        if isinstance(item, dangerous_types):
            raise SecurityViolationException("Dangerous type detected in queue")
            
        # String content validation
        if isinstance(item, str):
            if len(item) > 10000:  # 10KB max string size
                raise SecurityViolationException("Oversized string in queue")
                
            # Check for dangerous file paths
            for ext in cls.DANGEROUS_EXTENSIONS:
                if ext in item.lower():
                    raise SecurityViolationException(f"Dangerous file extension detected: {ext}")
                    
            # Enhanced pattern matching
            for pattern in cls.MALICIOUS_PATTERNS:
                if re.search(pattern, item, re.IGNORECASE):
                    raise SecurityViolationException(f"Potentially malicious content detected: {pattern}")
            
            # Check for excessive repetition (potential DoS)
            if cls._detect_excessive_repetition(item):
                raise SecurityViolationException("Excessive character repetition detected")
                
        # Recursive validation for containers with depth limiting
        if isinstance(item, (list, tuple)):
            for element in item:
                cls.validate_item_security(element)
                
        if isinstance(item, dict):
            for key, value in item.items():
                cls.validate_item_security(key)
                cls.validate_item_security(value)
                
        return True

    @classmethod
    def _detect_excessive_repetition(cls, text: str, threshold: int = 100) -> bool:
        """Detect excessive character repetition (potential DoS)"""
        if len(text) < threshold * 2:
            return False
            
        for char in set(text):
            if text.count(char) > threshold:
                return True
        return False

class QueueItemValidator:
    """💀 ULTIMATE: Enhanced queue item validation and sanitization"""
    
    @staticmethod
    def validate_item(item: Any, max_size: int = 1024 * 1024) -> Any:
        """Validate and sanitize queue item with comprehensive security checks"""
        if item is None:
            raise MalformedQueueItemException("Queue item cannot be None")
            
        # Security validation first
        try:
            QueueSecurityValidator.validate_item_security(item)
        except SecurityViolationException as e:
            logger.critical(f"🚨 SECURITY VIOLATION: {e}")
            raise MalformedQueueItemException(f"Security violation: {e}")
            
        # Check for excessive size
        try:
            item_size = QueueItemValidator._estimate_size(item)
            if item_size > max_size:
                raise MalformedQueueItemException(f"Item too large: {item_size} bytes")
        except Exception as e:
            raise MalformedQueueItemException(f"Size estimation failed: {e}")
            
        # Enhanced type checking
        if isinstance(item, type) or callable(item) and not isinstance(item, type):
            raise MalformedQueueItemException("Callables and types not allowed in queue")
            
        # Check for circular references
        try:
            if isinstance(item, (dict, list)):
                QueueItemValidator._check_circular_references(item, set())
        except RecursionError:
            raise MalformedQueueItemException("Circular reference detected")
            
        return item
    
    @staticmethod
    def _estimate_size(obj: Any) -> int:
        """Estimate object size in bytes with limits"""
        MAX_DEPTH = 10
        return QueueItemValidator._estimate_size_recursive(obj, MAX_DEPTH, set())
    
    @staticmethod
    def _estimate_size_recursive(obj: Any, depth: int, visited: set) -> int:
        """Recursive size estimation with cycle prevention"""
        if depth <= 0 or id(obj) in visited:
            return 0
            
        visited.add(id(obj))
        size = 0
        
        try:
            if obj is None:
                return 0
                
            if isinstance(obj, (int, float, str, bytes)):
                return len(str(obj)) if hasattr(obj, '__len__') else 100
                
            if isinstance(obj, (list, tuple, set)):
                for item in obj:
                    size += QueueItemValidator._estimate_size_recursive(item, depth-1, visited.copy())
                return size
                
            if isinstance(obj, dict):
                for k, v in obj.items():
                    size += QueueItemValidator._estimate_size_recursive(k, depth-1, visited.copy())
                    size += QueueItemValidator._estimate_size_recursive(v, depth-1, visited.copy())
                return size
                
            # For custom objects, use rough estimation
            return 1000
            
        except Exception:
            return 1000
        
    @staticmethod
    def _check_circular_references(obj: Any, visited: set, depth: int = 0):
        """Check for circular references with depth limiting"""
        if depth > 50:
            raise MalformedQueueItemException("Maximum depth exceeded")
            
        obj_id = id(obj)
        if obj_id in visited:
            raise MalformedQueueItemException("Circular reference detected")
            
        visited.add(obj_id)
        
        try:
            if isinstance(obj, dict):
                for key, value in obj.items():
                    QueueItemValidator._check_circular_references(key, visited.copy(), depth + 1)
                    QueueItemValidator._check_circular_references(value, visited.copy(), depth + 1)
            elif isinstance(obj, (list, tuple, set)):
                for item in obj:
                    QueueItemValidator._check_circular_references(item, visited.copy(), depth + 1)
        finally:
            visited.discard(obj_id)


class AtomicQueue:
    """💀 ULTIMATE: Fuzz-hardened thread-safe queue with malformed data protection"""
    
    def __init__(self, maxsize: Optional[int] = None):
        self.items = deque()
        self.maxsize = maxsize
        self.lock = threading.RLock()
        self.not_empty = threading.Condition(self.lock)
        self.not_full = threading.Condition(self.lock)
        self._shutdown = False
        self.unfinished_tasks = 0
        self.all_tasks_done = threading.Condition(self.lock)
        
        # Fuzz protection statistics
        self._fuzz_stats = {
            'malformed_items_rejected': 0,
            'oversized_items_rejected': 0,
            'invalid_types_rejected': 0,
            'successful_puts': 0,
            'successful_gets': 0,
            'security_violations_blocked': 0
        }
        
        # Memory protection
        self._stats_lock = threading.Lock()
        self._stats = {
            'puts': 0,
            'gets': 0,
            'timeouts': 0,
            'errors': 0,
            'start_time': time.time(),
            'memory_warnings': 0
        }
        
        self._cleanup_interval = 300
        self._last_cleanup = time.time()
        self._max_items_before_cleanup = 1000
        
        # ✅ FIXED: Enhanced memory tracking with HFT optimization
        self._memory_tracker = {
            'peak_size': 0,
            'total_processed': 0,
            'last_gc_time': time.time(),
            'gc_interval': 30,  # Reduced from 60s for HFT
            'hft_mode': True,
            'max_memory_mb': 50  # Hard limit for queue memory
        }
        
        # HFT-specific optimizations
        self._hft_stats = {
            'high_frequency_puts': 0,
            'rapid_gets': 0,
            'contention_count': 0,
            'backpressure_applied': 0
        }
        
    def put(self, item: Any, block: bool = True, timeout: Optional[float] = None) -> None:
        """💀 Atomic put with comprehensive malformed data protection and HFT optimization"""
        if self._shutdown:
            raise RuntimeError("Queue is shutdown")
            
        start_time = time.time()
        contention_start = None
            
        try:
            # ✅ ENHANCED: Item validation and sanitization with better error handling
            validated_item = QueueItemValidator.validate_item(item)
            
            with self.not_full:
                # HFT: Check for rapid contention
                if self.lock._is_owned():
                    contention_start = time.time()
                    self._hft_stats['contention_count'] += 1
                
                if self.maxsize is not None and len(self.items) >= self.maxsize:
                    if not block:
                        raise Exception("Queue full")
                    if timeout is None:
                        while len(self.items) >= self.maxsize and not self._shutdown:
                            self.not_full.wait(0.5)  # Reduced from 1.0s for HFT
                    elif timeout < 0:
                        raise ValueError("'timeout' must be non-negative")
                    else:
                        endtime = time.time() + timeout
                        while len(self.items) >= self.maxsize and not self._shutdown:
                            remaining = endtime - time.time()
                            if remaining <= 0.0:
                                with self._stats_lock:
                                    self._stats['timeouts'] += 1
                                raise Exception("Queue full")
                            self.not_full.wait(max(0.05, min(0.5, remaining)))  # HFT: Faster checks
                
                if self._shutdown:
                    raise RuntimeError("Queue is shutdown")
                    
                self.items.append(validated_item)
                self.unfinished_tasks += 1
                
                with self._stats_lock:
                    self._stats['puts'] += 1
                    self._fuzz_stats['successful_puts'] += 1
                    self._memory_tracker['total_processed'] += 1
                    current_size = len(self.items)
                    self._memory_tracker['peak_size'] = max(self._memory_tracker['peak_size'], current_size)
                    
                    # HFT statistics
                    self._hft_stats['high_frequency_puts'] += 1
                    
                # ✅ FIXED: Automatic memory management with HFT optimization
                self._auto_cleanup_hft()
                    
                self.not_empty.notify_all()
                
        except MalformedQueueItemException as e:
            with self._stats_lock:
                self._fuzz_stats['malformed_items_rejected'] += 1
            logger.warning(f"🚫 Malformed queue item rejected: {e}")
            raise
        except SecurityViolationException as e:
            with self._stats_lock:
                self._fuzz_stats['security_violations_blocked'] += 1
            logger.critical(f"🚨 SECURITY VIOLATION BLOCKED: {e}")
            raise MalformedQueueItemException(f"Security violation: {e}")
        except Exception as e:
            with self._stats_lock:
                self._stats['errors'] += 1
            logger.error(f"Queue put operation failed: {e}")
            raise
        finally:
            # HFT performance monitoring
            if contention_start:
                contention_time = time.time() - contention_start
                if contention_time > 0.1:  # 100ms threshold
                    self._hft_stats['backpressure_applied'] += 1
                    logger.warning(f"🚦 Queue contention detected: {contention_time:.3f}s")

    def get(self, block: bool = True, timeout: Optional[float] = None) -> Any:
        """💀 Atomic get with data integrity verification and HFT optimization"""
        if self._shutdown and not self.items:
            return None
            
        start_time = time.time()
            
        with self.not_empty:
            if not block and not self.items:
                raise Exception("Queue empty")
                
            if timeout is None:
                while not self.items and not self._shutdown:
                    self.not_empty.wait(0.5)  # Reduced from 1.0s for HFT
            elif timeout < 0:
                raise ValueError("'timeout' must be non-negative")
            else:
                endtime = time.time() + timeout
                while not self.items and not self._shutdown:
                    remaining = endtime - time.time()
                    if remaining <= 0.0:
                        if not self.items:
                            with self._stats_lock:
                                self._stats['timeouts'] += 1
                            raise Exception("Queue empty")
                        break
                    self.not_empty.wait(max(0.05, min(0.5, remaining)))  # HFT: Faster checks
            
            if self._shutdown and not self.items:
                return None
                
            item = self.items.popleft() if self.items else None
            
            if item is not None:
                try:
                    # ✅ ENHANCED: Verify item integrity after retrieval with security checks
                    QueueItemValidator.validate_item(item)
                    with self._stats_lock:
                        self._stats['gets'] += 1
                        self._fuzz_stats['successful_gets'] += 1
                        self._hft_stats['rapid_gets'] += 1
                except (MalformedQueueItemException, SecurityViolationException) as e:
                    logger.error(f"🚨 Data corruption detected in queue: {e}")
                    # Item is already removed, cannot recover
                    with self._stats_lock:
                        self._stats['errors'] += 1
                    item = None
            
            if self.maxsize is not None and len(self.items) < self.maxsize:
                self.not_full.notify_all()
                
            return item

    def _auto_cleanup_hft(self):
        """💀 Enhanced automatic memory cleanup optimized for HFT"""
        current_time = time.time()
        
        # Force GC more frequently in HFT mode
        if current_time - self._memory_tracker['last_gc_time'] > self._memory_tracker['gc_interval']:
            if len(self.items) > 100:  # Lower threshold for HFT
                collected = gc.collect()
                if collected > 0:
                    logger.debug(f"🧹 HFT GC collected {collected} objects")
            self._memory_tracker['last_gc_time'] = current_time
        
        # Regular cleanup with HFT optimization
        if current_time - self._last_cleanup > (self._cleanup_interval / 2):  # More frequent cleanup
            self._atomic_cleanup_hft()

    def _atomic_cleanup_hft(self):
        """💀 Atomic memory cleanup with HFT optimization"""
        current_time = time.time()
        if current_time - self._last_cleanup > (self._cleanup_interval / 2):
            with self.lock:
                if len(self.items) > (self._max_items_before_cleanup // 2):  # More aggressive cleanup
                    # Remove oldest items with validation
                    excess = len(self.items) - (self._max_items_before_cleanup // 4)
                    if excess > 0:
                        removed_count = 0
                        for _ in range(excess):
                            try:
                                item = self.items.popleft()
                                # ✅ FIXED: Explicit reference cleanup optimized for HFT
                                if hasattr(item, 'clear') and callable(item.clear):
                                    try:
                                        item.clear()
                                    except:
                                        pass  # Ignore clearance errors in HFT mode
                                del item
                                removed_count += 1
                            except Exception as e:
                                logger.warning(f"Cleanup removal failed: {e}")
                                break
                                
                        if removed_count > 0:
                            logger.debug(f"🧹 HFT Queue cleanup: removed {removed_count} items")
                            
                            # Force garbage collection after cleanup
                            if removed_count > 50:
                                gc.collect()
                                
                self._last_cleanup = current_time

    def put_multiple(self, items: List[Any], block: bool = True, timeout: Optional[float] = None) -> int:
        """💀 Batch put with comprehensive validation and HFT optimization"""
        if self._shutdown:
            raise RuntimeError("Queue is shutdown")
            
        validated_items = []
        rejected_count = 0
        
        # Validate all items first with size limit for HFT
        max_batch_size = min(50, len(items))  # Limit batch size for HFT
        for item in items[:max_batch_size]:
            try:
                validated_item = QueueItemValidator.validate_item(item)
                validated_items.append(validated_item)
            except (MalformedQueueItemException, SecurityViolationException) as e:
                with self._stats_lock:
                    self._fuzz_stats['malformed_items_rejected'] += 1
                rejected_count += 1
                logger.debug(f"Batch item rejected: {e}")
                
        if not validated_items:
            return 0
            
        # Add validated items to queue with HFT optimization
        added_count = 0
        batch_timeout = timeout if timeout else 5.0  # Default batch timeout
        
        for item in validated_items:
            try:
                self.put(item, block, batch_timeout)
                added_count += 1
            except Exception as e:
                logger.warning(f"Batch put failed for item: {e}")
                break
                
        return added_count

    def get_fuzz_stats(self) -> Dict:
        """Get fuzz protection statistics with HFT metrics"""
        with self._stats_lock:
            stats = self._stats.copy()
            stats.update(self._fuzz_stats)
            stats.update(self._hft_stats)
            stats['current_size'] = len(self.items)
            stats['unfinished_tasks'] = self.unfinished_tasks
            stats['uptime_seconds'] = time.time() - stats['start_time']
            stats['shutdown'] = self._shutdown
            stats['memory_tracker'] = self._memory_tracker.copy()
            stats['hft_mode'] = True
            return stats

    def clear(self) -> int:
        """💀 Atomic queue clearance with proper cleanup and HFT optimization"""
        with self.lock:
            count = len(self.items)
            
            # Clear items with explicit deletion optimized for HFT
            while self.items:
                item = self.items.popleft()
                # ✅ FIXED: Enhanced cleanup for complex objects
                try:
                    if isinstance(item, dict):
                        item.clear()
                    elif isinstance(item, list):
                        item.clear()
                except:
                    pass  # Ignore cleanup errors in HFT mode
                del item
                
            # Force garbage collection after large clear
            if count > 50:  # Lower threshold for HFT
                gc.collect()
                
            logger.info(f"🧹 HFT Queue cleared: {count} items removed")
            return count

    # Existing methods remain with additional validation...
    def task_done(self) -> None:
        """💀 Atomic task completion tracking"""
        with self.all_tasks_done:
            if self.unfinished_tasks <= 0:
                logger.warning('task_done() called too many times')
                return
            self.unfinished_tasks -= 1
            if self.unfinished_tasks == 0:
                self.all_tasks_done.notify_all()

    def join(self, timeout: Optional[float] = None) -> bool:
        """💀 Atomic join with timeout support and HFT optimization"""
        with self.all_tasks_done:
            if timeout is None:
                while self.unfinished_tasks > 0 and not self._shutdown:
                    self.all_tasks_done.wait(0.5)  # Reduced from 1.0s for HFT
                return self.unfinished_tasks == 0
            else:
                endtime = time.time() + timeout
                while self.unfinished_tasks > 0 and not self._shutdown:
                    remaining = endtime - time.time()
                    if remaining <= 0.0:
                        return False
                    self.all_tasks_done.wait(max(0.05, min(0.5, remaining)))  # HFT optimization
                return self.unfinished_tasks == 0

    def empty(self) -> bool:
        """💀 Atomic empty check"""
        with self.lock:
            return len(self.items) == 0

    def full(self) -> bool:
        """💀 Atomic full check"""
        with self.lock:
            if self.maxsize is None:
                return False
            return len(self.items) >= self.maxsize

    def qsize(self) -> int:
        """💀 Atomic size check"""
        with self.lock:
            return len(self.items)

    def shutdown(self) -> None:
        """💀 Atomic shutdown with notification and HFT optimization"""
        with self.lock:
            self._shutdown = True
            with self.not_empty:
                self.not_empty.notify_all()
            with self.not_full:
                self.not_full.notify_all()
            with self.all_tasks_done:
                self.all_tasks_done.notify_all()

    def __len__(self) -> int:
        return self.qsize()

    def __bool__(self) -> bool:
        return not self.empty()

    def __del__(self):
        """Destructor with proper cleanup optimized for HFT"""
        if not self._shutdown:
            self.shutdown()
        # ✅ FIXED: Final memory cleanup with HFT optimization
        try:
            self.clear()
            gc.collect()
        except:
            pass  # Ignore cleanup errors during destruction

    def get_hft_metrics(self) -> Dict:
        """Get HFT-specific performance metrics"""
        with self._stats_lock:
            return {
                'contention_rate': self._hft_stats['contention_count'] / max(1, self._stats['puts']),
                'backpressure_events': self._hft_stats['backpressure_applied'],
                'throughput_puts_per_sec': self._hft_stats['high_frequency_puts'] / max(1, (time.time() - self._stats['start_time'])),
                'average_queue_size': self._memory_tracker['total_processed'] / max(1, self._stats['puts']),
                'memory_efficiency': (self._memory_tracker['peak_size'] * 100) / max(1, self.maxsize) if self.maxsize else 0
            }

# Backward compatibility
Queue = AtomicQueue

### [FILE] run_ws.py
import asyncio
import websockets
import yaml
import json
from custom_types import List

async def handle_message(message: str, symbols: List[str]) -> None:
    try:
        data = json.loads(message)
        stream = data.get('stream', '')
        if any(symbol.lower() in stream for symbol in symbols):
            print(f"Received update: {data['data']}")
    except json.JSONDecodeError as e:
        print(f"Failed to parse message: {e}")

async def listen(symbols: List[str], max_retries: int = 10) -> None:
    retry_count = 0
    stream_names = [f"{symbol.lower()}@trade" for symbol in symbols]
    
    while retry_count < max_retries:
        try:
            url = f"wss://testnet.binance.vision/stream?streams={'/'.join(stream_names)}"
            async with websockets.connect(url, ping_interval=30) as websocket:
                print(f"[WS] Connected to {url}")
                retry_count = 0  # Reset on successful connection
                
                while True:
                    message = await websocket.recv()
                    await handle_message(message, symbols)
                    
        except Exception as e:
            retry_count += 1
            wait_time = min(5 * retry_count, 30)  # Exponential backoff with max 30s
            print(f"[WS] Error (attempt {retry_count}/{max_retries}): {e}. Reconnecting in {wait_time}s...")
            await asyncio.sleep(wait_time)

async def main():
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)
    await listen(config["symbols"])

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("WebSocket client shutdown")


### [FILE] run.py

import time
import logging
import logging.handlers
import os
import asyncio
import signal
import sys
import numpy as np
from utils.config_loader import load_config
from ml.chaos_module import LightChaos, MalformedMarketDataException
from binance_api.client import BinanceClient, MalformedDataException
from utils.advanced_logger import TradingLogger
from scripts.schedule_manager import get_scheduler
import threading
import gc
import psutil
from contextlib import AsyncExitStack
import resource
from decimal import Decimal, InvalidOperation
import random
import math
from datetime import datetime
import socket
import traceback
from typing import Dict, List, Optional
import weakref
from concurrent.futures import ThreadPoolExecutor
import re

logger = logging.getLogger('quantum_bot')

class FuzzProtectedPriceManipulationDetector:
    """✅ ENHANCED: Price manipulation detection with fuzz protection"""
    
    def __init__(self):
        self.price_history = {}
        self.anomaly_threshold = 0.05
        self._lock = asyncio.Lock()
        self._cleanup_interval = 3600
        self._last_cleanup = time.time()
        self._fuzz_stats = {'anomalies_detected': 0, 'malformed_prices_rejected': 0}
        
    async def detect(self, symbol: str, price: float) -> bool:
        async with self._lock:
            try:
                # Validate price before processing
                if not self._is_valid_price(price):
                    self._fuzz_stats['malformed_prices_rejected'] += 1
                    return False
                    
                if symbol not in self.price_history:
                    self.price_history[symbol] = []
                    
                prices = self.price_history[symbol]
                prices.append(price)
                
                # Keep only recent prices
                if len(prices) > 100:
                    prices.pop(0)
                    
                # Detect sudden spikes/drops
                if len(prices) >= 10:
                    recent_avg = np.mean(prices[-10:])
                    if recent_avg > 0 and abs(price - recent_avg) / recent_avg > self.anomaly_threshold:
                        self._fuzz_stats['anomalies_detected'] += 1
                        return True
                        
                return False
                
            except Exception as e:
                logger.error(f"Price manipulation detection failed: {e}")
                return False
                
    def _is_valid_price(self, price: float) -> bool:
        """Validate price for fuzz protection"""
        if not isinstance(price, (int, float)):
            return False
            
        if not math.isfinite(price):
            return False
            
        if price <= 0:
            return False
            
        if price > 1e12:  # $1 trillion limit
            return False
            
        return True

    async def _cleanup_old_data(self):
        """Cleanup old price data to prevent memory leaks"""
        current_time = time.time()
        if current_time - self._last_cleanup > self._cleanup_interval:
            symbols_to_remove = []
            for symbol, prices in self.price_history.items():
                if len(prices) == 0:
                    symbols_to_remove.append(symbol)
                    
            for symbol in symbols_to_remove:
                del self.price_history[symbol]
            self._last_cleanup = current_time

class FuzzHardenedCatastropheRecovery:
    """💀 ULTIMATE: Catastrophe recovery with fuzz-resistant monitoring"""
    
    def __init__(self, bot_instance):
        self.bot = weakref.proxy(bot_instance)
        self.catastrophe_level = 0
        self.last_recovery_time = 0
        self.recovery_attempts = 0
        self._recovery_lock = asyncio.Lock()
        self._fuzz_stats = {'system_checks': 0, 'malformed_metrics_rejected': 0}
        
    async def detect_catastrophe(self) -> int:
        """Atomic catastrophe detection with fuzz protection"""
        async with self._recovery_lock:
            self._fuzz_stats['system_checks'] += 1
            return await self._detect_catastrophe_internal()
                
    async def _detect_catastrophe_internal(self) -> int:
        """Internal catastrophe detection with malformed data protection"""
        current_time = time.time()
        new_level = 0
        
        try:
            # Memory exhaustion check with validation
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                memory_mb = memory_info.rss // 1024 // 1024
                
                if not isinstance(memory_mb, int) or memory_mb < 0:
                    self._fuzz_stats['malformed_metrics_rejected'] += 1
                    logger.warning("Invalid memory metrics detected")
                elif memory_mb > 800:
                    new_level = max(new_level, 3)
                    logger.warning(f"💾 Memory critical: {memory_mb}MB")
                    
            except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                logger.error(f"Memory check failed: {e}")
                new_level = max(new_level, 3)
            
            # CPU overload check with validation
            try:
                cpu_percent = psutil.cpu_percent(interval=0.5)
                if not isinstance(cpu_percent, (int, float)) or cpu_percent < 0:
                    self._fuzz_stats['malformed_metrics_rejected'] += 1
                elif cpu_percent > 90:
                    new_level = max(new_level, 2)
                    logger.warning(f"🔥 CPU critical: {cpu_percent}%")
            except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                logger.error(f"CPU check failed: {e}")
                new_level = max(new_level, 2)
            
            # Disk space check with validation
            try:
                disk_usage = psutil.disk_usage('/')
                if not isinstance(disk_usage.percent, (int, float)) or disk_usage.percent < 0:
                    self._fuzz_stats['malformed_metrics_rejected'] += 1
                elif disk_usage.percent > 95:
                    new_level = max(new_level, 4)
                    logger.critical(f"💽 Disk critical: {disk_usage.percent}%")
            except (OSError, FileNotFoundError) as e:
                logger.error(f"Disk check failed: {e}")
                new_level = max(new_level, 4)
            
            # Network connectivity check with timeout and validation
            try:
                loop = asyncio.get_event_loop()
                network_ok = await asyncio.wait_for(
                    loop.run_in_executor(None, self._check_network), 
                    timeout=10.0
                )
                if not network_ok:
                    new_level = max(new_level, 1)
                    logger.warning("🌐 Network connectivity issues")
            except (asyncio.TimeoutError, socket.timeout, socket.gaierror, OSError):
                new_level = max(new_level, 1)
                logger.warning("🌐 Network connectivity issues")
            
            # Clock skew check with Binance client
            if hasattr(self.bot, 'client') and self.bot.client:
                clock_status = self.bot.client.get_clock_sync_status()
                if clock_status.get('clock_sync_failed', False):
                    new_level = max(new_level, 1)
                    logger.warning("🕒 Clock synchronization failed")
                elif abs(clock_status.get('server_time_offset_ms', 0)) > 10000:  # 10 seconds
                    new_level = max(new_level, 2)
                    logger.warning(f"🕒 Large clock skew: {clock_status['server_time_offset_ms']}ms")
            
            # Update catastrophe level with hysteresis
            if new_level > self.catastrophe_level:
                self.catastrophe_level = new_level
                logger.critical(f"💀 Catastrophe level elevated to {new_level}")
            elif new_level == 0 and self.catastrophe_level > 0:
                if current_time - self.last_recovery_time > 120:
                    self.catastrophe_level = 0
                    logger.info("✅ Catastrophe level normalized")
            
            return self.catastrophe_level
            
        except Exception as e:
            logger.critical(f"💀 Catastrophe detection failed: {e}")
            return 5

    def _check_network(self):
        """Blocking network check for executor"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(5)
                sock.connect(("8.8.8.8", 53))
                return True
        except:
            return False

    async def emergency_cleanup(self):
        """Emergency memory cleanup"""
        try:
            # Force garbage collection
            gc.collect()
            logger.info("🧹 Emergency memory cleanup completed")
        except Exception as e:
            logger.error(f"Emergency cleanup failed: {e}")

class AtomicTradingState:
    """✅ FIXED: Enhanced atomic state management with version control and memory optimization"""
    
    def __init__(self):
        self._state = {
            'trading_enabled': False,
            'symbols_active': {},
            'orders_pending': {},
            'last_update': time.time()
        }
        self._lock = asyncio.Lock()
        self._version = 0
        self._state_history = []
        self._max_history = 50
        self._pending_operations = 0
        
    async def update_state(self, key: str, value, expected_version: int = None) -> bool:
        """Atomic state update with optimistic concurrency control"""
        async with self._lock:
            # Optimistic concurrency check
            if expected_version is not None and expected_version != self._version:
                return False
                
            self._state[key] = value
            self._state['last_update'] = time.time()
            self._version += 1
            
            # Store minimal history for debugging
            self._state_history.append({
                'timestamp': time.time(),
                'key': key,
                'value': value,
                'version': self._version
            })
            
            # Strict history size enforcement to prevent memory leaks
            if len(self._state_history) > self._max_history:
                self._state_history = self._state_history[-self._max_history:]
                
            return True

    async def get_state_with_version(self, key: str = None):
        """Get state and current version for optimistic updates"""
        async with self._lock:
            if key:
                return self._state.get(key), self._version
            return self._state.copy(), self._version

    async def cleanup(self):
        """Aggressive memory cleanup"""
        async with self._lock:
            self._state_history.clear()
            gc.collect()

def setup_logging():
    """✅ FIXED: Enhanced logging setup with memory monitoring and rotation"""
    log_format = '%(asctime)s | %(levelname)-8s | [MEM:%(memory)dMB|CPU:%(cpu).1f%%] | %(message)s'
    
    class SystemFilter(logging.Filter):
        def filter(self, record):
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                record.memory = memory_info.rss // 1024 // 1024
                record.cpu = psutil.cpu_percent(interval=0.1)
            except (psutil.NoSuchProcess, AttributeError):
                record.memory = 0
                record.cpu = 0.0
            return True
    
    os.makedirs('logs', exist_ok=True)
    
    # Configure root logger
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # Remove existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
        handler.close()
    
    # File handler with robust rotation and size limits
    file_handler = logging.handlers.RotatingFileHandler(
        'logs/live_trading.log',
        maxBytes=50*1024*1024,  # 50MB
        backupCount=5,  # ✅ FIXED: Reduced from 10 to 5 for free tier
        encoding='utf-8'
    )
    file_handler.setFormatter(logging.Formatter(log_format))
    file_handler.addFilter(SystemFilter())
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter(log_format))
    console_handler.addFilter(SystemFilter())
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    # Critical startup message
    logging.critical("🚀 FUZZ-HARDENED QUANTUM TRADING BOT INITIALIZED")

class QuantumTradingBot:
    """💀 ULTIMATE: Fuzz-hardened main trading bot"""
    
    def __init__(self):
        self.running = False
        self.client = None
        self.chaos = None
        self.config = None
        self.symbols = []
        self.logger = logging.getLogger('quantum_bot')
        self.last_activity = time.time()
        self.scheduler = get_scheduler()
        self._exit_stack = AsyncExitStack()
        self._tasks = set()
        self._monitor_tasks = set()
        self._circuit_breaker = False
        
        # ✅ ENHANCED: Fuzz-hardened components
        self._catastrophe_recovery = None
        self._price_manipulation_detector = FuzzProtectedPriceManipulationDetector()
        self._state_manager = AtomicTradingState()
        self._analysis_cache = {}
        self._error_count = 0
        self._last_successful_trade = 0
        self._shutdown_event = asyncio.Event()
        self._cycle_semaphore = asyncio.Semaphore(5)
        
        # Fuzz protection statistics
        self._fuzz_stats = {
            'malformed_market_data_rejected': 0,
            'invalid_symbols_skipped': 0,
            'corrupted_klines_handled': 0,
            'successful_analysis_cycles': 0,
            'clock_skew_issues_detected': 0  # ✅ ADDED: Track clock skew issues
        }
        
        self._memory_check_interval = 60
        self._last_memory_check = time.time()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="quantum_bot")
        self._cache_cleanup_interval = 300
        self._last_cache_cleanup = time.time()
        
    async def initialize(self):
        """💀 Enhanced initialization with fuzz protection"""
        try:
            self.logger.info("🚀 Initializing Fuzz-Hardened Quantum Trading Bot...")
            
            # Set resource limits
            await self._set_resource_limits()
            
            # Load configuration with validation
            self.config = await self._load_config_securely()
            if not self.config:
                raise Exception("Failed to load configuration")
                
            # Validate and limit symbols
            self.symbols = self._validate_symbols(self.config.get('symbols', []))
            
            # Initialize components with error handling
            self.client = await self._initialize_client_securely()
            self.chaos = await self._initialize_chaos_securely()
            
            # Initialize catastrophe recovery
            self._catastrophe_recovery = FuzzHardenedCatastropheRecovery(self)
            
            # Start monitoring
            await self._start_secure_monitoring()
                
            self.logger.info("✅ Bot initialized with fuzz-level resilience")
            return True
            
        except Exception as e:
            self.logger.error(f"❌ Secure initialization failed: {e}")
            traceback.print_exc()
            await self._emergency_cleanup()
            return False
            
    async def _set_resource_limits(self):
        """Set system resource limits"""
        try:
            resource.setrlimit(resource.RLIMIT_NOFILE, (65536, 65536))
        except (ValueError, resource.error) as e:
            self.logger.warning(f"Could not set resource limits: {e}")
            
    def _validate_symbols(self, symbols: List[str]) -> List[str]:
        """Validate and sanitize trading symbols"""
        validated = []
        for symbol in symbols:
            try:
                # Basic symbol validation
                if not isinstance(symbol, str):
                    self._fuzz_stats['invalid_symbols_skipped'] += 1
                    continue
                    
                if len(symbol) > 20:
                    self._fuzz_stats['invalid_symbols_skipped'] += 1
                    continue
                    
                # Check symbol format (adjust based on exchange)
                if not re.match(r'^[A-Z0-9]{1,20}$', symbol):
                    self._fuzz_stats['invalid_symbols_skipped'] += 1
                    continue
                    
                validated.append(symbol)
                
            except Exception as e:
                self.logger.warning(f"Symbol validation failed for {symbol}: {e}")
                continue
                
        # Limit symbols for free tier
        return validated[:6]
        
    async def _load_config_securely(self) -> Dict:
        """Load config with fuzz protection"""
        for attempt in range(5):
            try:
                config = load_config()
                if self._validate_config_structure(config):
                    return config
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                self.logger.error(f"Config load attempt {attempt + 1} failed: {e}")
                if attempt == 4:
                    return self._get_secure_fallback_config()
        return self._get_secure_fallback_config()
        
    def _validate_config_structure(self, config: Dict) -> bool:
        """Validate config structure to prevent injection"""
        try:
            if not isinstance(config, dict):
                return False
                
            essential_keys = ['binance', 'trading', 'performance', 'risk_management']
            if not all(key in config for key in essential_keys):
                return False
                
            # Validate nested structures
            trading = config['trading']
            if not isinstance(trading, dict):
                return False
                
            # Validate numeric values
            if (trading.get('min_order_size', 0) <= 0 or 
                trading.get('max_order_size', 0) <= trading.get('min_order_size', 1)):
                return False
                
            symbols = config.get('symbols', [])
            if not isinstance(symbols, list):
                return False
                
            if len(symbols) > 10:  # Enforce symbol limit
                return False
                
            return True
        except:
            return False
            
    def _get_secure_fallback_config(self) -> Dict:
        """Fallback config for secure conditions"""
        return {
            'binance': {'testnet': True, 'api_key': 'test', 'api_secret': 'test'},
            'trading': {'dry_run': True, 'min_order_size': 10, 'max_order_size': 100},
            'performance': {'polling_interval': 10.0, 'max_active_pairs': 2},
            'risk_management': {'max_risk_per_trade': 0.1},
            'symbols': ['BTCUSDC', 'ETHUSDC']  # Minimal symbol set
        }

    async def _initialize_client_securely(self):
        """Initialize Binance client with secure error handling"""
        for attempt in range(3):
            try:
                api_key = self.config['binance'].get('api_key', 'test')
                api_secret = self.config['binance'].get('api_secret', 'test')
                testnet = self.config['binance'].get('testnet', True)
                
                client = BinanceClient(api_key, api_secret, testnet)
                
                # ✅ ENHANCED: Test connection with clock sync check
                if await client.test_connection():
                    # Check clock sync status
                    clock_status = client.get_clock_sync_status()
                    if clock_status.get('clock_sync_failed', False):
                        self.logger.warning("⚠️ Clock synchronization failed during initialization")
                        self._fuzz_stats['clock_skew_issues_detected'] += 1
                    elif abs(clock_status.get('server_time_offset_ms', 0)) > 5000:
                        self.logger.warning(f"⚠️ Large clock skew detected: {clock_status['server_time_offset_ms']}ms")
                        self._fuzz_stats['clock_skew_issues_detected'] += 1
                    
                    self.logger.info("✅ Binance client initialized successfully")
                    return client
                else:
                    raise Exception("Connection test failed")
                    
            except Exception as e:
                self.logger.error(f"Client initialization attempt {attempt + 1} failed: {e}")
                if attempt == 2:
                    raise
                await asyncio.sleep(2 ** attempt)
                
    async def _initialize_chaos_securely(self):
        """Initialize chaos module with secure error handling"""
        try:
            chaos = LightChaos(self.config)
            self.logger.info("✅ Chaos module initialized successfully")
            return chaos
        except Exception as e:
            self.logger.error(f"Chaos module initialization failed: {e}")
            return LightChaos({})
            
    async def _start_secure_monitoring(self):
        """Start monitoring tasks with secure task management"""
        try:
            monitor_task = asyncio.create_task(self._secure_monitor_loop())
            self._monitor_tasks.add(monitor_task)
            monitor_task.add_done_callback(lambda t: self._monitor_tasks.discard(t))
            self.logger.info("✅ Secure monitoring started")
        except Exception as e:
            self.logger.error(f"Monitoring start failed: {e}")
            
    async def _secure_monitor_loop(self):
        """✅ ENHANCED: Secure monitoring loop with fuzz protection and clock skew monitoring"""
        while self.running and not self._shutdown_event.is_set():
            try:
                # Memory management check
                current_time = time.time()
                if current_time - self._last_memory_check > self._memory_check_interval:
                    await self._check_memory_usage()
                    self._last_memory_check = current_time
                
                # Regular cache cleanup
                if current_time - self._last_cache_cleanup > self._cache_cleanup_interval:
                    await self._cleanup_caches()
                    self._last_cache_cleanup = current_time
                
                # Check catastrophe level
                if self._catastrophe_recovery:
                    catastrophe_level = await self._catastrophe_recovery.detect_catastrophe()
                    
                    if catastrophe_level >= 3:
                        self.logger.critical(f"💀 CATASTROPHE LEVEL {catastrophe_level}")
                        await self._emergency_recovery(catastrophe_level)
                
                # ✅ ADDED: Clock skew monitoring
                if self.client and hasattr(self.client, 'get_clock_sync_status'):
                    clock_status = self.client.get_clock_sync_status()
                    if clock_status.get('clock_sync_failed', False):
                        self.logger.warning("🕒 Clock synchronization is failing")
                        self._fuzz_stats['clock_skew_issues_detected'] += 1
                    elif abs(clock_status.get('server_time_offset_ms', 0)) > 10000:  # 10 seconds
                        self.logger.warning(f"🕒 Large clock skew: {clock_status['server_time_offset_ms']}ms")
                        self._fuzz_stats['clock_skew_issues_detected'] += 1
                
                await asyncio.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Monitoring loop error: {e}")
                await asyncio.sleep(60)
                
    async def _cleanup_caches(self):
        """Cleanup various caches to prevent memory leaks"""
        try:
            # Clear analysis cache
            cache_size_before = len(self._analysis_cache)
            self._analysis_cache.clear()
            
            # Cleanup state manager
            if self._state_manager:
                await self._state_manager.cleanup()
                
            # Cleanup chaos module if it exists
            if self.chaos and hasattr(self.chaos, 'clear_cache'):
                self.chaos.clear_cache()
                
            # Cleanup price manipulation detector
            if self._price_manipulation_detector:
                await self._price_manipulation_detector._cleanup_old_data()
                
            self.logger.info(f"🧹 Cache cleanup completed - freed {cache_size_before} cache entries")
            
        except Exception as e:
            self.logger.error(f"Cache cleanup failed: {e}")
                
    async def _check_memory_usage(self):
        """Check and manage memory usage"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
            
            if memory_mb > 700:
                self.logger.warning(f"🧹 High memory usage: {memory_mb}MB, cleaning up...")
                await self._perform_memory_cleanup()
                
        except Exception as e:
            self.logger.error(f"Memory check failed: {e}")
            
    async def _perform_memory_cleanup(self):
        """Perform memory cleanup"""
        try:
            # Clear caches
            await self._cleanup_caches()
            
            # Force garbage collection
            for _ in range(3):
                gc.collect()
                
            # Cleanup catastrophe recovery
            if self._catastrophe_recovery:
                await self._catastrophe_recovery.emergency_cleanup()
                
            self.logger.info("✅ Memory cleanup completed")
            
        except Exception as e:
            self.logger.error(f"Memory cleanup failed: {e}")
                
    async def _emergency_recovery(self, level: int):
        """Emergency recovery procedures"""
        if level >= 4:
            self._circuit_breaker = True
            self.logger.critical("🔌 Circuit breaker activated - pausing trading")
            await asyncio.sleep(60)
            self._circuit_breaker = False
            self._error_count = 0
            
        if level >= 5:
            self.logger.critical("💀 CATASTROPHIC FAILURE - Initiating restart")
            await self.shutdown()

    async def _run_trading_cycle(self, cycle_count: int):
        """✅ ENHANCED: Trading cycle with comprehensive fuzz protection and clock skew awareness"""
        try:
            self.last_activity = time.time()
            
            # Check if we should run
            if (self._circuit_breaker or 
                (self._catastrophe_recovery and 
                 await self._catastrophe_recovery.detect_catastrophe() > 2)):
                self.logger.warning("🔌 Circuit breaker active - skipping cycle")
                return
                
            # Update trading state
            await self._state_manager.update_state('trading_enabled', True)
            
            # ✅ ADDED: Check clock skew before trading
            if self.client and hasattr(self.client, 'get_clock_sync_status'):
                clock_status = self.client.get_clock_sync_status()
                if clock_status.get('clock_sync_failed', False):
                    self.logger.warning("⏰ Clock sync failed - using extended safety measures")
                elif abs(clock_status.get('server_time_offset_ms', 0)) > 30000:  # 30 seconds
                    self.logger.error("🕒 Critical clock skew - pausing trading")
                    return
            
            # Simulate trading analysis with symbol limits and validation
            for symbol in self.symbols[:3]:  # Strict symbol limit
                try:
                    # Get market data with error handling
                    klines = await self._get_klines_securely(symbol)
                    if not klines:
                        continue
                        
                    # Extract and validate prices
                    prices = self._extract_prices_securely(klines)
                    if not prices:
                        self._fuzz_stats['corrupted_klines_handled'] += 1
                        continue
                        
                    # Analyze market with protected chaos module
                    market_state = await self._analyze_market_securely(symbol, prices)
                    
                    # Log analysis
                    self.logger.info(f"🔍 {symbol} | State: {market_state} | Cycle: {cycle_count}")
                    
                    # Simulate trade decision with validation
                    if "BULLISH" in market_state and random.random() > 0.7:
                        await self._execute_protected_trade(symbol, "BUY", prices[-1])
                        
                    self._fuzz_stats['successful_analysis_cycles'] += 1
                        
                except MalformedMarketDataException as e:
                    self._fuzz_stats['malformed_market_data_rejected'] += 1
                    logger.warning(f"Malformed market data for {symbol}: {e}")
                except Exception as e:
                    self.logger.error(f"Symbol {symbol} analysis failed: {e}")
                    self._error_count += 1
                    
        except Exception as e:
            self.logger.error(f"Trading cycle {cycle_count} failed: {e}")
            self._error_count += 1
            
    async def _get_klines_securely(self, symbol: str) -> Optional[List]:
        """Get klines data with comprehensive error handling"""
        try:
            klines = await self.client.get_klines(symbol, "1m", 20)
            
            # Validate klines structure
            if not isinstance(klines, list):
                return None
                
            if len(klines) == 0:
                return None
                
            # Check for reasonable kline count
            if len(klines) > 1000:
                logger.warning(f"Excessive klines for {symbol}, truncating")
                klines = klines[:100]
                
            return klines
            
        except MalformedDataException as e:
            self._fuzz_stats['malformed_market_data_rejected'] += 1
            logger.warning(f"Malformed klines data for {symbol}: {e}")
            return None
        except Exception as e:
            logger.error(f"Klines retrieval failed for {symbol}: {e}")
            return None
            
    def _extract_prices_securely(self, klines: List) -> List[float]:
        """Extract prices from klines with validation"""
        prices = []
        for i, kline in enumerate(klines):
            try:
                if not isinstance(kline, list) or len(kline) < 5:
                    continue
                    
                # Extract closing price (index 4)
                price_str = kline[4]
                if not isinstance(price_str, (str, int, float)):
                    continue
                    
                price_float = float(price_str)
                
                # Validate price
                if (math.isfinite(price_float) and 
                    price_float > 0 and 
                    price_float < 1e12):  # Reasonable price limit
                    prices.append(price_float)
                    
            except (ValueError, TypeError, IndexError) as e:
                logger.debug(f"Price extraction failed at index {i}: {e}")
                continue
                
        return prices
        
    async def _analyze_market_securely(self, symbol: str, prices: List[float]) -> str:
        """Analyze market with protected chaos module"""
        try:
            market_state = self.chaos.update(prices)
            
            # Validate market state response
            if not isinstance(market_state, str):
                return "ANALYSIS_ERROR"
                
            if len(market_state) > 100:
                return "STATE_TOO_LONG"
                
            return market_state
            
        except MalformedMarketDataException as e:
            self._fuzz_stats['malformed_market_data_rejected'] += 1
            logger.warning(f"Chaos analysis rejected data for {symbol}: {e}")
            return "MALFORMED_DATA"
        except Exception as e:
            logger.error(f"Market analysis failed for {symbol}: {e}")
            return "ANALYSIS_ERROR"
            
    async def _execute_protected_trade(self, symbol: str, side: str, price: float):
        """Execute trade with comprehensive validation and clock skew awareness"""
        try:
            # Validate inputs
            if not self._is_valid_symbol(symbol):
                raise MalformedDataException(f"Invalid symbol: {symbol}")
                
            if side not in ['BUY', 'SELL']:
                raise MalformedDataException(f"Invalid side: {side}")
                
            if not self._is_valid_price(price):
                raise MalformedDataException(f"Invalid price: {price}")
                
            # ✅ ADDED: Check clock skew before executing trade
            if self.client and hasattr(self.client, 'get_clock_sync_status'):
                clock_status = self.client.get_clock_sync_status()
                if clock_status.get('clock_sync_failed', False):
                    self.logger.warning("⏰ Clock sync failed - using extended recvWindow")
                elif abs(clock_status.get('server_time_offset_ms', 0)) > 15000:  # 15 seconds
                    self.logger.warning(f"🕒 Significant clock skew: {clock_status['server_time_offset_ms']}ms")
            
            # Execute simulated trade
            if self.config['trading'].get('dry_run', True):
                self.logger.info(f"📄 SIMULATED {side} {symbol} @ ${price:.2f}")
                
                trade_data = {
                    'symbol': symbol,
                    'side': side,
                    'price': price,
                    'quantity': 0.001,
                    'profit': random.uniform(-10, 20),
                    'order_id': f"sim_{int(time.time())}"
                }
                
                # Use TradingLogger if available
                if hasattr(self, 'logger') and hasattr(self.logger, 'log_trade'):
                    self.logger.log_trade(trade_data)
                else:
                    self.logger.info(f"💹 TRADE: {trade_data}")
                    
            else:
                self.logger.warning(f"🚀 LIVE {side} {symbol} @ ${price:.2f}")
                
        except MalformedDataException as e:
            logger.warning(f"Trade execution blocked: {e}")
        except Exception as e:
            logger.error(f"Trade execution failed: {e}")
            
    def _is_valid_symbol(self, symbol: str) -> bool:
        """Validate symbol format"""
        if not isinstance(symbol, str):
            return False
        if len(symbol) > 20:
            return False
        return re.match(r'^[A-Z0-9]{1,20}$', symbol) is not None
            
    def _is_valid_price(self, price: float) -> bool:
        """Validate price for trading"""
        if not isinstance(price, (int, float)):
            return False
            
        if not math.isfinite(price):
            return False
            
        if price <= 0:
            return False
            
        if price > 1e9:  # $1 billion limit
            return False
            
        return True

    async def run(self):
        """✅ ENHANCED: Main trading loop with fuzz protection and clock skew resilience"""
        self.running = True
        cycle_count = 0
        
        self.logger.info("🚀 Starting fuzz-hardened trading bot")
        
        # Log fuzz protection status
        self.logger.info("🛡️  Fuzz protection: ACTIVE")
        self.logger.info(f"🔍 Monitoring {len(self.symbols)} validated symbols")
        
        while self.running and not self._shutdown_event.is_set():
            try:
                cycle_count += 1
                
                # Rate limiting with semaphore
                async with self._cycle_semaphore:
                    await self._run_trading_cycle(cycle_count)
                
                # Memory management
                if cycle_count % 10 == 0:
                    gc.collect()
                    
                # Log fuzz stats periodically
                if cycle_count % 50 == 0:
                    self._log_fuzz_stats()
                    
                # Adaptive sleep
                sleep_time = self._calculate_adaptive_sleep(cycle_count)
                await asyncio.sleep(sleep_time)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self._error_count += 1
                self.logger.error(f"Trading cycle {cycle_count} failed: {e}")
                
                # Circuit breaker with exponential backoff
                if self._error_count > 5:
                    backoff_time = min(300, 2 ** self._error_count)
                    self.logger.critical(f"🔌 Circuit breaker: backing off for {backoff_time}s")
                    await asyncio.sleep(backoff_time)
                    
                if self._error_count > 20:
                    self.logger.critical("💀 Too many errors - initiating shutdown")
                    await self.shutdown()
                    break
                    
    def _calculate_adaptive_sleep(self, cycle_count: int) -> float:
        """Calculate sleep time based on system conditions"""
        base_interval = self.config.get('performance', {}).get('polling_interval', 5.0)
        
        try:
            # Adjust based on CPU load
            cpu_percent = psutil.cpu_percent(interval=0.1)
            if cpu_percent > 80:
                return min(30.0, base_interval * 2)
            elif cpu_percent > 60:
                return min(15.0, base_interval * 1.5)
                
            # Adjust based on memory pressure
            memory_mb = psutil.Process().memory_info().rss // 1024 // 1024
            if memory_mb > 700:
                return min(20.0, base_interval * 1.5)
                
            # ✅ ADDED: Adjust based on clock skew
            if self.client and hasattr(self.client, 'get_clock_sync_status'):
                clock_status = self.client.get_clock_sync_status()
                if clock_status.get('clock_sync_failed', False) or abs(clock_status.get('server_time_offset_ms', 0)) > 10000:
                    return min(10.0, base_interval * 1.2)  # Slow down if clock issues
                
            return max(1.0, base_interval)
        except:
            return base_interval
            
    def _log_fuzz_stats(self):
        """Log fuzz protection statistics including clock skew"""
        stats = self.get_fuzz_stats()
        total_rejected = (stats['malformed_market_data_rejected'] + 
                         stats['invalid_symbols_skipped'] + 
                         stats['corrupted_klines_handled'])
        
        if total_rejected > 0:
            self.logger.info(f"🛡️  Fuzz protection: {total_rejected} malformed items rejected")
        
        # ✅ ADDED: Log clock skew statistics
        if stats.get('clock_skew_issues_detected', 0) > 0:
            self.logger.info(f"🕒 Clock skew issues detected: {stats['clock_skew_issues_detected']}")
            
    def get_fuzz_stats(self) -> Dict:
        """Get comprehensive fuzz protection statistics including clock skew"""
        stats = self._fuzz_stats.copy()
        
        # Add component stats if available
        if self.chaos and hasattr(self.chaos, 'get_fuzz_stats'):
            stats['chaos_module'] = self.chaos.get_fuzz_stats()
            
        if self.client and hasattr(self.client, 'get_fuzz_stats'):
            stats['binance_client'] = self.client.get_fuzz_stats()
            
        if self._price_manipulation_detector:
            stats['price_detector'] = self._price_manipulation_detector._fuzz_stats
            
        if self._catastrophe_recovery:
            stats['catastrophe_recovery'] = self._catastrophe_recovery._fuzz_stats
            
        # ✅ ADDED: Clock sync status
        if self.client and hasattr(self.client, 'get_clock_sync_status'):
            clock_status = self.client.get_clock_sync_status()
            stats['clock_sync'] = {
                'offset_ms': clock_status.get('server_time_offset_ms', 0),
                'sync_failed': clock_status.get('clock_sync_failed', False),
                'last_sync': clock_status.get('last_sync_time', 0)
            }
            
        return stats

    async def shutdown(self):
        """✅ ENHANCED: Secure shutdown with statistics logging"""
        self.logger.critical("🛑 Initiating secure shutdown...")
        self.running = False
        self._shutdown_event.set()
        
        # Log final fuzz statistics including clock skew
        fuzz_stats = self.get_fuzz_stats()
        self.logger.info("📊 Fuzz Protection Summary:")
        for component, stats in fuzz_stats.items():
            if isinstance(stats, dict):
                self.logger.info(f"  {component}: {stats}")
            else:
                self.logger.info(f"  {component}: {stats}")
        
        # Cancel all monitoring tasks
        for task in self._monitor_tasks:
            if not task.done():
                task.cancel()
                
        # Wait for tasks to complete with timeout
        if self._monitor_tasks:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*self._monitor_tasks, return_exceptions=True),
                    timeout=10.0
                )
            except asyncio.TimeoutError:
                self.logger.warning("⚠️ Some tasks didn't finish gracefully")
                
        # Close client
        if self.client:
            await self.client.close()
            
        # Shutdown executor
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=False)
            
        # Final memory cleanup
        await self._perform_memory_cleanup()
            
        self.logger.info("✅ Secure shutdown complete")

    async def _emergency_cleanup(self):
        """Emergency cleanup when initialization fails"""
        try:
            if self.client:
                await self.client.close()
            if hasattr(self, '_executor'):
                self._executor.shutdown(wait=False)
            # Force garbage collection
            for _ in range(3):
                gc.collect()
        except Exception as e:
            self.logger.error(f"Emergency cleanup failed: {e}")

async def main():
    """Main entry point with fuzz protection and clock skew resilience"""
    setup_logging()
    logger = logging.getLogger('quantum_bot')
    
    # Event loop configuration
    if sys.platform != 'win32':
        try:
            import uvloop
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
            logger.info("✅ Using uvloop for enhanced performance")
        except ImportError:
            logger.info("⚠️ Using standard asyncio event loop")
    
    asyncio.get_event_loop().set_debug(True)
    
    try:
        bot = QuantumTradingBot()
        
        # Enhanced signal handling
        def signal_handler(signum, frame):
            logger.critical(f"💀 Received signal {signum} with fuzz protection active!")
            # Log fuzz stats before shutdown
            if hasattr(bot, 'get_fuzz_stats'):
                stats = bot.get_fuzz_stats()
                logger.info(f"🛡️  Final fuzz stats: {stats}")
            asyncio.create_task(bot.shutdown())
                
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        if await bot.initialize():
            await bot.run()
        else:
            logger.error("💀 Secure initialization failed")
            sys.exit(1)
            
    except MemoryError:
        logger.critical("💀 System out of memory - emergency shutdown")
        sys.exit(1)
    except KeyboardInterrupt:
        logger.info("🛑 Bot stopped by user")
    except Exception as e:
        logger.critical(f"💀 Ultimate failure: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())


### [FILE] requirements.txt

python-binance==1.0.19
aiohttp==3.8.6
websockets==12.0
ccxt==4.1.72
python-dotenv==1.0.0
numpy==1.24.4
pandas==2.0.3
scipy==1.11.4
scikit-learn==1.3.2
tensorflow-cpu==2.13.0
numba==0.57.0
uvloop==0.19.0; sys_platform != 'win32'
psutil==5.9.6
boto3==1.28.62
pyyaml==6.0.1
tqdm==4.66.1
cryptography==41.0.7
gymnasium==0.29.1
stable-baselines3==2.0.0
schedule==1.2.0
pytz==2023.3

### [FILE] main.py
import time
import yaml  # Changed from json to yaml
from binance_api.binance_client import BinanceClient
from ml.chaos_module import ChaosModule
from config.config_secrets import BINANCE_API_KEY, BINANCE_API_SECRET

# Load config - fixed file extension and loader
with open("config/config.yaml") as f:
    config = yaml.safe_load(f)

client = BinanceClient(config["binance"]["api_key"], config["binance"]["api_secret"], testnet=True)
chaos = ChaosModule(config)

symbols = config["symbols"]

while True:
    for symbol in symbols:
        print(f"\n🔍 Analyzing {symbol}")
        klines = client.get_klines(symbol, interval="1m", limit=config["chaos"]["window_size"])
        if not klines:
            print(f"❌ Failed to get data for {symbol}")
            continue

        prices = [float(kline[4]) for kline in klines]  # Use closing prices
        result = chaos.update(prices)  # Changed from analyze() to update()
        print(f"[{symbol}] Chaos Analysis → {result.upper()}")

    time.sleep(60)  # Run every minute


### [FILE] custom_collections.py
import threading
from collections import deque
from custom_types import Any, Optional, TypeVar
from dataclasses import dataclass

T = TypeVar('T')

@dataclass
class PairScore:
    symbol: str
    weight: float
    last_updated: float
    active: bool = True

class ThreadSafeDeque:
    def __init__(self, maxlen: Optional[int] = None):
        self._deque = deque(maxlen=maxlen)
        self._lock = threading.Lock()
        self._not_empty = threading.Condition(self._lock)

    def append(self, item: T) -> None:
        with self._lock:
            was_empty = len(self._deque) == 0
            self._deque.append(item)
            if was_empty:
                self._not_empty.notify()

    def popleft(self) -> T:
        with self._not_empty:
            while len(self._deque) == 0:
                self._not_empty.wait()
            return self._deque.popleft()

    def __len__(self) -> int:
        with self._lock:
            return len(self._deque)

    def __getitem__(self, index: int) -> T:
        with self._lock:
            return self._deque[index]

    def empty(self) -> bool:
        with self._lock:
            return len(self._deque) == 0

    def clear(self) -> None:
        with self._lock:
            self._deque.clear()

### [FILE] ws/ws_client.py

import asyncio
import websockets
import json
import logging
from typing import Callable, List
import time
import random
import ssl

logger = logging.getLogger('ws_client')

class BinanceWebSocketClient:
    def __init__(
        self,
        symbols: List[str],
        on_message: Callable[[dict], None],
        testnet: bool = False,
        reconnect_interval: int = 3,  # Reduced for HFT
        max_retries: int = 15,  # Increased for HFT resilience
        ping_interval: int = 20  # Reduced for HFT
    ):
        self.symbols = [s.lower() for s in symbols][:6]
        self.on_message = on_message
        self.base_url = "wss://testnet.binance.vision/stream" if testnet else "wss://stream.binance.com:9443/stream"
        self.reconnect_interval = reconnect_interval
        self.max_retries = max_retries
        self.ping_interval = ping_interval
        self._ws = None
        self._running = False
        self._retry_count = 0
        self._logger = logging.getLogger('websocket')
        self._logger.setLevel(logging.INFO)
        self._last_message_time = time.time()
        self._connection_lock = asyncio.Lock()
        self._reconnect_task = None
        self._ping_task = None
        self._message_queue = asyncio.Queue(maxsize=1000)  # HFT message buffer
        self._processing_task = None
        
        # HFT performance metrics
        self._hft_metrics = {
            'messages_processed': 0,
            'messages_dropped': 0,
            'avg_processing_time': 0,
            'peak_message_rate': 0,
            'reconnects': 0,
            'last_message_timestamp': 0
        }
        
    async def _run(self) -> None:
        """✅ FIXED: Robust WebSocket with HFT-optimized reconnection management"""
        while self._running and self._retry_count < self.max_retries:
            try:
                async with self._connection_lock:
                    streams = [f"{symbol}@trade" for symbol in self.symbols]
                    url = f"{self.base_url}?streams={'/'.join(streams)}"
                    
                    self._logger.info(f"🔌 Connecting to WebSocket: {url}")
                    
                    # SSL context for secure connection
                    ssl_context = ssl.create_default_context()
                    ssl_context.check_hostname = True
                    ssl_context.verify_mode = ssl.CERT_REQUIRED
                    
                    # HFT: Reduced timeouts for faster reconnection
                    async with websockets.connect(
                        url, 
                        ping_interval=self.ping_interval,
                        ping_timeout=5,  # Reduced from 10s
                        close_timeout=5,  # Reduced from 10s
                        ssl=ssl_context
                    ) as websocket:
                        
                        self._logger.info(f"✅ WebSocket connected to {url}")
                        self._retry_count = 0
                        self._last_message_time = time.time()
                        self._ws = websocket
                        self._hft_metrics['reconnects'] += 1
                        
                        # Start ping task
                        self._ping_task = asyncio.create_task(self._ping_loop(websocket))
                        
                        # Start message processing task
                        if not self._processing_task or self._processing_task.done():
                            self._processing_task = asyncio.create_task(self._process_message_queue())
                        
                        while self._running:
                            try:
                                # HFT: Reduced timeout for faster market data
                                message = await asyncio.wait_for(websocket.recv(), timeout=10)
                                self._last_message_time = time.time()
                                self._hft_metrics['last_message_timestamp'] = time.time()
                                
                                # HFT: Non-blocking queue put with drop policy
                                try:
                                    self._message_queue.put_nowait(message)
                                except asyncio.QueueFull:
                                    self._hft_metrics['messages_dropped'] += 1
                                    if self._hft_metrics['messages_dropped'] % 100 == 0:
                                        self._logger.warning(f"🚨 HFT Message queue full, dropped {self._hft_metrics['messages_dropped']} messages")
                                    
                            except asyncio.TimeoutError:
                                # Check if connection is still alive
                                if time.time() - self._last_message_time > self.ping_interval:
                                    try:
                                        await websocket.ping()
                                        self._logger.debug("Sent ping to keep connection alive")
                                    except Exception:
                                        break  # Reconnect on ping failure
                                continue
                            except websockets.exceptions.ConnectionClosed:
                                self._logger.warning("WebSocket connection closed")
                                break
                            except Exception as e:
                                self._logger.error(f"WebSocket receive error: {e}")
                                break
                                
            except (websockets.exceptions.ConnectionClosed, websockets.exceptions.InvalidURI) as e:
                await self._handle_reconnection(e)
            except Exception as e:
                await self._handle_reconnection(e)
                
        if self._retry_count >= self.max_retries:
            self._logger.error("❌ Maximum reconnection attempts reached")
            self._running = False

    async def _process_message_queue(self):
        """HFT-optimized message processing with rate limiting"""
        while self._running:
            try:
                # HFT: Process messages with timeout to prevent blocking
                message = await asyncio.wait_for(self._message_queue.get(), timeout=1.0)
                start_time = time.time()
                
                await self._process_message(message)
                
                processing_time = time.time() - start_time
                self._hft_metrics['messages_processed'] += 1
                
                # Update average processing time (exponential moving average)
                if self._hft_metrics['avg_processing_time'] == 0:
                    self._hft_metrics['avg_processing_time'] = processing_time
                else:
                    self._hft_metrics['avg_processing_time'] = (
                        0.9 * self._hft_metrics['avg_processing_time'] + 0.1 * processing_time
                    )
                
                # Track peak message rate
                current_rate = self._hft_metrics['messages_processed'] / (time.time() - self._hft_metrics.get('start_time', time.time()))
                self._hft_metrics['peak_message_rate'] = max(self._hft_metrics['peak_message_rate'], current_rate)
                
            except asyncio.TimeoutError:
                continue  # No messages, continue waiting
            except Exception as e:
                self._logger.error(f"Message queue processing error: {e}")
                await asyncio.sleep(0.1)  # Brief pause on error

    async def _process_message(self, message: str):
        """Process incoming message with HFT-optimized error handling"""
        try:
            data = json.loads(message)
            if isinstance(self.on_message, Callable):
                if asyncio.iscoroutinefunction(self.on_message):
                    await self.on_message(data)
                else:
                    # For sync callbacks, run in executor to avoid blocking
                    await asyncio.get_event_loop().run_in_executor(
                        None, self.on_message, data
                    )
        except json.JSONDecodeError as e:
            self._logger.error(f"JSON decode error: {e}")
        except Exception as e:
            self._logger.error(f"Message processing error: {e}")

    async def _ping_loop(self, websocket):
        """Background ping loop optimized for HFT"""
        while self._running and websocket.open:
            try:
                await asyncio.sleep(self.ping_interval)
                if time.time() - self._last_message_time > self.ping_interval:
                    ping_start = time.time()
                    await websocket.ping()
                    ping_time = time.time() - ping_start
                    if ping_time > 0.1:  # Log slow pings
                        self._logger.debug(f"Slow ping: {ping_time:.3f}s")
                    else:
                        self._logger.debug("Sent periodic ping")
            except Exception as e:
                self._logger.error(f"Ping error: {e}")
                break

    async def _handle_reconnection(self, error: Exception):
        """Handle reconnection with HFT-optimized exponential backoff"""
        self._retry_count += 1
        wait_time = self._calculate_backoff(self._retry_count)
        self._logger.error(f"WebSocket error (attempt {self._retry_count}/{self.max_retries}): {error}. Reconnecting in {wait_time}s...")
        
        # Cancel tasks
        if self._ping_task and not self._ping_task.done():
            self._ping_task.cancel()
            
        # Clear message queue on reconnection to prevent backlog
        while not self._message_queue.empty():
            try:
                self._message_queue.get_nowait()
                self._hft_metrics['messages_dropped'] += 1
            except:
                break
            
        await asyncio.sleep(wait_time)

    def _calculate_backoff(self, attempt: int) -> float:
        """Calculate exponential backoff with jitter optimized for HFT"""
        base_delay = min(self.reconnect_interval * (2 ** (attempt - 1)), 60)  # Reduced max from 300s
        jitter = random.uniform(0, base_delay * 0.1)
        return base_delay + jitter

    async def connect(self) -> None:
        """Start the WebSocket connection with HFT optimization"""
        if not self._running:
            self._running = True
            self._retry_count = 0
            self._hft_metrics['start_time'] = time.time()
            self._reconnect_task = asyncio.create_task(self._run())
            self._logger.info("WebSocket client started in HFT mode")

    async def disconnect(self) -> None:
        """Gracefully shutdown the connection with HFT cleanup"""
        self._running = False
        
        # Cancel tasks
        if self._ping_task and not self._ping_task.done():
            self._ping_task.cancel()
            
        if self._reconnect_task and not self._reconnect_task.done():
            self._reconnect_task.cancel()
            
        if self._processing_task and not self._processing_task.done():
            self._processing_task.cancel()
            
        # Clear message queue
        while not self._message_queue.empty():
            try:
                self._message_queue.get_nowait()
            except:
                break
            
        # Close connection
        if self._ws:
            await self._ws.close()
            
        self._logger.info("WebSocket client stopped")

    def get_hft_metrics(self) -> dict:
        """Get HFT performance metrics"""
        current_time = time.time()
        uptime = current_time - self._hft_metrics.get('start_time', current_time)
        
        return {
            'uptime_seconds': uptime,
            'messages_processed': self._hft_metrics['messages_processed'],
            'messages_dropped': self._hft_metrics['messages_dropped'],
            'drop_rate_percent': (self._hft_metrics['messages_dropped'] / 
                                max(1, self._hft_metrics['messages_processed'] + self._hft_metrics['messages_dropped'])) * 100,
            'avg_processing_time_ms': self._hft_metrics['avg_processing_time'] * 1000,
            'peak_message_rate_ps': self._hft_metrics['peak_message_rate'],
            'current_message_rate_ps': self._hft_metrics['messages_processed'] / max(1, uptime),
            'reconnects': self._hft_metrics['reconnects'],
            'queue_size': self._message_queue.qsize(),
            'time_since_last_message': current_time - self._hft_metrics['last_message_timestamp'] if self._hft_metrics['last_message_timestamp'] > 0 else float('inf')
        }




### [FILE] binance_api/client.py
import aiohttp
import asyncio
import time
import os
import logging
from contextlib import asynccontextmanager
import hmac
import hashlib
from urllib.parse import urlencode
import json
import websockets
import ssl
from decimal import Decimal, InvalidOperation
from typing import Optional, Dict, List
from datetime import datetime
import traceback
from concurrent.futures import ThreadPoolExecutor
import re
import struct
import zlib
import math

logger = logging.getLogger('binance_client')

# ✅ ENHANCED: Advanced fuzz protection patterns
MALICIOUS_DATA_PATTERNS = {
    'sql_injection': re.compile(r'(\%27)|(\')|(\-\-)|(\%23)|(#)', re.IGNORECASE),
    'xss_attempt': re.compile(r'((\%3C)|<)((\%2F)|\/)*[a-z0-9\%]+((\%3E)|>)', re.IGNORECASE),
    'path_traversal': re.compile(r'\.\.\/|\.\.\\', re.IGNORECASE),
    'binary_injection': re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]'),
    'oversized_data': re.compile(r'.{100000,}'),  # Strings longer than 100k chars
    'number_overflow': re.compile(r'[0-9]{50,}'),  # Very long numbers
    'command_injection': re.compile(r'[&\|;`\$]'),
    'xml_injection': re.compile(r'<!ENTITY|<!DOCTYPE|<\?xml'),
    'json_injection': re.compile(r'\$\{[^}]+\}'),  # JSON injection templates
    'prototype_pollution': re.compile(r'__proto__|constructor\.prototype'),
}

class SecureBinanceClient:
    """🔒 SECURITY HARDENED: Binance client with enhanced security"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._security_logger = logging.getLogger('security.binance')
        self._sensitive_patterns = [
            re.compile(r'("api_key"\s*:\s*")[^"]+(")'),
            re.compile(r'("api_secret"\s*:\s*")[^"]+(")'),
            re.compile(r'(api_key=)[^&]+'),
            re.compile(r'(signature=)[^&]+'),
            re.compile(r'("password"\s*:\s*")[^"]+(")'),
            re.compile(r'("private_key"\s*:\s*")[^"]+(")'),
        ]
    
    def _sanitize_log_data(self, data: str) -> str:
        """Sanitize sensitive data from logs with enhanced protection"""
        if not isinstance(data, str):
            return str(data)
            
        sanitized = data
        for pattern in self._sensitive_patterns:
            sanitized = pattern.sub(r'\1***REDACTED***\2', sanitized)
            
        # Additional sanitization for potential secrets
        sanitized = re.sub(r'(-----BEGIN.*PRIVATE KEY-----)[^-]+(-----END.*PRIVATE KEY-----)', 
                          r'\1***REDACTED***\2', sanitized, flags=re.DOTALL)
        
        return sanitized
    
    async def _make_request(self, method: str, endpoint: str, params: Dict = None, signed: bool = False) -> Dict:
        """🔒 SECURE: Request handling with enhanced sensitive data protection"""
        try:
            # Log sanitized request details
            sanitized_params = self._sanitize_log_data(str(params)) if params else None
            self._security_logger.debug(f"Secure request: {method} {endpoint}")
            
            result = await super()._make_request(method, endpoint, params, signed)
            return result
            
        except Exception as e:
            # Sanitize error messages
            error_msg = self._sanitize_log_data(str(e))
            self._security_logger.error(f"Secure request failed: {error_msg}")
            raise

class MalformedDataException(Exception):
    """Custom exception for malformed data detection"""
    pass

class DataSanitizer:
    """💀 ULTIMATE: Advanced data sanitization with comprehensive fuzz protection"""
    
    MAX_DEPTH = 20
    MAX_STRING_LENGTH = 10000
    MAX_ARRAY_SIZE = 1000
    MAX_OBJECT_KEYS = 500
    
    @staticmethod
    def sanitize_json_data(data: any, max_depth: int = MAX_DEPTH) -> any:
        """Recursively sanitize JSON data with depth limiting and enhanced security"""
        return DataSanitizer._sanitize_recursive(data, max_depth, 0, set())
    
    @staticmethod
    def _sanitize_recursive(obj: any, max_depth: int, current_depth: int, visited: set) -> any:
        """Recursive sanitization with depth protection and cycle detection"""
        if current_depth > max_depth:
            raise MalformedDataException(f"Maximum depth {max_depth} exceeded")
            
        if obj is None:
            return None
            
        # Cycle detection
        obj_id = id(obj)
        if obj_id in visited:
            raise MalformedDataException("Circular reference detected")
        visited.add(obj_id)
            
        try:
            if isinstance(obj, (int, float)):
                return DataSanitizer._sanitize_number(obj)
                
            if isinstance(obj, str):
                return DataSanitizer._sanitize_string(obj)
                
            if isinstance(obj, dict):
                return DataSanitizer._sanitize_dict(obj, max_depth, current_depth, visited)
                
            if isinstance(obj, (list, tuple)):
                return DataSanitizer._sanitize_array(obj, max_depth, current_depth, visited)
                
            # Reject any other types with detailed logging
            raise MalformedDataException(f"Unsupported data type: {type(obj)}")
            
        finally:
            visited.discard(obj_id)
    
    @staticmethod
    def _sanitize_dict(obj: dict, max_depth: int, current_depth: int, visited: set) -> dict:
        """Sanitize dictionary with key validation"""
        if len(obj) > DataSanitizer.MAX_OBJECT_KEYS:
            raise MalformedDataException(f"Object has too many keys: {len(obj)}")
            
        sanitized = {}
        for key, value in obj.items():
            if not DataSanitizer._is_valid_key(key):
                raise MalformedDataException(f"Invalid dictionary key: {key}")
                
            try:
                sanitized[key] = DataSanitizer._sanitize_recursive(
                    value, max_depth, current_depth + 1, visited.copy()
                )
            except MalformedDataException as e:
                # Log but continue processing other keys
                logger.warning(f"Key '{key}' sanitization failed: {e}")
                continue
                
        return sanitized
    
    @staticmethod
    def _sanitize_array(obj: list, max_depth: int, current_depth: int, visited: set) -> list:
        """Sanitize array with size limits"""
        if len(obj) > DataSanitizer.MAX_ARRAY_SIZE:
            raise MalformedDataException(f"Array too large: {len(obj)}")
            
        sanitized = []
        for item in obj:
            try:
                sanitized.append(DataSanitizer._sanitize_recursive(
                    item, max_depth, current_depth + 1, visited.copy()
                ))
            except MalformedDataException as e:
                logger.warning(f"Array item sanitization failed: {e}")
                continue
                
        return sanitized
    
    @staticmethod
    def _sanitize_number(num: any) -> float:
        """Sanitize numbers with overflow protection and enhanced validation"""
        try:
            if isinstance(num, str):
                # Enhanced numeric string validation
                if not re.match(r'^-?\d*\.?\d+([eE][-+]?\d+)?$', num.strip()):
                    raise MalformedDataException("Invalid numeric format")
                
                # Check for scientific notation attacks
                if 'e' in num.lower() and len(num) > 20:
                    raise MalformedDataException("Potentially malicious scientific notation")
                
            result = float(num)
            
            # Enhanced infinity and NaN checks
            if not math.isfinite(result):
                raise MalformedDataException("Numeric value is not finite")
                
            # Check for unreasonable sizes (beyond typical crypto prices)
            if abs(result) > 1e15:  # $1 quadrillion limit
                raise MalformedDataException("Numeric value unrealistically large")
                
            # Check for denormalized numbers
            if abs(result) < 1e-308 and result != 0:
                raise MalformedDataException("Denormalized number detected")
                
            return result
            
        except (ValueError, TypeError, OverflowError) as e:
            raise MalformedDataException(f"Number sanitization failed: {e}")
    
    @staticmethod
    def _sanitize_string(s: str) -> str:
        """Sanitize strings with comprehensive injection protection"""
        if s is None:
            return ""
            
        # Convert to string if needed
        str_value = str(s) if not isinstance(s, str) else s
        
        # Check length limits
        if len(str_value) > DataSanitizer.MAX_STRING_LENGTH:
            raise MalformedDataException("String too long")
            
        # Enhanced security: Check for malicious patterns
        for pattern_name, pattern in MALICIOUS_DATA_PATTERNS.items():
            if pattern.search(str_value):
                raise MalformedDataException(f"Malicious content detected: {pattern_name}")
                
        # Remove control characters except common whitespace
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', str_value)
        
        # Normalize line endings to prevent injection
        cleaned = cleaned.replace('\r\n', '\n').replace('\r', '\n')
        
        # Check for excessive whitespace (potential DoS)
        if re.search(r'\s{100,}', cleaned):
            raise MalformedDataException("Excessive whitespace detected")
            
        return cleaned
    
    @staticmethod
    def _is_valid_key(key: any) -> bool:
        """Enhanced dictionary key validation"""
        if not isinstance(key, str):
            return False
            
        if len(key) > 256:
            return False
            
        # Enhanced suspicious key patterns
        suspicious_patterns = [
            r'__.*__',  # Python internal
            r'\.\.',    # Dot dot
            r'[\[\]{}]',  # Brackets and braces
            r'[\(\)]',  # Parentheses
            r'[\x00-\x1F\x7F]',  # Control characters
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, key):
                return False
                
        return True

    @staticmethod
    def detect_data_anomalies(data: any) -> List[str]:
        """Detect potential data anomalies and return list of issues"""
        anomalies = []
        
        try:
            if isinstance(data, (list, tuple)):
                # Check for array anomalies
                if len(data) > 1000:
                    anomalies.append("Large array detected")
                    
                # Check for duplicate items (potential amplification)
                if len(data) != len(set(str(item) for item in data)):
                    anomalies.append("Duplicate items in array")
                    
            elif isinstance(data, dict):
                # Check for suspicious key patterns
                for key in data.keys():
                    if any(pattern in key.lower() for pattern in ['password', 'secret', 'key', 'token']):
                        anomalies.append(f"Sensitive key pattern detected: {key}")
                        
            elif isinstance(data, str):
                # Check string anomalies
                if len(data) > 10000:
                    anomalies.append("Oversized string")
                    
                # Check entropy (potential encrypted/compressed data)
                if DataSanitizer._calculate_entropy(data) > 0.9:
                    anomalies.append("High entropy string (potential encrypted data)")
                    
        except Exception as e:
            anomalies.append(f"Anomaly detection failed: {e}")
            
        return anomalies

    @staticmethod
    def _calculate_entropy(data: str) -> float:
        """Calculate Shannon entropy of a string"""
        if not data:
            return 0
            
        entropy = 0
        for x in range(256):
            p_x = float(data.count(chr(x))) / len(data)
            if p_x > 0:
                entropy += - p_x * math.log(p_x, 2)
                
        return entropy / 8.0  # Normalize to 0-1

# ✅ FIXED: Global SSL context for consistent SSL handling
def create_ssl_context():
    """Create robust SSL context for Binance API with enhanced security"""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = True
    ssl_context.verify_mode = ssl.CERT_REQUIRED
    
    # Enhanced SSL security settings
    ssl_context.set_ciphers('ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!MD5:!DSS')
    ssl_context.options |= ssl.OP_NO_SSLv2
    ssl_context.options |= ssl.OP_NO_SSLv3
    ssl_context.options |= ssl.OP_NO_TLSv1
    ssl_context.options |= ssl.OP_NO_TLSv1_1
    ssl_context.options |= ssl.OP_NO_COMPRESSION
    
    # Load system certificates
    cert_paths = [
        '/etc/ssl/certs/ca-certificates.crt',
        '/etc/pki/tls/certs/ca-bundle.crt', 
        '/etc/ssl/cert.pem',
    ]
    
    for cert_path in cert_paths:
        if os.path.exists(cert_path):
            try:
                ssl_context.load_verify_locations(cert_path)
                break
            except Exception as e:
                logger.debug(f"Failed to load cert from {cert_path}: {e}")
                continue
    else:
        logger.warning("Using default SSL certificates - may cause issues in Docker")
    
    return ssl_context

# Global SSL context
GLOBAL_SSL_CONTEXT = create_ssl_context()

class AsyncMutex:
    """✅ FIXED: Async mutex for connection pool race conditions"""
    def __init__(self):
        self._lock = asyncio.Lock()
        self._waiters = 0
        self._name = "anonymous"

    def set_name(self, name: str):
        self._name = name

    async def __aenter__(self):
        self._waiters += 1
        await self._lock.acquire()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        self._lock.release()
        self._waiters -= 1

    @property
    def waiters(self):
        return self._waiters

    @property 
    def name(self):
        return self._name

class ConnectionPool:
    """💀 ULTIMATE: Fixed connection pool with brutal resilience and race condition prevention"""
    
    def __init__(self, api_key: str, max_size: int = 8, ttl: int = 300):
        self.api_key = api_key
        self.max_size = max_size
        self.ttl = ttl
        self._sessions = []
        self._mutex = AsyncMutex()
        self._mutex.set_name("ConnectionPool")
        self._cleanup_task = None
        self._stats = {
            'created': 0,
            'reused': 0,
            'closed': 0,
            'errors': 0,
            'active_connections': 0,
            'start_time': time.time(),
            'pool_exhaustions': 0,
            'race_conditions_prevented': 0,
            'malformed_data_rejected': 0
        }
        self._session_creation_mutex = AsyncMutex()
        self._session_creation_mutex.set_name("SessionCreation")
        self._shutdown = False
        
        # ✅ FIXED: Enhanced connection pool exhaustion recovery
        self._exhaustion_recovery = {
            'last_exhaustion': 0,
            'exhaustion_count': 0,
            'recovery_mode': False,
            'emergency_sessions': []
        }
        
    async def get_session(self) -> aiohttp.ClientSession:
        """💀 ATOMIC session acquisition with brutal error handling and race condition prevention"""
        if self._shutdown:
            raise RuntimeError("Connection pool is shutdown")
            
        async with self._mutex:
            try:
                await self._cleanup_stale_sessions_atomic()
                
                # ✅ FIXED: Connection pool exhaustion recovery logic
                if self._exhaustion_recovery['recovery_mode']:
                    if time.time() - self._exhaustion_recovery['last_exhaustion'] > 30:
                        self._exhaustion_recovery['recovery_mode'] = False
                        logger.info("🔄 Exiting connection pool recovery mode")
                    else:
                        # Use emergency session
                        if self._exhaustion_recovery['emergency_sessions']:
                            session = self._exhaustion_recovery['emergency_sessions'].pop()
                            session._last_used = time.time()
                            logger.warning("🚨 Using emergency connection pool session")
                            return session
                
                # Try to find an available session
                current_time = time.time()
                for session in list(self._sessions):
                    if not session.closed and current_time - getattr(session, '_last_used', 0) < self.ttl:
                        session._last_used = current_time
                        session._use_count += 1
                        self._stats['reused'] += 1
                        self._stats['active_connections'] = len([s for s in self._sessions if not s.closed])
                        logger.debug("♻️ Reusing session")
                        return session
                        
                # No available session, create new one with creation lock
                async with self._session_creation_mutex:
                    # Double-check after acquiring creation lock (prevent race condition)
                    current_time = time.time()
                    for session in list(self._sessions):
                        if not session.closed and current_time - getattr(session, '_last_used', 0) < self.ttl:
                            session._last_used = current_time
                            session._use_count += 1
                            self._stats['reused'] += 1
                            self._stats['race_conditions_prevented'] += 1
                            return session
                    
                    active_count = len([s for s in self._sessions if not s.closed])
                    if active_count >= self.max_size:
                        self._stats['pool_exhaustions'] += 1
                        self._handle_pool_exhaustion()
                        logger.warning(f"📊 Connection pool full ({active_count}/{self.max_size}), creating overflow session")
                    
                    # ✅ FIXED: Use global SSL context for consistency
                    timeout = aiohttp.ClientTimeout(
                        total=30, 
                        connect=10, 
                        sock_connect=10, 
                        sock_read=20
                    )
                    connector = aiohttp.TCPConnector(
                        ssl=GLOBAL_SSL_CONTEXT,  # ✅ FIXED: Use global SSL context
                        limit=10, 
                        limit_per_host=5,
                        use_dns_cache=True,
                        keepalive_timeout=30
                    )
                    
                    session = aiohttp.ClientSession(
                        timeout=timeout,
                        connector=connector,
                        headers={
                            'X-MBX-APIKEY': self.api_key,
                            'X-MBX-TIME-UNIT': 'microsecond',
                            'User-Agent': 'QuantumBot/1.0.0'
                        }
                    )
                    
                    session._created_time = time.time()
                    session._last_used = time.time()
                    session._use_count = 1
                    session._session_id = f"sess_{self._stats['created']}_{int(time.time())}"
                    
                    self._sessions.append(session)
                    self._stats['created'] += 1
                    self._stats['active_connections'] = len([s for s in self._sessions if not s.closed])
                    
                    logger.info(f"🔗 Created new session {session._session_id} (pool: {self._stats['active_connections']}/{self.max_size})")
                    
                    if not self._cleanup_task or self._cleanup_task.done():
                        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
                        
                    return session
                    
            except Exception as e:
                self._stats['errors'] += 1
                logger.error(f"❌ Connection pool error: {e}")
                
                # ✅ FIXED: Emergency session creation on critical failure
                try:
                    emergency_session = await self._create_emergency_session()
                    return emergency_session
                except Exception as emergency_error:
                    logger.critical(f"💀 Emergency session creation failed: {emergency_error}")
                    raise
                
    def _handle_pool_exhaustion(self):
        """Handle connection pool exhaustion with recovery logic"""
        current_time = time.time()
        self._exhaustion_recovery['last_exhaustion'] = current_time
        self._exhaustion_recovery['exhaustion_count'] += 1
        
        # Enter recovery mode after multiple exhaustions
        if self._exhaustion_recovery['exhaustion_count'] >= 3:
            self._exhaustion_recovery['recovery_mode'] = True
            logger.warning("🚨 Entering connection pool recovery mode")
            
            # Create emergency sessions
            asyncio.create_task(self._create_emergency_sessions())
            
    async def _create_emergency_sessions(self):
        """Create emergency sessions for recovery mode"""
        try:
            for i in range(2):  # Create 2 emergency sessions
                session = await self._create_emergency_session()
                self._exhaustion_recovery['emergency_sessions'].append(session)
                logger.info(f"🆘 Created emergency session {i+1}/2")
        except Exception as e:
            logger.error(f"Emergency session creation failed: {e}")
            
    async def _create_emergency_session(self) -> aiohttp.ClientSession:
        """Create an emergency session with minimal configuration"""
        timeout = aiohttp.ClientTimeout(total=15, connect=5, sock_connect=5, sock_read=10)
        connector = aiohttp.TCPConnector(
            ssl=GLOBAL_SSL_CONTEXT,
            limit=2,
            limit_per_host=1,
            use_dns_cache=False,  # Bypass DNS cache in emergency
        )
        
        session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                'X-MBX-APIKEY': self.api_key,
                'User-Agent': 'QuantumBot/Emergency'
            }
        )
        
        session._created_time = time.time()
        session._last_used = time.time()
        session._use_count = 0
        session._session_id = f"emergency_{int(time.time())}"
        session._emergency = True  # Mark as emergency session
        
        return session

    async def _cleanup_stale_sessions_atomic(self):
        """💀 Atomic cleanup to prevent race conditions"""
        current_time = time.time()
        sessions_to_remove = []
        
        for session in list(self._sessions):
            try:
                session_age = current_time - getattr(session, '_created_time', 0)
                session_idle = current_time - getattr(session, '_last_used', 0)
                
                if (session.closed or 
                    session_age > self.ttl or
                    session_idle > (self.ttl / 2)):
                    
                    sessions_to_remove.append(session)
                    if not session.closed:
                        await session.close()
                        self._stats['closed'] += 1
            except Exception as e:
                logger.warning(f"⚠️ Session cleanup warning: {e}")
                sessions_to_remove.append(session)
                
        for stale_session in sessions_to_remove:
            if stale_session in self._sessions:
                self._sessions.remove(stale_session)
            
        if sessions_to_remove:
            active_count = len([s for s in self._sessions if not s.closed])
            logger.info(f"🧹 Cleaned up {len(sessions_to_remove)} stale sessions (active: {active_count})")
                
    async def _periodic_cleanup(self):
        """💀 Continuous atomic cleanup"""
        while not self._shutdown:
            try:
                await asyncio.sleep(60)
                async with self._mutex:
                    await self._cleanup_stale_sessions_atomic()
                    
                # Log pool statistics every 5 minutes
                if int(time.time()) % 300 == 0:
                    stats = self.get_stats()
                    logger.info(f"📊 Connection pool stats: {stats['active_sessions']} active, "
                               f"{stats['created']} created, {stats['reused']} reused, "
                               f"{stats['pool_exhaustions']} exhaustions")
                    
            except Exception as e:
                logger.error(f"Periodic cleanup error: {e}")
                await asyncio.sleep(30)
                
    async def close_all(self):
        """💀 Atomic shutdown with brutal cleanup"""
        self._shutdown = True
        async with self._mutex:
            closing_tasks = []
            for session in list(self._sessions):
                if not session.closed:
                    closing_tasks.append(session.close())
                    
            # Close emergency sessions
            for session in self._exhaustion_recovery['emergency_sessions']:
                if not session.closed:
                    closing_tasks.append(session.close())
            self._exhaustion_recovery['emergency_sessions'].clear()
                    
            if closing_tasks:
                results = await asyncio.gather(*closing_tasks, return_exceptions=True)
                errors = [r for r in results if isinstance(r, Exception)]
                if errors:
                    logger.error(f"❌ {len(errors)} session close errors")
                self._sessions.clear()
                logger.info("🔚 Closed all connection pool sessions")
                
    def get_stats(self):
        """Get atomic statistics"""
        stats = self._stats.copy()
        stats['pool_size'] = len(self._sessions)
        stats['active_sessions'] = len([s for s in self._sessions if not s.closed])
        stats['uptime_seconds'] = time.time() - stats['start_time']
        stats['mutex_waiters'] = self._mutex.waiters
        stats['exhaustion_recovery'] = self._exhaustion_recovery.copy()
        return stats

class BinanceClient:
    """💀 ULTIMATE: Fuzz-hardened Binance client with malformed data protection, atomic operations, and CLOCK SKEW RESILIENCE"""
    
    def __init__(self, api_key: str = None, api_secret: str = None, testnet: bool = False):
        self.base_url = 'https://testnet.binance.vision/api' if testnet else 'https://api.binance.com'
        self.ws_url = 'wss://testnet.binance.vision/stream' if testnet else 'wss://stream.binance.com:9443/stream'
        
        self.api_key = api_key or os.getenv('BINANCE_API_KEY', '')
        self.api_secret = api_secret or os.getenv('BINANCE_API_SECRET', '')
        
        # ✅ ENHANCED: Validate credentials format
        if not self._validate_credentials():
            raise ValueError("Invalid API credentials format")
            
        self._session_pool = ConnectionPool(self.api_key, max_size=6, ttl=300)
        self.testnet = testnet
        self._request_mutex = AsyncMutex()
        self._request_mutex.set_name("RequestMutex")
        self._order_mutex = AsyncMutex()
        self._order_mutex.set_name("OrderMutex")
        self._shutdown = False
        self._partial_orders = {}
        
        # ✅ ADDED: Clock skew resilience
        self._server_time_offset = 0  # Difference between server time and local time (ms)
        self._last_time_sync = 0
        self._time_sync_interval = 3600  # Sync time every hour
        self._time_sync_mutex = asyncio.Lock()
        self._max_clock_skew = 5000  # 5 seconds maximum allowed clock skew
        self._sync_attempts = 0
        self._max_sync_attempts = 3
        self._clock_sync_failed = False
        
        # ✅ ADDED: Enhanced dependency failure tracking
        self._dependency_stats = {
            'connection_failures': 0,
            'ssl_errors': 0,
            'rate_limits': 0,
            'auth_failures': 0,
            'server_errors': 0,
            'timeouts': 0,
            'recovery_attempts': 0,
            'successful_recoveries': 0,
            'last_failure_time': 0,
            'degraded_mode': False,
            'clock_sync_attempts': 0,
            'clock_sync_failures': 0,
            'clock_sync_recoveries': 0  # ✅ ADDED: Track successful recoveries
        }
        
        # ✅ ADDED: Fuzz testing counters
        self._fuzz_stats = {
            'malformed_responses_rejected': 0,
            'invalid_json_encountered': 0,
            'oversized_data_rejected': 0,
            'injection_attempts_blocked': 0,
            'malformed_market_data_rejected': 0,
            'successful_requests': 0
        }
        
        # ✅ FIXED: Enhanced Binance error handling with specific recovery strategies
        self._binance_error_codes = {
            -1000: ('UNKNOWN', 'recoverable', 'General unknown error'),
            -1001: ('DISCONNECTED', 'recoverable', 'Internal error; please retry'),
            -1003: ('TOO_MANY_REQUESTS', 'recoverable', 'Rate limit exceeded'),
            -1006: ('UNEXPECTED_RESP', 'recoverable', 'An unexpected response was received'),
            -1007: ('TIMEOUT', 'recoverable', 'Timeout waiting for response from backend server'),
            -1013: ('INVALID_MESSAGE', 'fatal', 'Invalid message received'),
            -1015: ('TOO_MANY_ORDERS', 'fatal', 'Too many new orders'),
            -1021: ('INVALID_TIMESTAMP', 'recoverable', 'Timestamp for this request is outside of the recvWindow'),
            -2010: ('NEW_ORDER_REJECTED', 'fatal', 'New order rejected'),
            -2011: ('CANCEL_REJECTED', 'fatal', 'Cancel rejected'),
        }
        
        self._rate_limiter = asyncio.Semaphore(4)
        self._last_request_time = 0
        self._request_times = []
        
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="binance_client")
        
        # Memory leak prevention
        self._cleanup_interval = 300
        self._last_cleanup = time.time()
        
        # Start clock synchronization
        asyncio.create_task(self._periodic_time_sync())
        
        logger.info(f"✅ Binance client initialized (testnet: {testnet})")
        
    async def _periodic_time_sync(self):
        """Periodically synchronize time with Binance servers"""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._time_sync_interval)
                await self._sync_server_time()
            except Exception as e:
                logger.error(f"Periodic time sync failed: {e}")
                await asyncio.sleep(300)  # Retry in 5 minutes on error

        async def _sync_server_time(self):
        """💀 ULTIMATE: Synchronize local clock with Binance server time with HFT optimization"""
        async with self._time_sync_mutex:
            try:
                self._dependency_stats['clock_sync_attempts'] += 1
                self._sync_attempts += 1
                
                # HFT: Use multiple time samples for better accuracy
                time_samples = []
                for sample in range(3):  # Take 3 samples
                    # Get server time
                    endpoint = 'v3/time'
                    session = await self._session_pool.get_session()
                    
                    start_time = time.time() * 1000  # Local time in milliseconds
                    
                    async with session.get(f"{self.base_url}/{endpoint}") as response:
                        if response.status == 200:
                            data = await response.json()
                            server_time = data['serverTime']
                            end_time = time.time() * 1000  # Local time after request
                            
                            # Calculate round-trip time and estimate one-way latency
                            rtt = end_time - start_time
                            estimated_latency = rtt / 2
                            
                            # Calculate server time offset (server_time - local_time)
                            # Use the midpoint of the request for better accuracy
                            midpoint_time = start_time + estimated_latency
                            sample_offset = server_time - midpoint_time
                            
                            time_samples.append({
                                'offset': sample_offset,
                                'rtt': rtt,
                                'latency': estimated_latency,
                                'timestamp': time.time()
                            })
                    
                    # HFT: Brief pause between samples
                    if sample < 2:
                        await asyncio.sleep(0.1)
                
                # Use the sample with lowest latency for highest accuracy
                best_sample = min(time_samples, key=lambda x: x['latency'])
                new_offset = best_sample['offset']
                
                # ✅ ENHANCED: Validate the new offset is reasonable for HFT
                if abs(new_offset) > 60000:  # 1 minute maximum reasonable offset for HFT
                    logger.warning(f"⚠️ Suspicious clock offset detected: {new_offset}ms")
                    if self._sync_attempts < self._max_sync_attempts:
                        logger.info("🔄 Retrying time synchronization...")
                        await asyncio.sleep(1)
                        return await self._sync_server_time()
                    else:
                        logger.error("💀 Maximum time sync attempts reached, using fallback")
                        self._clock_sync_failed = True
                        return
                
                # HFT: Smooth offset adjustment to prevent sudden jumps
                if abs(self._server_time_offset - new_offset) > 1000:  # More than 1 second change
                    smoothing_factor = 0.7  # Smooth large changes
                    self._server_time_offset = (
                        smoothing_factor * self._server_time_offset + 
                        (1 - smoothing_factor) * new_offset
                    )
                else:
                    self._server_time_offset = new_offset
                    
                self._last_time_sync = time.time()
                self._sync_attempts = 0  # Reset on success
                self._clock_sync_failed = False
                self._dependency_stats['clock_sync_recoveries'] += 1
                
                # HFT: Check if clock skew is within acceptable limits
                if abs(self._server_time_offset) > 1000:  # 1 second threshold for HFT
                    logger.warning(f"⚠️ Clock skew suboptimal for HFT: {self._server_time_offset:.0f}ms")
                elif abs(self._server_time_offset) > 100:  # 100ms threshold for optimal HFT
                    logger.info(f"🕒 Clock synchronized. Offset: {self._server_time_offset:.0f}ms")
                else:
                    logger.info(f"🎯 HFT Clock sync optimal: {self._server_time_offset:.0f}ms")
                    
            except Exception as e:
                self._dependency_stats['clock_sync_failures'] += 1
                logger.error(f"❌ Time synchronization failed: {e}")
                
                if self._sync_attempts < self._max_sync_attempts:
                    wait_time = 2 ** self._sync_attempts  # Exponential backoff
                    logger.info(f"🔄 Retrying time sync in {wait_time}s (attempt {self._sync_attempts + 1})")
                    await asyncio.sleep(wait_time)
                    return await self._sync_server_time()
                else:
                    logger.critical("💀 Maximum time synchronization attempts reached")
                    self._clock_sync_failed = True
                    # Don't update offset on failure - use last known good offset

    def _get_adjusted_timestamp(self) -> int:
        """Get timestamp adjusted for server time offset with fallback"""
        local_time_ms = int(time.time() * 1000)
        
        if self._clock_sync_failed:
            # If sync failed, use local time with warning
            logger.warning("⚠️ Using local time due to clock sync failure")
            return local_time_ms
            
        adjusted_time = local_time_ms + int(self._server_time_offset)
        return adjusted_time

    async def _ensure_time_synchronized(self):
        """Ensure time is synchronized before making signed requests with fallback"""
        current_time = time.time()
        
        # Sync if never synced or if last sync was too long ago or if clock skew is too large
        needs_sync = (
            self._last_time_sync == 0 or 
            current_time - self._last_time_sync > self._time_sync_interval or
            abs(self._server_time_offset) > self._max_clock_skew or
            self._clock_sync_failed
        )
        
        if needs_sync and not self._clock_sync_failed:
            logger.info("🔄 Synchronizing time with Binance server...")
            await self._sync_server_time()
        elif self._clock_sync_failed:
            logger.warning("⏰ Clock sync previously failed, using local time with recvWindow buffer")

    def _validate_credentials(self) -> bool:
        """Validate API credential format to prevent injection"""
        if not self.api_key or not self.api_secret:
            return False
            
        # Check for reasonable length and character set
        if len(self.api_key) < 20 or len(self.api_key) > 100:
            return False
        if len(self.api_secret) < 20 or len(self.api_secret) > 100:
            return False
            
        # Check for only valid characters
        if not re.match(r'^[a-zA-Z0-9]+$', self.api_key):
            return False
            
        return True

    async def _make_request(self, method: str, endpoint: str, params: Dict = None, signed: bool = False) -> Dict:
        """✅ ENHANCED: Atomic request handling with comprehensive fuzz protection, dependency failure recovery, and CLOCK SKEW RESILIENCE"""
        attempt = 0
        max_attempts = 3
        
        # Memory cleanup check
        current_time = time.time()
        if current_time - self._last_cleanup > self._cleanup_interval:
            await self._cleanup_memory()
            self._last_cleanup = current_time
        
        # ✅ FIXED: Enhanced dependency failure detection
        if self._dependency_stats['degraded_mode']:
            # Check if we should exit degraded mode
            if current_time - self._dependency_stats['last_failure_time'] > 300:  # 5 minutes
                self._dependency_stats['degraded_mode'] = False
                logger.info("🔄 Exiting degraded mode - dependencies stabilized")
        
        # ✅ FIXED: Rate limiting with backpressure and dependency awareness
        current_time = time.time()
        if self._request_times:
            avg_request_time = sum(self._request_times) / len(self._request_times)
            min_interval = max(0.15, avg_request_time * 2.0)
            time_since_last = current_time - self._last_request_time
            if time_since_last < min_interval:
                await asyncio.sleep(min_interval - time_since_last)
        
        while attempt < max_attempts:
            attempt += 1
            session = None
            
            try:
                async with self._rate_limiter:
                    start_time = time.time()
                    
                    # ✅ ADDED: Parameter sanitization
                    if params is None:
                        params = {}
                    else:
                        params = self._sanitize_params(params)
                        
                    if signed:
                        # ✅ ENHANCED: Ensure time synchronization before signed requests with fallback
                        await self._ensure_time_synchronized()
                        
                        # ✅ FIXED: Use server-adjusted timestamp for signed requests with fallback
                        timestamp = self._get_adjusted_timestamp()
                        params['timestamp'] = timestamp
                        
                        # ✅ ADDED: Larger recvWindow if clock sync failed
                        if self._clock_sync_failed:
                            params['recvWindow'] = 15000  # 15 seconds instead of default 5
                            logger.warning("⏰ Using extended recvWindow due to clock sync issues")
                        
                        params['signature'] = self._sign_request(params)
                    
                    session = await self._session_pool.get_session()
                    url = f"{self.base_url}/{endpoint}"
                    
                    async with session.request(method.upper(), url, params=params) as response:
                        result = await self._handle_response_with_fuzz_protection(response, attempt, max_attempts)
                        
                    request_time = time.time() - start_time
                    self._request_times.append(request_time)
                    if len(self._request_times) > 40:
                        self._request_times.pop(0)
                    self._last_request_time = time.time()
                    self._fuzz_stats['successful_requests'] += 1
                    
                    # ✅ FIXED: Reset dependency failure counters on success
                    if self._dependency_stats['degraded_mode']:
                        self._dependency_stats['successful_recoveries'] += 1
                    
                    return result
                    
            except MalformedDataException as e:
                self._fuzz_stats['malformed_responses_rejected'] += 1
                logger.warning(f"🚫 Malformed data rejected: {e}")
                if attempt < max_attempts:
                    await asyncio.sleep(2 ** attempt)
                    continue
                raise
            except asyncio.TimeoutError:
                self._dependency_stats['timeouts'] += 1
                self._update_dependency_health()
                logger.error(f"⏰ Request timeout (attempt {attempt}/{max_attempts})")
                if attempt < max_attempts:
                    await self._handle_dependency_failure("timeout", attempt, max_attempts)
                    continue
                # ✅ FIXED: Binance API timeout specifics
                raise Exception("Timeout waiting for response from backend server. Send status unknown; execution status unknown.")
            except aiohttp.ClientError as e:
                self._dependency_stats['connection_failures'] += 1
                self._update_dependency_health()
                logger.error(f"🌐 Client error (attempt {attempt}/{max_attempts}): {e}")
                if attempt < max_attempts:
                    await self._handle_dependency_failure("client_error", attempt, max_attempts)
                    continue
                raise
            except ssl.SSLError as e:
                self._dependency_stats['ssl_errors'] += 1
                self._update_dependency_health()
                logger.error(f"🔒 SSL error (attempt {attempt}/{max_attempts}): {e}")
                if attempt < max_attempts:
                    await self._handle_dependency_failure("ssl_error", attempt, max_attempts)
                    continue
                raise
            except Exception as e:
                logger.error(f"💀 Unexpected error (attempt {attempt}/{max_attempts}): {e}")
                if attempt < max_attempts:
                    await self._handle_retry(attempt, "general_error")
                    continue
                raise
            finally:
                if session:
                    session._last_used = time.time()
                    
        raise Exception(f"All {max_attempts} request attempts failed")

    def _update_dependency_health(self):
        """Update dependency health status and enter degraded mode if needed"""
        current_time = time.time()
        self._dependency_stats['last_failure_time'] = current_time
        self._dependency_stats['recovery_attempts'] += 1
        
        # Enter degraded mode after multiple failures
        recent_failures = sum([
            self._dependency_stats['connection_failures'],
            self._dependency_stats['ssl_errors'], 
            self._dependency_stats['timeouts']
        ])
        
        if recent_failures >= 5 and not self._dependency_stats['degraded_mode']:
            self._dependency_stats['degraded_mode'] = True
            logger.warning("🚨 Entering degraded mode due to dependency failures")
            
    async def _handle_dependency_failure(self, error_type: str, attempt: int, max_attempts: int):
        """Handle dependency failures with intelligent backoff"""
        self._dependency_stats['recovery_attempts'] += 1
        
        if self._dependency_stats['degraded_mode']:
            # Longer backoff in degraded mode
            wait_time = min(60, 10 * attempt)
            logger.warning(f"🔄 Degraded mode backoff: {wait_time}s (attempt {attempt})")
        else:
            wait_time = min(30, 2 ** attempt)
            logger.warning(f"🔄 Dependency failure backoff: {wait_time}s (attempt {attempt})")
            
        await asyncio.sleep(wait_time)

    def _sanitize_params(self, params: Dict) -> Dict:
        """Sanitize request parameters"""
        sanitized = {}
        for key, value in params.items():
            try:
                # Sanitize key
                if not DataSanitizer._is_valid_key(str(key)):
                    continue
                    
                # Sanitize value based on type
                if isinstance(value, (int, float)):
                    sanitized[key] = DataSanitizer._sanitize_number(value)
                elif isinstance(value, str):
                    sanitized[key] = DataSanitizer._sanitize_string(value)
                elif isinstance(value, (list, dict)):
                    # Complex types - sanitize recursively
                    sanitized[key] = DataSanitizer.sanitize_json_data(value)
                else:
                    # Convert other types to string and sanitize
                    sanitized[key] = DataSanitizer._sanitize_string(str(value))
                    
            except MalformedDataException as e:
                logger.warning(f"Parameter {key} sanitization failed: {e}")
                continue
                
        return sanitized

    async def _handle_response_with_fuzz_protection(self, response, attempt: int, max_attempts: int) -> Dict:
        """✅ ENHANCED: Response handling with advanced fuzz protection and anomaly detection"""
        if response.status == 200:
            raw_data = await response.text()
            
            # ✅ ENHANCED: Size check before parsing
            if len(raw_data) > 10 * 1024 * 1024:  # 10MB limit
                self._fuzz_stats['oversized_data_rejected'] += 1
                raise MalformedDataException("Response too large")
                
            try:
                # ✅ ENHANCED: JSON parsing with fuzz protection and anomaly detection
                data = self._safe_json_parse(raw_data)
                
                # ✅ ADDED: Anomaly detection
                anomalies = DataSanitizer.detect_data_anomalies(data)
                if anomalies:
                    logger.warning(f"Data anomalies detected: {anomalies}")
                    self._fuzz_stats['malformed_responses_rejected'] += 1
                
                # ✅ ENHANCED: Structure validation
                if isinstance(data, dict) and 'code' in data and data['code'] != 0:
                    error_code = data['code']
                    await self._handle_binance_error_code(error_code, data.get('msg', ''), attempt, max_attempts)
                
                # ✅ ENHANCED: Deep sanitization with anomaly tracking
                sanitized_data = DataSanitizer.sanitize_json_data(data)
                return sanitized_data
                
            except MalformedDataException as e:
                self._fuzz_stats['malformed_responses_rejected'] += 1
                logger.error(f"🚫 Malformed data rejected: {e}")
                raise
            except json.JSONDecodeError as e:
                self._fuzz_stats['invalid_json_encountered'] += 1
                logger.error(f"❌ Invalid JSON response: {e}")
                raise MalformedDataException("Invalid JSON response")
                
        else:
            error_text = await response.text()
            
            # ✅ ENHANCED: Error response sanitization with security logging
            try:
                sanitized_error = DataSanitizer._sanitize_string(error_text)
            except MalformedDataException:
                sanitized_error = "Malformed error response"
                
            # Log security event for non-200 responses
            await self._log_security_event('WARNING', 'http_response', 
                                         f"Non-200 HTTP response: {response.status}",
                                         {'status': response.status, 'error_preview': sanitized_error[:100]})
            
            if response.status == 429:
                self._dependency_stats['rate_limits'] += 1
                retry_after = response.headers.get('Retry-After', 60)
                logger.warning(f"🚦 Rate limited, retrying after {retry_after}s")
                await asyncio.sleep(int(retry_after))
                raise Exception("Rate limit exceeded")
            elif response.status >= 500:
                self._dependency_stats['server_errors'] += 1
                self._update_dependency_health()
                logger.error(f"🔧 Server error {response.status}, retrying...")
                raise Exception(f"Server error: {response.status}")
            elif response.status == 401:
                self._dependency_stats['auth_failures'] += 1
                logger.error(f"🔐 Authentication failed (HTTP 401)")
                raise Exception("Authentication failed - check API credentials")
            else:
                await self._handle_binance_error_code(None, sanitized_error, attempt, max_attempts)
                raise Exception(f"HTTP {response.status}: {sanitized_error}")

    def _safe_json_parse(self, raw_json: str) -> any:
        """Safe JSON parsing with fuzz protection"""
        try:
            # Check for deeply nested structures
            depth = 0
            for char in raw_json[:10000]:  # Check first 10k chars
                if char == '{' or char == '[':
                    depth += 1
                elif char == '}' or char == ']':
                    depth -= 1
                    
                if depth > 50:  # Maximum depth
                    raise MalformedDataException("JSON structure too deep")
                    
            # Parse with limits
            return json.loads(raw_json)
            
        except (json.JSONDecodeError, RecursionError) as e:
            raise MalformedDataException(f"JSON parsing failed: {e}")

    async def _cleanup_memory(self):
        """Memory leak prevention for long-running clients"""
        try:
            # Clean up old request times
            if len(self._request_times) > 50:
                self._request_times = self._request_times[-40:]
                
            # Clean up old partial orders
            current_time = time.time()
            orders_to_remove = []
            for order_id, order_data in self._partial_orders.items():
                if current_time - order_data.get('timestamp', 0) > 3600:  # 1 hour old
                    orders_to_remove.append(order_id)
                    
            for order_id in orders_to_remove:
                del self._partial_orders[order_id]
                
            if orders_to_remove:
                logger.info(f"🧹 Cleaned up {len(orders_to_remove)} old partial orders")
                
        except Exception as e:
            logger.error(f"Memory cleanup failed: {e}")

    async def _handle_binance_error_code(self, error_code: int, error_msg: str, attempt: int, max_attempts: int):
        """✅ FIXED: Binance-specific error handling with retry logic and CLOCK SKEW RECOVERY"""
        if error_code is None:
            # Generic error, retry based on status code
            if attempt < max_attempts:
                wait_time = 2 ** attempt
                logger.warning(f"🔧 Generic error, retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)
            return
            
        error_info = self._binance_error_codes.get(error_code, ('UNKNOWN', 'fatal', 'Unknown error'))
        error_name, error_type, error_desc = error_info
        
        logger.error(f"📟 Binance error {error_code} ({error_name}): {error_msg} - {error_desc}")
        
        # ✅ ENHANCED: Special handling for timestamp errors with clock sync
        if error_code == -1021:  # INVALID_TIMESTAMP
            logger.warning("🕒 Timestamp error detected, synchronizing time with Binance server...")
            await self._sync_server_time()
            if attempt < max_attempts:
                wait_time = 2 ** attempt
                logger.info(f"🔄 Retrying with synchronized time in {wait_time}s...")
                await asyncio.sleep(wait_time)
                return True  # retry
        
        if error_type == 'recoverable' and attempt < max_attempts:
            wait_time = 2 ** attempt
            logger.warning(f"🔧 Recoverable error, retrying in {wait_time}s...")
            await asyncio.sleep(wait_time)
            return True
        elif error_type == 'fatal':
            logger.critical(f"💀 Fatal Binance error {error_code}, not retrying")
            raise Exception(f"Fatal Binance error {error_code}: {error_msg}")
        else:
            return False

    async def create_order(self, symbol: str, side: str, order_type: str, quantity: float, price: float = None):
        """✅ ENHANCED: Atomic order creation with fuzz protection, brutal error handling, and CLOCK SKEW RESILIENCE"""
        if self._shutdown:
            raise RuntimeError("Client is shutdown")
            
        # ✅ ADDED: Input validation
        if not self._is_valid_symbol(symbol):
            raise MalformedDataException(f"Invalid symbol: {symbol}")
            
        if side not in ['BUY', 'SELL']:
            raise MalformedDataException(f"Invalid side: {side}")
            
        if not isinstance(quantity, (int, float)) or quantity <= 0:
            raise MalformedDataException(f"Invalid quantity: {quantity}")
            
        async with self._order_mutex:
            order_id = None
            attempt = 0
            max_attempts = 3
            
            while attempt < max_attempts:
                try:
                    attempt += 1
                    
                    # ✅ FIXED: Precision handling with Decimal to avoid floating point errors
                    quantity_dec = Decimal(str(quantity)).quantize(Decimal('0.000001'))
                    if price:
                        price_dec = Decimal(str(price)).quantize(Decimal('0.00000001'))
                    
                    dry_run = hasattr(self, 'config') and self.config.get('trading', {}).get('dry_run', True)
                    if dry_run:
                        logger.info(f"📄 DRY RUN: {side} {quantity} {symbol} at {price or 'market'}")
                        return await self._create_test_order(symbol, side, order_type, quantity, price)
                    
                    logger.warning(f"🚀 LIVE ORDER (attempt {attempt}): {side} {quantity} {symbol}")
                    
                    # ✅ ENHANCED: Ensure time synchronization before order creation
                    await self._ensure_time_synchronized()
                    
                    # ✅ FIXED: Use server-adjusted timestamp
                    timestamp = self._get_adjusted_timestamp()
                    params = {
                        'symbol': symbol,
                        'side': side,
                        'type': order_type,
                        'quantity': format(float(quantity_dec), 'f').rstrip('0').rstrip('.'),
                        'timestamp': timestamp,
                        'recvWindow': 15000 if self._clock_sync_failed else 10000,  # ✅ ENHANCED: Larger window if sync failed
                        'newOrderRespType': 'FULL'  # ✅ FIXED: Get full order response for partial execution handling
                    }
                    
                    if price is not None:
                        params['price'] = format(float(price_dec), 'f').rstrip('0').rstrip('.'))
                        
                    order_data = await self._make_request('POST', 'v3/order', params, signed=True)
                    order_id = order_data.get('orderId')
                    
                    await self._handle_order_response_brutal(order_data, symbol)
                    
                    logger.critical(f"✅ LIVE ORDER EXECUTED: {order_id}")
                    return order_data
                    
                except MalformedDataException as e:
                    self._fuzz_stats['malformed_market_data_rejected'] += 1
                    logger.warning(f"Order creation blocked: {e}")
                    raise
                except asyncio.TimeoutError:
                    logger.error(f"❌ Order timeout (attempt {attempt})")
                    if attempt < max_attempts:
                        await asyncio.sleep(2 ** attempt)
                        continue
                    
                    if order_id:
                        recovered = await self._recover_timed_out_order(symbol, order_id)
                        if recovered:
                            return recovered
                    raise
                except Exception as e:
                    logger.error(f"❌ Order creation failed (attempt {attempt}): {e}")
                    if attempt < max_attempts:
                        await asyncio.sleep(2 ** attempt)
                        continue
                    raise
                    
            raise Exception(f"All {max_attempts} order attempts failed")

    async def _handle_order_response_brutal(self, order_data: Dict, symbol: str):
        """✅ FIXED: Comprehensive order response handling with partial execution tracking"""
        if not order_data or 'orderId' not in order_data:
            raise Exception("Invalid order response")
            
        status = order_data.get('status')
        order_id = order_data.get('orderId')
        
        self._partial_orders[order_id] = {
            'symbol': symbol,
            'status': status,
            'executed_qty': float(order_data.get('executedQty', 0)),
            'orig_qty': float(order_data.get('origQty', 0)),
            'side': order_data.get('side'),
            'type': order_data.get('type'),
            'timestamp': time.time(),
            'fills': order_data.get('fills', [])
        }
        
        if status == 'PARTIALLY_FILLED':
            executed = float(order_data.get('executedQty', 0))
            original = float(order_data.get('origQty', 0))
            logger.warning(f"⚠️ PARTIAL EXECUTION: {executed}/{original} {symbol}")
            
            # ✅ FIXED: Partial execution recovery - track for potential cleanup
            asyncio.create_task(self._monitor_partial_order(order_id))
        
        logger.info(f"📋 Order {order_id} - Status: {status} - Symbol: {symbol}")

    async def _monitor_partial_order(self, order_id: int):
        """Monitor partially filled orders for completion or cancellation"""
        try:
            # Monitor for up to 5 minutes
            for _ in range(30):
                await asyncio.sleep(10)
                
                if order_id not in self._partial_orders:
                    break
                    
                order_info = self._partial_orders[order_id]
                current_status = await self.get_order(order_info['symbol'], order_id)
                
                if not current_status or current_status.get('status') in ['FILLED', 'CANCELED', 'EXPIRED']:
                    if order_id in self._partial_orders:
                        del self._partial_orders[order_id]
                    break
                    
        except Exception as e:
            logger.error(f"Partial order monitoring failed for {order_id}: {e}")

    def _is_valid_symbol(self, symbol: str) -> bool:
        """Validate symbol format"""
        if not isinstance(symbol, str):
            return False
            
        if len(symbol) > 20:
            return False
            
        # Basic symbol format validation (adjust based on Binance specs)
        if not re.match(r'^[A-Z0-9]{1,20}$', symbol):
            return False
            
        return True

    async def connect_websocket(self, symbols: list, callback: callable):
        """✅ ENHANCED: WebSocket with fuzz-resistant message handling and robust reconnection"""
        max_reconnect_attempts = 10
        reconnect_delay = 5
        
        for attempt in range(max_reconnect_attempts):
            try:
                # ✅ ADDED: Symbol validation
                valid_symbols = [s for s in symbols[:4] if self._is_valid_symbol(s)]
                streams = [f"{symbol.lower()}@trade" for symbol in valid_symbols]
                
                if not streams:
                    raise MalformedDataException("No valid symbols provided")
                    
                url = f"{self.ws_url}?streams={'/'.join(streams)}"
                
                logger.info(f"🔌 WebSocket attempt {attempt + 1}/{max_reconnect_attempts}")
                
                # ✅ FIXED: Use global SSL context for WebSocket
                async with websockets.connect(
                    url, 
                    ping_interval=30,
                    ping_timeout=10,
                    close_timeout=10,
                    ssl=GLOBAL_SSL_CONTEXT  # ✅ FIXED: Consistent SSL handling
                ) as websocket:
                    
                    logger.info("✅ WebSocket connected successfully")
                    self._last_message_time = time.time()
                    
                    while not self._shutdown:
                        try:
                            message = await asyncio.wait_for(websocket.recv(), timeout=30)
                            self._last_message_time = time.time()
                            
                            # ✅ ADDED: Message size validation
                            if len(message) > 100000:  # 100KB limit
                                logger.warning("Oversized WebSocket message rejected")
                                continue
                                
                            # ✅ ADDED: Safe message processing
                            processed_data = await self._safe_process_websocket_message(message)
                            if processed_data:
                                if asyncio.iscoroutinefunction(callback):
                                    await callback(processed_data)
                                else:
                                    callback(processed_data)
                                    
                        except asyncio.TimeoutError:
                            # ✅ FIXED: Send ping to keep connection alive
                            if time.time() - self._last_message_time > 20:
                                try:
                                    await websocket.ping()
                                    logger.debug("Sent ping to keep connection alive")
                                except Exception:
                                    break
                            continue
                        except websockets.exceptions.ConnectionClosed:
                            logger.warning("WebSocket connection closed")
                            break
                        except MalformedDataException as e:
                            logger.warning(f"Malformed WebSocket message: {e}")
                            continue
                        except Exception as e:
                            logger.error(f"WebSocket message error: {e}")
                            break
                            
            except Exception as e:
                logger.error(f"WebSocket connection failed: {e}")
                if attempt < max_reconnect_attempts - 1:
                    wait_time = min(reconnect_delay * (attempt + 1), 60)
                    logger.info(f"🔄 Reconnecting WebSocket in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("❌ Maximum WebSocket reconnection attempts reached")
                    break

    async def _safe_process_websocket_message(self, message: str) -> Optional[Dict]:
        """Safely process WebSocket message with fuzz protection"""
        try:
            # Parse and sanitize message
            data = self._safe_json_parse(message)
            sanitized_data = DataSanitizer.sanitize_json_data(data)
            
            # Validate required WebSocket structure
            if not isinstance(sanitized_data, dict):
                raise MalformedDataException("WebSocket message must be an object")
                
            if 'stream' not in sanitized_data and 'data' not in sanitized_data:
                raise MalformedDataException("Invalid WebSocket message structure")
                
            return sanitized_data
            
        except MalformedDataException as e:
            self._fuzz_stats['malformed_responses_rejected'] += 1
            logger.debug(f"WebSocket message rejected: {e}")
            return None

    async def close(self):
        """✅ ENHANCED: Async client shutdown with proper cleanup"""
        self._shutdown = True
        await self._session_pool.close_all()
        self._executor.shutdown(wait=True)
        logger.info("✅ Binance client closed completely")

    def _sign_request(self, params: Dict) -> str:
        """Generate HMAC signature for signed requests"""
        query_string = urlencode(params)
        return hmac.new(
            self.api_secret.encode('utf-8'),
            query_string.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()

    async def _handle_retry(self, attempt: int, error_type: str):
        """Handle retry with exponential backoff"""
        wait_time = min(2 ** attempt, 30)
        logger.info(f"🔄 Retrying in {wait_time}s (attempt {attempt}, {error_type})")
        await asyncio.sleep(wait_time)

    async def _recover_timed_out_order(self, symbol: str, order_id: int) -> Optional[Dict]:
        """Recover from order timeout by checking order status"""
        try:
            logger.warning(f"🕵️ Attempting to recover order {order_id} for {symbol}")
            order_status = await self.get_order(symbol, order_id)
            if order_status and order_status.get('orderId') == order_id:
                logger.info(f"✅ Successfully recovered order {order_id}")
                return order_status
            return None
        except Exception as e:
            logger.error(f"❌ Order recovery failed: {e}")
            return None

    async def get_order(self, symbol: str, order_id: int) -> Optional[Dict]:
        """Get order status with fuzz protection"""
        try:
            params = {'symbol': symbol, 'orderId': order_id}
            return await self._make_request('GET', 'v3/order', params, signed=True)
        except Exception as e:
            logger.error(f"Failed to get order {order_id}: {e}")
            return None

    async def _create_test_order(self, symbol: str, side: str, order_type: str, quantity: float, price: float = None) -> Dict:
        """Create test order for dry run mode"""
        return {
            'symbol': symbol,
            'orderId': 999999,
            'clientOrderId': 'test_' + str(int(time.time())),
            'transactTime': int(time.time() * 1000),
            'price': str(price) if price else '0',
            'origQty': str(quantity),
            'executedQty': '0',
            'status': 'TEST',
            'timeInForce': 'GTC',
            'type': order_type,
            'side': side
        }

    async def get_klines(self, symbol: str, interval: str = "1m", limit: int = 100) -> Optional[List]:
        """✅ ENHANCED: Get klines with fuzz protection"""
        try:
            # Validate inputs
            if not self._is_valid_symbol(symbol):
                raise MalformedDataException(f"Invalid symbol: {symbol}")
                
            valid_intervals = ['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M']
            if interval not in valid_intervals:
                raise MalformedDataException(f"Invalid interval: {interval}")
                
            if not isinstance(limit, int) or limit <= 0 or limit > 1000:
                raise MalformedDataException(f"Invalid limit: {limit}")
                
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': min(limit, 1000)  # Enforce maximum limit
            }
            
            return await self._make_request('GET', 'v3/klines', params)
            
        except MalformedDataException as e:
            self._fuzz_stats['malformed_market_data_rejected'] += 1
            logger.warning(f"Klines request blocked: {e}")
            return None
        except Exception as e:
            logger.error(f"Klines request failed: {e}")
            return None

    async def test_connection(self) -> bool:
        """Test connection to Binance API with enhanced failure recovery and CLOCK SYNC"""
        try:
            # First synchronize time
            await self._sync_server_time()
            
            # Then test connection
            result = await self._make_request('GET', 'v3/ping')
            return result == {}  # Binance returns empty object on success
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            
            # ✅ FIXED: Enhanced connection test with fallback strategies
            if "SSL" in str(e):
                logger.warning("🔒 SSL issue detected, trying without certificate verification")
                return await self._test_connection_fallback()
            elif "timeout" in str(e).lower():
                logger.warning("⏰ Timeout detected, trying with shorter timeout")
                return await self._test_connection_fallback()
                
            return False
            
    async def _test_connection_fallback(self) -> bool:
        """Fallback connection test with relaxed settings"""
        try:
            # Create a temporary session with relaxed settings
            timeout = aiohttp.ClientTimeout(total=10, connect=5, sock_connect=5, sock_read=5)
            connector = aiohttp.TCPConnector(ssl=False)  # Disable SSL verification as last resort
            
            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                async with session.get(f"{self.base_url}/v3/ping") as response:
                    return response.status == 200
        except Exception as e:
            logger.error(f"Fallback connection test also failed: {e}")
            return False

    async def _log_security_event(self, level: str, component: str, message: str, details: Dict = None):
        """Log security event with enhanced details"""
        security_logger = logging.getLogger('security')
        log_message = f"[{component}] {message}"
        
        if level == 'CRITICAL':
            security_logger.critical(log_message, extra={'details': details})
        elif level == 'WARNING':
            security_logger.warning(log_message, extra={'details': details})
        else:
            security_logger.info(log_message, extra={'details': details})

    def get_stats(self):
        """Get client statistics"""
        pool_stats = self._session_pool.get_stats()
        return {
            'pool_stats': pool_stats,
            'request_times_count': len(self._request_times),
            'partial_orders_count': len(self._partial_orders),
            'shutdown': self._shutdown,
            'mutex_waiters': {
                'request': self._request_mutex.waiters,
                'order': self._order_mutex.waiters
            },
            'fuzz_protection_stats': self._fuzz_stats.copy(),
            'dependency_stats': self.get_dependency_stats(),
            'clock_sync_stats': {
                'server_time_offset_ms': self._server_time_offset,
                'last_time_sync': self._last_time_sync,
                'sync_attempts': self._dependency_stats['clock_sync_attempts'],
                'sync_failures': self._dependency_stats['clock_sync_failures'],
                'sync_recoveries': self._dependency_stats['clock_sync_recoveries'],
                'clock_sync_failed': self._clock_sync_failed,
                'current_sync_attempts': self._sync_attempts
            }
        }

    def get_fuzz_stats(self) -> Dict:
        """Get fuzz protection statistics"""
        return self._fuzz_stats.copy()

    def get_dependency_stats(self) -> Dict:
        """Get dependency failure statistics"""
        stats = self._dependency_stats.copy()
        stats['health_status'] = 'HEALTHY' if not stats['degraded_mode'] else 'DEGRADED'
        stats['recovery_success_rate'] = (
            stats['successful_recoveries'] / stats['recovery_attempts'] 
            if stats['recovery_attempts'] > 0 else 1.0
        )
        return stats

    def get_clock_sync_status(self) -> Dict:
        """Get clock synchronization status"""
        return {
            'server_time_offset_ms': self._server_time_offset,
            'last_sync_time': self._last_time_sync,
            'time_since_last_sync': time.time() - self._last_time_sync,
            'within_acceptable_skew': abs(self._server_time_offset) <= self._max_clock_skew,
            'sync_attempts': self._dependency_stats['clock_sync_attempts'],
            'sync_failures': self._dependency_stats['clock_sync_failures'],
            'sync_recoveries': self._dependency_stats['clock_sync_recoveries'],
            'clock_sync_failed': self._clock_sync_failed,
            'current_sync_attempts': self._sync_attempts
        }


### [FILE] alerts/monitor.py
import logging
import time
from typing import Dict, List, Any, Optional, Callable
from ..utils.advanced_logger import AdvancedLogger
from ..utils.type_checker import TypeChecker
from .email_notifier import EmailNotifier
from .daily_summary import DailySummary

class AlertMonitor:
    """
    Enhanced alert monitor with robust error handling and malformed data protection.
    Preserves all existing functionality while adding resilience.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.logger = AdvancedLogger(__name__)
        self.type_checker = TypeChecker()
        
        self.config = config
        self.alerts_enabled = config.get('alerts_enabled', True)
        self.alert_thresholds = config.get('alert_thresholds', {})
        self.alert_history = []
        
        # Enhanced alert tracking
        self.alert_count = 0
        self.last_alert_time = 0
        self.consecutive_errors = 0
        
        # Initialize components with error handling
        try:
            self.email_notifier = EmailNotifier(config.get('email_settings', {}))
            self.daily_summary = DailySummary(config.get('summary_settings', {}))
        except Exception as e:
            self.logger.error(f"Alert component initialization failed: {str(e)}")
            self.alerts_enabled = False

    def _safe_validate_alert_data(self, alert_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        NEW: Safely validates alert data with comprehensive error handling.
        Preserves existing data flow while adding robustness.
        """
        validation_result = {
            "is_valid": False,
            "cleaned_data": {},
            "errors": [],
            "warnings": []
        }
        
        try:
            # Null and type checking
            if alert_data is None:
                validation_result["errors"].append("Alert data is None")
                return validation_result
                
            if not self.type_checker.is_dict(alert_data):
                validation_result["errors"].append(f"Alert data must be dict, got {type(alert_data)}")
                return validation_result
            
            # Clean and validate each field
            cleaned_data = {}
            
            # Message field validation
            message = alert_data.get('message')
            if message is not None:
                if self.type_checker.safe_string_check(message, max_length=1000):
                    cleaned_data['message'] = str(message)[:1000]  # Truncate for safety
                else:
                    validation_result["warnings"].append("Invalid message field")
                    cleaned_data['message'] = "Invalid alert message"
            
            # Level field validation
            level = alert_data.get('level', 'info')
            if self.type_checker.safe_string_check(level) and level.lower() in ['info', 'warning', 'error', 'critical']:
                cleaned_data['level'] = level.lower()
            else:
                validation_result["warnings"].append("Invalid level field, using default")
                cleaned_data['level'] = 'info'
            
            # Timestamp validation
            timestamp = alert_data.get('timestamp')
            if timestamp is None:
                cleaned_data['timestamp'] = time.time()
            elif self.type_checker.safe_number_check(timestamp, min_value=0):
                cleaned_data['timestamp'] = float(timestamp)
            else:
                validation_result["warnings"].append("Invalid timestamp, using current time")
                cleaned_data['timestamp'] = time.time()
            
            # Additional data validation
            data = alert_data.get('data', {})
            if self.type_checker.safe_dict_check(data, max_keys=50):
                cleaned_data['data'] = data
            else:
                validation_result["warnings"].append("Invalid data field")
                cleaned_data['data'] = {}
            
            validation_result["cleaned_data"] = cleaned_data
            validation_result["is_valid"] = True
            
        except Exception as e:
            validation_result["errors"].append(f"Alert data validation error: {str(e)}")
            # Provide minimal safe data structure
            validation_result["cleaned_data"] = {
                'message': 'Alert data validation failed',
                'level': 'error',
                'timestamp': time.time(),
                'data': {}
            }
        
        return validation_result

    def send_alert(self, alert_data: Dict[str, Any]) -> bool:
        """
        Enhanced alert method with malformed data protection.
        Preserves existing alert functionality while adding safety.
        """
        if not self.alerts_enabled:
            self.logger.debug("Alerts are disabled")
            return False
        
        # Validate and clean alert data
        validation_result = self._safe_validate_alert_data(alert_data)
        clean_data = validation_result["cleaned_data"]
        
        # Log validation issues
        for error in validation_result["errors"]:
            self.logger.error(f"Alert data error: {error}")
            
        for warning in validation_result["warnings"]:
            self.logger.warning(f"Alert data warning: {warning}")
        
        # Update alert metrics
        self.alert_count += 1
        self.last_alert_time = time.time()
        
        try:
            # Execute alert with error handling
            success = self._execute_alert(clean_data)
            
            if success:
                self.logger.info(f"Alert sent: {clean_data.get('message', 'Unknown')}")
            else:
                self.logger.error(f"Alert failed: {clean_data.get('message', 'Unknown')}")
                self.consecutive_errors += 1
                
            # Store in history with size limit
            self.alert_history.append({
                'timestamp': clean_data['timestamp'],
                'data': clean_data,
                'success': success
            })
            
            # Maintain history size
            if len(self.alert_history) > 1000:
                self.alert_history = self.alert_history[-500:]
            
            return success
            
        except Exception as e:
            self.logger.error(f"Alert execution error: {str(e)}")
            self.consecutive_errors += 1
            return False

    def _execute_alert(self, clean_data: Dict[str, Any]) -> bool:
        """
        Enhanced alert execution with multiple delivery methods.
        Preserves existing notification functionality.
        """
        try:
            message = clean_data.get('message', 'No message')
            level = clean_data.get('level', 'info')
            
            # Log-based alerting (always works)
            if level == 'error' or level == 'critical':
                self.logger.error(f"ALERT: {message}")
            elif level == 'warning':
                self.logger.warning(f"ALERT: {message}")
            else:
                self.logger.info(f"ALERT: {message}")
            
            # Email notifications for critical alerts
            if level in ['error', 'critical'] and hasattr(self, 'email_notifier'):
                try:
                    email_success = self.email_notifier.send_alert(
                        subject=f"Trading Bot Alert - {level.upper()}",
                        message=message,
                        alert_data=clean_data
                    )
                    if not email_success:
                        self.logger.warning("Email notification failed")
                except Exception as e:
                    self.logger.error(f"Email notification error: {str(e)}")
            
            # Daily summary tracking
            if hasattr(self, 'daily_summary'):
                try:
                    self.daily_summary.record_alert(clean_data)
                except Exception as e:
                    self.logger.error(f"Daily summary recording error: {str(e)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Alert execution failed: {str(e)}")
            return False

    # PRESERVED EXISTING METHODS
    def check_price_alert(self, symbol: str, current_price: float, previous_price: float) -> bool:
        """Preserved existing price alert method"""
        if not self.alerts_enabled:
            return False
            
        price_change = abs((current_price - previous_price) / previous_price * 100)
        threshold = self.alert_thresholds.get('price_change_percent', 5.0)
        
        if price_change >= threshold:
            alert_data = {
                'message': f"Significant price change for {symbol}: {price_change:.2f}%",
                'level': 'warning',
                'data': {
                    'symbol': symbol,
                    'current_price': current_price,
                    'previous_price': previous_price,
                    'change_percent': price_change
                }
            }
            return self.send_alert(alert_data)
            
        return False

    def check_volume_alert(self, symbol: str, volume: float, average_volume: float) -> bool:
        """Preserved existing volume alert method"""
        if not self.alerts_enabled:
            return False
            
        volume_ratio = volume / average_volume if average_volume > 0 else 0
        threshold = self.alert_thresholds.get('volume_ratio', 3.0)
        
        if volume_ratio >= threshold:
            alert_data = {
                'message': f"Unusual volume for {symbol}: {volume_ratio:.2f}x average",
                'level': 'info',
                'data': {
                    'symbol': symbol,
                    'volume': volume,
                    'average_volume': average_volume,
                    'volume_ratio': volume_ratio
                }
            }
            return self.send_alert(alert_data)
            
        return False

    def check_error_alert(self, error_message: str, component: str = "unknown") -> bool:
        """Preserved existing error alert method"""
        if not self.alerts_enabled:
            return False
            
        alert_data = {
            'message': f"Error in {component}: {error_message}",
            'level': 'error',
            'data': {
                'component': component,
                'error_message': error_message
            }
        }
        return self.send_alert(alert_data)

    def get_alert_stats(self) -> Dict[str, Any]:
        """Preserved existing stats method"""
        return {
            'total_alerts': self.alert_count,
            'last_alert_time': self.last_alert_time,
            'consecutive_errors': self.consecutive_errors,
            'alerts_enabled': self.alerts_enabled
        }

    # NEW: Enhanced monitoring methods
    def test_malformed_alert_handling(self, test_alerts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        NEW METHOD: Tests the alert system's handling of malformed alert data.
        This method is added for robustness testing and does not affect
        existing functionality.
        """
        self.logger.info("Testing malformed alert handling")
        
        results = {
            "total_tests": len(test_alerts),
            "successful_alerts": 0,
            "failed_alerts": 0,
            "details": []
        }
        
        for i, test_alert in enumerate(test_alerts):
            try:
                success = self.send_alert(test_alert)
                results["details"].append({
                    "test_case": i + 1,
                    "success": success,
                    "alert_data": test_alert
                })
                
                if success:
                    results["successful_alerts"] += 1
                else:
                    results["failed_alerts"] += 1
                    
            except Exception as e:
                results["details"].append({
                    "test_case": i + 1,
                    "success": False,
                    "error": str(e),
                    "alert_data": test_alert
                })
                results["failed_alerts"] += 1
        
        return results

    def get_health_status(self) -> Dict[str, Any]:
        """
        NEW METHOD: Provides health status of the alert monitoring system.
        This method adds monitoring capabilities without modifying
        existing functionality.
        """
        return {
            "alerts_enabled": self.alerts_enabled,
            "total_alerts_sent": self.alert_count,
            "consecutive_errors": self.consecutive_errors,
            "alert_history_size": len(self.alert_history),
            "components_healthy": all([
                hasattr(self, 'email_notifier'),
                hasattr(self, 'daily_summary')
            ])
        }

# PRESERVED EXISTING FUNCTIONS
def create_alert_monitor(config: Dict[str, Any]) -> AlertMonitor:
    """Preserved existing factory function"""
    return AlertMonitor(config)

class AlertPriority:
    """Preserved existing priority class"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


### [FILE] app/bootstrap.py
import os
import sys
import logging
import asyncio
import signal
import time
from datetime import datetime
import threading

# Add project root to Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    from binance_api.binance_client import BinanceClient
    from utils.advanced_logger import TradingLogger
    from monitoring.free_tier_guard import FreeTierGuard
    from utils.config_loader import load_config
    from ml.chaos_module import LightChaos
    from core.portfolio_manager import PortfolioManager
    from scripts.schedule_manager import get_scheduler
    import yaml
except ImportError as e:
    print(f"❌ Import error: {e}")
    sys.exit(1)

class QuantumBotBootstrap:
    def __init__(self):
        self.config = load_config()
        self.live_trading = self.config.get('trading', {}).get('live_trading', False)
        self.logger = TradingLogger(free_tier=True, live_trading=self.live_trading)
        self.running = False
        self.client = None
        self.chaos_module = None
        self.portfolio_manager = None
        self.scheduler = get_scheduler()
        
    def setup_logging(self):
        """Configure logging for live trading"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/live_trading.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
    def setup_signal_handlers(self):
        """Handle graceful shutdown"""
        def signal_handler(sig, frame):
            self.logger.critical(f"🛑 Emergency shutdown signal {sig} received!")
            self.running = False
            self._cleanup()
                
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
    def _cleanup(self):
        """Cleanup resources"""
        self.logger.info("🧹 Cleaning up resources...")
        if self.client:
            asyncio.run(self.client.__aexit__(None, None, None))
        
    async def initialize_components(self):
        """Initialize all components"""
        try:
            self.logger.info("Initializing Quantum Trading Bot...")
            
            # Initialize Binance client
            self.client = BinanceClient(
                api_key=os.getenv('BINANCE_API_KEY', 'test'),
                api_secret=os.getenv('BINANCE_API_SECRET', 'test'),
                testnet=not self.live_trading
            )
            
            # Test connection
            if not await self.client.test_connection():
                self.logger.error("❌ Failed to connect to Binance API")
                return False
                
            mode = "LIVE" if self.live_trading else "TESTNET"
            self.logger.info(f"✅ Connected to Binance {mode} API")
            
            # Initialize components
            self.chaos_module = LightChaos(self.config)
            self.portfolio_manager = PortfolioManager(self.client, self.config)
            
            # Log trading schedule
            uptime_info = self.scheduler.get_uptime_estimate()
            windows = uptime_info.get('trading_windows', [])
            self.logger.info(f"📅 Configured trading windows: {windows}")
            
            # Initialize Free Tier Guard
            guard = FreeTierGuard(self.config)
            guard.register_reduction_action(self.reduce_workload)
            asyncio.create_task(guard.monitor())
            
            self.logger.info("✅ All components initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"❌ Component initialization failed: {e}")
            return False
            
    async def reduce_workload(self, memory_pressure, cpu_pressure, level):
        """Workload reduction callback"""
        action = "AGGRESSIVE" if level >= 2 else "CONSERVATIVE"
        self.logger.warning(f"🔄 {action} workload reduction activated")
            
    async def execute_trade(self, symbol, signal, confidence, current_price):
        """Execute trade based on signal - with schedule check"""
        # Check if we should trade based on schedule
        if not self.scheduler.trading_enabled:
            uptime_info = self.scheduler.get_uptime_estimate()
            current_hour = uptime_info.get('current_utc_hour', 0)
            self.logger.info(f"💤 Trading paused (outside schedule): {current_hour:02d}:00 UTC")
            return False
            
        if not self.live_trading:
            self.logger.info(f"📄 PAPER: Would trade {symbol} | {signal} @ ${current_price:,.2f}")
            return True
            
        try:
            # Calculate position size
            balance = await self.portfolio_manager.get_balance("USDC")
            quantity = self.portfolio_manager.calculate_position_size(current_price, balance)
            
            if quantity <= 0:
                self.logger.warning("Insufficient balance for trade")
                return False
                
            # Validate order
            if not await self.portfolio_manager.validate_order(symbol, quantity, current_price):
                self.logger.warning("Order validation failed")
                return False
                
            # Execute order
            side = "BUY" if "BULLISH" in signal.upper() else "SELL"
            order = await self.client.create_order(
                symbol=symbol,
                side=side,
                type='MARKET',
                quantity=quantity
            )
            
            # Log execution
            self.logger.log_order_execution(symbol, side, quantity, current_price, order['orderId'])
            self.logger.log_trade({
                'symbol': symbol, 'side': side, 'quantity': quantity,
                'price': current_price, 'profit': 0.0, 'order_id': order['orderId']
            })
            
            return True
            
        except Exception as e:
            self.logger.error(f"❌ Trade execution failed: {e}")
            return False
            
    async def run_trading_cycle(self, cycle_count):
        """Execute one trading cycle with schedule awareness"""
        # Check schedule first
        if not self.scheduler.should_run():
            uptime_info = self.scheduler.get_uptime_estimate()
            
            if uptime_info['daily_used'] >= 6:
                self.logger.info(f"⏰ Daily limit reached: {uptime_info['daily_used']}/6 hours")
                await asyncio.sleep(300)  # Sleep 5 minutes before checking again
                return
            else:
                current_hour = uptime_info.get('current_utc_hour', 0)
                remaining = uptime_info.get('current_window_remaining', 0)
                self.logger.info(f"😴 Outside trading hours: {current_hour:02d}:00 UTC (next window in {remaining}h)")
                await asyncio.sleep(60)
                return
                
        self.logger.info(f"=== Trading Cycle {cycle_count} ===")
        
        symbols = self.config.get('symbols', ['BTCUSDC', 'ETHUSDC', 'BNBUSDC'])
        
        for symbol in symbols:
            try:
                # Get market data
                klines = await self.client.get_klines(symbol, interval="1m", limit=20)
                if not klines:
                    continue
                    
                prices = [float(kline[4]) for kline in klines]
                current_price = prices[-1]
                
                # Analyze market condition
                market_state = self.chaos_module.update(prices)
                confidence = 0.7 + (cycle_count % 10) * 0.03
                
                # Log analysis
                mode = "🚀 LIVE" if self.live_trading else "📄 PAPER"
                self.logger.info(f"{mode} {symbol}: ${current_price:,.2f} | Signal: {market_state} | Conf: {confidence:.2f}")
                
                # Execute trade if conditions met
                if confidence > 0.75 and "BULLISH" in market_state.upper():
                    await self.execute_trade(symbol, market_state, confidence, current_price)
                    
            except Exception as e:
                self.logger.error(f"Symbol {symbol} analysis failed: {e}")
                
    async def run_bot(self):
        """Main trading execution loop with configured schedule"""
        self.running = True
        
        # Start scheduler in separate thread
        scheduler_thread = threading.Thread(target=self.scheduler.run_scheduler, daemon=True)
        scheduler_thread.start()
        
        mode = "LIVE" if self.live_trading else "PAPER"
        self.logger.info(f"🚀 Starting Quantum Trading Bot - {mode} MODE")
        
        # Log schedule information
        uptime_info = self.scheduler.get_uptime_estimate()
        windows = uptime_info.get('trading_windows', [])
        window_str = ", ".join([f"{start:02d}:00-{end:02d}:00 UTC" for start, end in windows])
        self.logger.info(f"⏰ Trading Schedule: {window_str}")
        self.logger.info(f"📊 Daily Limit: 6 hours maximum")
        self.logger.info(f"📍 AWS Region: ap-southeast-1 (Singapore)")
        self.logger.info(f"💻 Instance: t2.micro (Free Tier)")
        
        cycle_count = 0
        while self.running:
            try:
                cycle_count += 1
                await self.run_trading_cycle(cycle_count)
                
                # Update scheduler usage
                self.scheduler.update_usage()
                
                # Resource monitoring
                if cycle_count % 10 == 0:
                    uptime_info = self.scheduler.get_uptime_estimate()
                    used = uptime_info['daily_used']
                    remaining = uptime_info['daily_remaining']
                    self.logger.info(f"🔍 Daily Usage: {used:.1f}h used, {remaining:.1f}h remaining")
                    
                sleep_time = self.config.get('performance', {}).get('polling_interval', 5)
                await asyncio.sleep(sleep_time)
                    
            except Exception as e:
                self.logger.error(f"Trading cycle error: {e}")
                await asyncio.sleep(10)
                
    async def shutdown(self):
        """Graceful shutdown"""
        self.running = False
        self.logger.info("🛑 Shutting down...")
        await asyncio.sleep(1)
        
async def main():
    bootstrap = QuantumBotBootstrap()
    bootstrap.setup_logging()
    bootstrap.setup_signal_handlers()
    
    if await bootstrap.initialize_components():
        await bootstrap.run_bot()
    else:
        bootstrap.logger.error("❌ Failed to initialize components")
        
    await bootstrap.shutdown()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n🛑 Trading bot stopped by user")
    except Exception as e:
        logging.error(f"❌ Fatal error: {e}")
        sys.exit(1)

### [FILE] alerts/email_notifier.py

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class EmailNotifier:
    def __init__(self, sender, app_password, recipient):
        self.sender = sender
        self.password = app_password
        self.recipient = recipient

    def send(self, subject, message):
        try:
            msg = MIMEMultipart()
            msg["From"] = self.sender
            msg["To"] = self.recipient
            msg["Subject"] = subject

            msg.attach(MIMEText(message, "plain"))

            with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
                server.login(self.sender, self.password)
                server.sendmail(self.sender, self.recipient, msg.as_string())
                print(f"[EMAIL SENT] {subject}")
        except Exception as e:
            print(f"[EMAIL ERROR] {e}")


### [FILE] alerts/daily_summary.py

import sys
import os
import json
import gzip  # ✅ ADDED: Missing import
from datetime import datetime
import logging
import numpy as np

logger = logging.getLogger('daily_summary')

LOG_PATH = "logs/trades.json.gz"

def load_trade_stats():
    """Load and analyze trade statistics"""
    if not os.path.exists(LOG_PATH):
        logger.warning("Trade log file not found")
        return {"trades": 0, "wins": 0, "losses": 0, "pnl": 0.0}

    try:
        # ✅ FIXED: Proper gzip file handling
        with gzip.open(LOG_PATH, 'rt') as f:
            trades = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError, OSError) as e:
        logger.error(f"Failed to load trade log: {e}")
        trades = []

    # Analyze trades
    wins = sum(1 for t in trades if t.get("status") == "win")
    losses = sum(1 for t in trades if t.get("status") == "loss")
    pnl = sum(t.get("profit", 0.0) for t in trades)

    return {
        "trades": len(trades),
        "wins": wins,
        "losses": losses,
        "pnl": pnl
    }

def send_daily_update():
    """Send daily trading summary"""
    try:
        now = datetime.now().strftime("%Y-%m-%d %H:%M")
        stats = load_trade_stats()
        
        # ✅ FIXED: Safe win rate calculation
        total_trades = stats["trades"]
        if total_trades > 0:
            win_rate = (stats["wins"] / total_trades) * 100
        else:
            win_rate = 0

        subject = f"📊 Daily QuantumBot Report - {now}"
        body = f"""
📅 Date: {now}
🤖 Mode: {'LIVE' if os.getenv('TRADING_MODE') == 'live' else 'PAPER'}
📈 Tracked Pairs: {len(os.getenv('SYMBOLS', '4').split(','))}
💹 Trades Today: {stats["trades"]}
✅ Wins: {stats["wins"]}
❌ Losses: {stats["losses"]}
🎯 Win Rate: {win_rate:.2f}%
💰 Profit/Loss: ${stats["pnl"]:.2f}

QuantumBot – Stay Quantum ⚛️
        """
        
        # ✅ FIXED: Use email notifier
        from alerts.email_notifier import EmailNotifier
        
        # Get email credentials from environment
        sender = os.getenv('EMAIL_SENDER')
        password = os.getenv('EMAIL_APP_PASSWORD')
        recipient = os.getenv('EMAIL_RECIPIENT')
        
        if all([sender, password, recipient]):
            notifier = EmailNotifier(sender, password, recipient)
            notifier.send(subject, body)
            logger.info("✅ Daily report sent successfully")
        else:
            logger.warning("Email credentials not configured, skipping email send")
            print(f"DAILY REPORT:\n{body}")
            
    except Exception as e:
        logger.error(f"Daily update failed: {e}")

def generate_performance_report(days=30):
    """Generate comprehensive performance report"""
    try:
        if not os.path.exists(LOG_PATH):
            return {"error": "No trade data available"}
        
        with gzip.open(LOG_PATH, 'rt') as f:
            all_trades = json.load(f)
        
        # Filter trades for the specified period
        cutoff_date = datetime.now().timestamp() - (days * 24 * 3600)
        recent_trades = [t for t in all_trades if t.get('timestamp', 0) > cutoff_date]
        
        if not recent_trades:
            return {"error": f"No trades in last {days} days"}
        
        # Calculate advanced metrics
        winning_trades = [t for t in recent_trades if t.get('profit', 0) > 0]
        losing_trades = [t for t in recent_trades if t.get('profit', 0) <= 0]
        
        total_profit = sum(t.get('profit', 0) for t in recent_trades)
        avg_win = np.mean([t.get('profit', 0) for t in winning_trades]) if winning_trades else 0
        avg_loss = np.mean([t.get('profit', 0) for t in losing_trades]) if losing_trades else 0
        
        # Sharpe ratio (simplified)
        returns = [t.get('profit', 0) / 1000 for t in recent_trades]  # Normalize
        sharpe = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
        
        return {
            'period_days': days,
            'total_trades': len(recent_trades),
            'winning_trades': len(winning_trades),
            'losing_trades': len(losing_trades),
            'win_rate': (len(winning_trades) / len(recent_trades)) * 100,
            'total_profit': total_profit,
            'avg_profit_per_trade': total_profit / len(recent_trades),
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'profit_factor': abs(avg_win / avg_loss) if avg_loss != 0 else float('inf'),
            'sharpe_ratio': sharpe,
            'best_trade': max([t.get('profit', 0) for t in recent_trades]) if recent_trades else 0,
            'worst_trade': min([t.get('profit', 0) for t in recent_trades]) if recent_trades else 0
        }
        
    except Exception as e:
        logger.error(f"Performance report generation failed: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    send_daily_update()
    
    # Generate 30-day performance report
    report = generate_performance_report(30)
    print("\n30-DAY PERFORMANCE REPORT:")
    print(json.dumps(report, indent=2))


### [FILE] alerts/email/send_email.py

import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from dotenv import load_dotenv
import time
from config_secrets import BINANCE_API_KEY, BINANCE_API_SECRET

load_dotenv()

class EmailNotifier:
    def __init__(self):
        self.sender = os.getenv('EMAIL_SENDER')
        self.password = os.getenv('EMAIL_APP_PASSWORD')
        self.recipient = os.getenv('EMAIL_RECIPIENT')
        
    async def send(self, subject: str, body: str) -> bool:
        msg = MIMEMultipart()
        msg['From'] = self.sender
        msg['To'] = self.recipient
        msg['Subject'] = subject
        msg.attach(MIMEText(body, 'plain'))
        
        for attempt in range(3):
            try:
                with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
                    server.login(self.sender, self.password)
                    server.send_message(msg)
                    return True
            except Exception as e:
                print(f"Email send failed (attempt {attempt+1}): {str(e)}")
                time.sleep(2 ** attempt)  # Exponential backoff
        return False

# Example usage:
# notifier = EmailNotifier()
# await notifier.send("Test", "This is a test message")


### [FILE] utils/type_checker.py
import inspect
from typing import Any, Type

class TypeChecker:
    @staticmethod
    def enforce_interface(obj: Any, interface: Type) -> None:
        """Strict interface validation with type hints"""
        required_methods = {
            name: method
            for name, method in inspect.getmembers(interface, inspect.isfunction)
            if not name.startswith('_')
        }
        
        missing_methods = []
        for name, method in required_methods.items():
            if not hasattr(obj, name):
                missing_methods.append(name)
                continue
                
            # Check method signatures
            obj_method = getattr(obj, name)
            if not callable(obj_method):
                missing_methods.append(name)
                continue
                
            # Basic signature check (can be enhanced with full signature comparison)
            obj_sig = inspect.signature(obj_method)
            interface_sig = inspect.signature(method)
            
            # Simple parameter count check
            if len(obj_sig.parameters) != len(interface_sig.parameters):
                raise TypeError(
                    f"Method {name} parameter count mismatch. "
                    f"Expected {len(interface_sig.parameters)}, got {len(obj_sig.parameters)}"
                )
        
        if missing_methods:
            raise ValueError(
                f"Object missing required methods: {', '.join(missing_methods)}"
            )

    @staticmethod
    def enforce_chaos_module(obj: Any) -> None:
        """Validate chaos module interface"""
        required_attrs = ['health_score', 'update', 'get_metrics']
        for attr in required_attrs:
            if not hasattr(obj, attr):
                raise ValueError(f"Chaos module missing required attribute: {attr}")

    @staticmethod
    def enforce_trainer(obj: Any) -> None:
        """Validate trainer interface"""
        required_methods = ['record_trade', 'get_testnet_symbols']
        for method in required_methods:
            if not hasattr(obj, method) or not callable(getattr(obj, method)):
                raise ValueError(f"Trainer missing required method: {method}")

### [FILE] core/pair_manager.py
import logging
from typing import Dict, List, Optional, Any, Set
from utils.advanced_logger import AdvancedLogger
from utils.config_loader import ConfigLoader

class PairManager:
    """
    Enhanced pair manager with dynamic bridge currency support
    Handles USDC trading pairs and symbol validation
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = AdvancedLogger(__name__)
        
        # === CRITICAL: Use configured bridge currency, not hardcoded USDT ===
        self.bridge_currency = self.config.get('trading', {}).get('bridge_currency', 'USDC')
        self.supported_coins = self.config.get('trading', {}).get('supported_coin_list', [])
        
        # Initialize symbols
        self.symbols: List[str] = []
        self.active_symbols: Set[str] = set()
        self.symbol_filters: Dict[str, Dict] = {}
        
        self._initialize_symbols()
        self.logger.info(f"PairManager initialized with bridge: {self.bridge_currency}")

    def _initialize_symbols(self):
        """Initialize trading symbols based on configured coins and bridge currency"""
        self.symbols = []
        
        for coin in self.supported_coins:
            if coin != self.bridge_currency:
                symbol = f"{coin}{self.bridge_currency}"
                self.symbols.append(symbol)
                
        self.logger.info(f"Generated {len(self.symbols)} symbols: {self.symbols}")

    def set_active_symbols(self, symbols: List[str]):
        """Set active trading symbols with validation"""
        valid_symbols = []
        
        for symbol in symbols:
            if symbol in self.symbols:
                valid_symbols.append(symbol)
            else:
                self.logger.warning(f"Symbol {symbol} not in supported symbols list")
                
        self.active_symbols = set(valid_symbols)
        self.logger.info(f"Set {len(self.active_symbols)} active symbols: {list(self.active_symbols)}")

    def get_all_symbols(self) -> List[str]:
        """Get all supported symbols"""
        return self.symbols.copy()

    def get_active_symbols(self) -> List[str]:
        """Get active trading symbols"""
        return list(self.active_symbols)

    def is_symbol_supported(self, symbol: str) -> bool:
        """Check if symbol is supported"""
        return symbol in self.symbols

    def is_symbol_active(self, symbol: str) -> bool:
        """Check if symbol is active for trading"""
        return symbol in self.active_symbols

    def activate_symbol(self, symbol: str) -> bool:
        """Activate a symbol for trading"""
        if symbol not in self.symbols:
            self.logger.warning(f"Cannot activate unsupported symbol: {symbol}")
            return False
            
        self.active_symbols.add(symbol)
        self.logger.info(f"Activated symbol: {symbol}")
        return True

    def deactivate_symbol(self, symbol: str) -> bool:
        """Deactivate a symbol for trading"""
        if symbol in self.active_symbols:
            self.active_symbols.remove(symbol)
            self.logger.info(f"Deactivated symbol: {symbol}")
            return True
        return False

    def get_quote_asset(self, symbol: str) -> str:
        """Get quote asset for symbol"""
        # For USDC pairs, quote asset is USDC
        if symbol.endswith(self.bridge_currency):
            return self.bridge_currency
            
        # Fallback: try to extract quote asset
        for asset in [self.bridge_currency, 'USDT', 'BUSD', 'BTC', 'ETH']:
            if symbol.endswith(asset):
                return asset
                
        self.logger.warning(f"Could not determine quote asset for {symbol}")
        return 'UNKNOWN'

    def get_base_asset(self, symbol: str) -> str:
        """Get base asset for symbol"""
        quote_asset = self.get_quote_asset(symbol)
        if quote_asset != 'UNKNOWN':
            return symbol[:-len(quote_asset)]
        return 'UNKNOWN'

    def validate_symbol_format(self, symbol: str) -> bool:
        """Validate symbol format"""
        if not symbol or len(symbol) < 3:
            return False
            
        # Check if symbol ends with bridge currency
        if not symbol.endswith(self.bridge_currency):
            return False
            
        base_asset = symbol[:-len(self.bridge_currency)]
        if not base_asset or base_asset not in self.supported_coins:
            return False
            
        return True

    def get_trading_pairs_by_volume(self, api_client, limit: int = 10) -> List[str]:
        """Get top trading pairs by volume"""
        try:
            # This would typically call Binance API to get 24hr volume
            # For now, return active symbols sorted by configured priority
            active_symbols = list(self.active_symbols)
            
            # Priority: BTC, ETH, BNB first
            priority_order = ['BTC', 'ETH', 'BNB', 'SOL', 'XRP', 'ADA', 'DOGE', 'DOT']
            
            def symbol_priority(symbol):
                base = self.get_base_asset(symbol)
                try:
                    return priority_order.index(base)
                except ValueError:
                    return len(priority_order)
                    
            active_symbols.sort(key=symbol_priority)
            return active_symbols[:limit]
            
        except Exception as e:
            self.logger.error(f"Error getting trading pairs by volume: {e}")
            return list(self.active_symbols)[:limit]

    def update_symbol_filters(self, api_client, symbols: List[str] = None):
        """Update symbol filters from exchange"""
        try:
            symbols_to_update = symbols or self.symbols
            
            for symbol in symbols_to_update:
                try:
                    symbol_info = api_client.get_symbol_info(symbol)
                    if symbol_info:
                        filters = {}
                        for f in symbol_info['filters']:
                            filters[f['filterType']] = f
                        self.symbol_filters[symbol] = filters
                        
                except Exception as e:
                    self.logger.warning(f"Could not get filters for {symbol}: {e}")
                    
            self.logger.info(f"Updated filters for {len(self.symbol_filters)} symbols")
            
        except Exception as e:
            self.logger.error(f"Error updating symbol filters: {e}")

    def validate_order_parameters(self, symbol: str, quantity: float, price: float = None) -> Dict[str, Any]:
        """Validate order parameters against symbol filters"""
        result = {
            'valid': True,
            'errors': [],
            'adjusted_quantity': quantity,
            'adjusted_price': price
        }
        
        if symbol not in self.symbol_filters:
            result['errors'].append(f"No filters available for {symbol}")
            result['valid'] = False
            return result
            
        filters = self.symbol_filters[symbol]
        
        # Validate LOT_SIZE filter
        if 'LOT_SIZE' in filters:
            lot_filter = filters['LOT_SIZE']
            min_qty = float(lot_filter['minQty'])
            max_qty = float(lot_filter['maxQty'])
            step_size = float(lot_filter['stepSize'])
            
            if quantity < min_qty:
                result['errors'].append(f"Quantity {quantity} below minimum {min_qty}")
                result['valid'] = False
            elif quantity > max_qty:
                result['errors'].append(f"Quantity {quantity} above maximum {max_qty}")
                result['valid'] = False
                
            # Adjust to step size
            if step_size > 0:
                adjusted = round(quantity / step_size) * step_size
                result['adjusted_quantity'] = adjusted
                
        # Validate PRICE_FILTER if price is provided
        if price is not None and 'PRICE_FILTER' in filters:
            price_filter = filters['PRICE_FILTER']
            min_price = float(price_filter['minPrice'])
            max_price = float(price_filter['maxPrice'])
            tick_size = float(price_filter['tickSize'])
            
            if price < min_price:
                result['errors'].append(f"Price {price} below minimum {min_price}")
                result['valid'] = False
            elif price > max_price:
                result['errors'].append(f"Price {price} above maximum {max_price}")
                result['valid'] = False
                
            # Adjust to tick size
            if tick_size > 0:
                adjusted = round(price / tick_size) * tick_size
                result['adjusted_price'] = adjusted
                
        return result

    def get_symbol_statistics(self, api_client, symbols: List[str] = None) -> Dict[str, Any]:
        """Get statistics for symbols"""
        symbols_to_check = symbols or self.active_symbols
        statistics = {}
        
        try:
            for symbol in symbols_to_check:
                try:
                    ticker = api_client.get_24hr_ticker(symbol)
                    statistics[symbol] = {
                        'price_change': float(ticker.get('priceChange', 0)),
                        'price_change_percent': float(ticker.get('priceChangePercent', 0)),
                        'volume': float(ticker.get('volume', 0)),
                        'high_price': float(ticker.get('highPrice', 0)),
                        'low_price': float(ticker.get('lowPrice', 0)),
                        'last_price': float(ticker.get('lastPrice', 0))
                    }
                except Exception as e:
                    self.logger.warning(f"Could not get statistics for {symbol}: {e}")
                    
            self.logger.info(f"Retrieved statistics for {len(statistics)} symbols")
            return statistics
            
        except Exception as e:
            self.logger.error(f"Error getting symbol statistics: {e}")
            return {}

    def suggest_symbols_for_trading(self, api_client, max_symbols: int = 5) -> List[str]:
        """Suggest symbols for trading based on volume and volatility"""
        try:
            statistics = self.get_symbol_statistics(api_client, self.active_symbols)
            
            # Score symbols based on volume and volatility
            scored_symbols = []
            for symbol, stats in statistics.items():
                volume_score = stats['volume']
                volatility_score = abs(stats['price_change_percent'])
                total_score = volume_score * (1 + volatility_score * 0.1)  # Prefer volatile symbols
                
                scored_symbols.append((symbol, total_score))
                
            # Sort by score descending
            scored_symbols.sort(key=lambda x: x[1], reverse=True)
            
            suggested = [symbol for symbol, score in scored_symbols[:max_symbols]]
            self.logger.info(f"Suggested symbols for trading: {suggested}")
            return suggested
            
        except Exception as e:
            self.logger.error(f"Error suggesting symbols: {e}")
            return list(self.active_symbols)[:max_symbols]

    def get_correlation_matrix(self, api_client, symbols: List[str] = None, 
                             period: str = '1d', limit: int = 100) -> Dict[str, Dict[str, float]]:
        """Calculate correlation matrix between symbols"""
        symbols_to_analyze = symbols or self.active_symbols
        correlation_matrix = {}
        
        try:
            # This would typically involve fetching historical data and calculating correlations
            # For now, return a simple implementation
            prices = {}
            
            for symbol in symbols_to_analyze:
                try:
                    klines = api_client.get_klines(symbol, period, limit)
                    close_prices = [float(k[4]) for k in klines]
                    prices[symbol] = close_prices
                except Exception as e:
                    self.logger.warning(f"Could not get prices for {symbol}: {e}")
                    
            # Calculate correlations
            for sym1 in prices:
                correlation_matrix[sym1] = {}
                for sym2 in prices:
                    if len(prices[sym1]) == len(prices[sym2]) and len(prices[sym1]) > 1:
                        corr = self._calculate_correlation(prices[sym1], prices[sym2])
                        correlation_matrix[sym1][sym2] = corr
                    else:
                        correlation_matrix[sym1][sym2] = 0.0
                        
            self.logger.info(f"Calculated correlation matrix for {len(correlation_matrix)} symbols")
            return correlation_matrix
            
        except Exception as e:
            self.logger.error(f"Error calculating correlation matrix: {e}")
            return {}

    def _calculate_correlation(self, prices1: List[float], prices2: List[float]) -> float:
        """Calculate correlation between two price series"""
        if len(prices1) != len(prices2) or len(prices1) < 2:
            return 0.0
            
        try:
            returns1 = [prices1[i] / prices1[i-1] - 1 for i in range(1, len(prices1))]
            returns2 = [prices2[i] / prices2[i-1] - 1 for i in range(1, len(prices2))]
            
            mean1 = sum(returns1) / len(returns1)
            mean2 = sum(returns2) / len(returns2)
            
            covariance = sum((r1 - mean1) * (r2 - mean2) for r1, r2 in zip(returns1, returns2))
            variance1 = sum((r1 - mean1) ** 2 for r1 in returns1)
            variance2 = sum((r2 - mean2) ** 2 for r2 in returns2)
            
            if variance1 == 0 or variance2 == 0:
                return 0.0
                
            correlation = covariance / (variance1 ** 0.5 * variance2 ** 0.5)
            return correlation
            
        except:
            return 0.0


### [FILE] optimizations/vectorized_math.py
import numpy as np
from numba import jit

class VectorizedArbMath:
    @staticmethod
    @jit(nopython=True)
    def calculate_all_paths(bid_ask_matrix: np.ndarray, fee: float = 0.001) -> np.ndarray:
        """
        Vectorized calculation of all possible triangular paths
        Returns: 3D numpy array of profit percentages
        """
        n = bid_ask_matrix.shape[0]
        profits = np.zeros((n, n, n))
        fee_factor = (1 - fee) ** 3
        
        for i in range(n):
            for j in range(n):
                for k in range(n):
                    if i != j and j != k and i != k:
                        # ✅ FIXED: Correct triangular arbitrage formula
                        # Path: Currency1 -> Currency2 -> Currency3 -> Currency1
                        # Buy at ask, sell at bid, buy at ask
                        profits[i,j,k] = (
                            (1 / bid_ask_matrix[i, 1]) *  # Buy first (ask)
                            bid_ask_matrix[j, 0] *        # Sell second (bid)  
                            (1 / bid_ask_matrix[k, 1]) *  # Buy third (ask)
                            bid_ask_matrix[i, 0] *        # Sell back to first (bid)
                            fee_factor - 1
                        )
        return profits

    @staticmethod
    @jit(nopython=True)
    def get_top_paths(profits: np.ndarray, threshold: float = 0.001) -> np.ndarray:
        """Returns indices of profitable paths above threshold"""
        profitable = np.argwhere(profits > threshold)
        # Sort by profit descending
        if len(profitable) > 0:
            profits_values = profits[profitable[:,0], profitable[:,1], profitable[:,2]]
            sorted_indices = np.argsort(-profits_values)
            return profitable[sorted_indices]
        return profitable

### [FILE] optimizations/async_executor.py
import asyncio
from custom_types import List, Tuple
import numpy as np
from custom_collections import deque

class AsyncArbExecutor:
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.latency_stats = deque(maxlen=1000)
        
    async def execute_arbitrage(self, path: Tuple[str, str, str], bid_ask_data: dict) -> float:
        """Execute single arbitrage path with rate limiting"""
        async with self.semaphore:
            start_time = asyncio.get_event_loop().time()
            
            try:
                # Simulate order execution
                a, b, c = path
                profit = (
                    bid_ask_data[a]['bid'] *
                    (1 / bid_ask_data[b]['ask']) *
                    bid_ask_data[c]['bid'] - 1
                )
                
                # Simulate network latency (50-150ms)
                await asyncio.sleep(np.random.uniform(0.05, 0.15))
                
                return max(0, profit)  # Only return positive profits
                
            finally:
                latency = (asyncio.get_event_loop().time() - start_time) * 1000
                self.latency_stats.append(latency)

    async def batch_execute(self, paths: List[Tuple[str, str, str]], bid_ask_data: dict) -> List[float]:
        """Process multiple paths concurrently"""
        tasks = [self.execute_arbitrage(p, bid_ask_data) for p in paths]
        return await asyncio.gather(*tasks, return_exceptions=True)

### [FILE] monitoring/aws_watchdog.py
import psutil
import gc

class FreeTierGuard:
    def __init__(self):
        self.memory_limit = 128 * 1024 * 1024  # 128MB
        self.cpu_limit = 0.5  # 50% CPU max
        
    def start(self):
        pass  # No thread needed now
        
    def check_resources(self):
        mem = psutil.virtual_memory()
        if mem.used > self.memory_limit:
            self.free_memory()
            
    def free_memory(self):
        gc.collect()
        # Reduce active components
        global ACTIVE_COMPONENTS
        ACTIVE_COMPONENTS = [c for c in ACTIVE_COMPONENTS 
                           if c.__class__.__name__ in ('TurboArbitrage', 'PairManager')]

### [FILE] monitoring/performance.py
import time
import psutil
import os  # ✅ ADDED: Missing import
from dataclasses import dataclass
from typing import List, Dict
import logging

logger = logging.getLogger('performance')

@dataclass
class SystemMetrics:
    cpu_priority: int
    load_avg: float
    network_latency: float
    trade_latency: float

class PerformanceOptimizer:
    def __init__(self):
        self.metrics_history = []
        
    def collect_metrics(self) -> SystemMetrics:
        """Collect system performance metrics"""
        try:
            # ✅ FIXED: Safe priority retrieval
            try:
                cpu_priority = os.getpriority(os.PRIO_PROCESS, 0)
            except (AttributeError, OSError):
                cpu_priority = 0  # Default if not available
                
            # ✅ FIXED: Safe load average retrieval
            try:
                load_avg = os.getloadavg()[0]
            except (AttributeError, OSError):
                load_avg = 0.0
                
            return SystemMetrics(
                cpu_priority=cpu_priority,
                load_avg=load_avg,
                network_latency=self._measure_ping(),
                trade_latency=self._get_trade_latency()
            )
        except Exception as e:
            logger.error(f"Metrics collection failed: {e}")
            return SystemMetrics(0, 0.0, 0.0, 0.0)
    
    def _measure_ping(self) -> float:
        """Measure network latency (simulated)"""
        try:
            # Simulate ping measurement
            return 0.05  # 50ms default
        except:
            return 0.1  # Fallback
    
    def _get_trade_latency(self) -> float:
        """Get average trade execution latency"""
        try:
            # This would come from actual trade metrics
            return 0.2  # 200ms default
        except:
            return 0.3  # Fallback
    
    def optimize_based_on_metrics(self):
        """Optimize performance based on collected metrics"""
        try:
            metrics = self.collect_metrics()
            self.metrics_history.append(metrics)
            
            # Dynamic priority adjustment rules
            if metrics.load_avg > 1.5 and metrics.cpu_priority < 10:
                try:
                    os.nice(10 - metrics.cpu_priority)
                    logger.info(f"Adjusted CPU priority to {10 - metrics.cpu_priority}")
                except OSError as e:
                    logger.warning(f"Could not adjust priority: {e}")
                    
            elif metrics.trade_latency > 0.5:
                try:
                    current_priority = os.getpriority(os.PRIO_PROCESS, 0)
                    new_priority = max(0, current_priority - 2)
                    os.nice(new_priority - current_priority)
                    logger.info(f"Reduced CPU priority to {new_priority} due to high latency")
                except OSError as e:
                    logger.warning(f"Could not adjust priority: {e}")
                    
        except Exception as e:
            logger.error(f"Performance optimization failed: {e}")

class PriorityManager:
    @staticmethod
    def optimize_system():
        """Set optimal priorities for free tier"""
        try:
            # ✅ FIXED: Safe priority adjustment
            current_priority = os.getpriority(os.PRIO_PROCESS, 0)
            if current_priority < 10:
                os.nice(10 - current_priority)
                logger.info("Set optimal CPU priority for free tier")
            
            # Reduce polling frequency during high CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            if cpu_percent > 70:
                # This would adjust the global config
                logger.warning(f"High CPU usage detected: {cpu_percent}%")
                
        except Exception as e:
            logger.warning(f"System optimization limited: {e}")

### [FILE] optimizations/jit_utils.py
import numpy as np
from numba import jit

@jit(nopython=True)
def calculate_path_profit(bid_prices: np.ndarray, ask_prices: np.ndarray, fee: float) -> np.ndarray:
    """
    Vectorized profit calculation for all possible triangular paths
    Returns: 3D numpy array of profit percentages
    """
    n = len(bid_prices)
    profits = np.zeros((n, n, n))
    
    for i in range(n):
        for j in range(n):
            for k in range(n):
                if i != j and j != k and i != k:
                    profits[i,j,k] = (
                        bid_prices[i] * 
                        (1/ask_prices[j]) * 
                        bid_prices[k] * 
                        (1 - fee)**3 - 1
                    )
    return profits

@jit(nopython=True)
def find_top_paths(profits: np.ndarray, threshold: float) -> np.ndarray:
    """Find paths exceeding profit threshold"""
    return np.argwhere(profits > threshold)

# types.py (renamed from typing.py)
"""
Type hints and core data structures for the arbitrage bot
"""
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass

@dataclass
class OrderBook:
    bids: List[Tuple[float, float]]  # (price, quantity)
    asks: List[Tuple[float, float]]
    timestamp: float

@dataclass 
class TradeSignal:
    path: Tuple[str, str, str]  # (A/B, B/C, C/A)
    expected_profit: float
    timestamp: float

### [FILE] utils/threading_utils.py
import threading
from typing import Callable, Any

class SafeThread(threading.Thread):
    """Thread class with error handling and restart capability"""
    def __init__(self, target: Callable, *args: Any, **kwargs: Any):
        super().__init__(target=target, *args, **kwargs)
        self._stop_event = threading.Event()
        self.daemon = True
        
    def stop(self) -> None:
        self._stop_event.set()
        
    def stopped(self) -> bool:
        return self._stop_event.is_set()

class ThreadPool:
    """Lightweight thread pool for concurrent execution"""
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self._workers = []
        
    def submit(self, task: Callable, *args: Any, **kwargs: Any) -> None:
        """Add task to the pool"""
        if len(self._workers) >= self.max_workers:
            self._cleanup()
        worker = SafeThread(target=task, args=args, kwargs=kwargs)
        worker.start()
        self._workers.append(worker)
        
    def _cleanup(self) -> None:
        """Remove completed workers"""
        self._workers = [w for w in self._workers if w.is_alive()]

### [FILE] ml/rl_env.py
import numpy as np
from gymnasium import Env
from typing import Tuple

class ArbitrageEnv(Env):
    """Custom RL environment for arbitrage strategy optimization"""
    def __init__(self, num_pairs: int = 10):
        self.num_pairs = num_pairs
        self.action_space = spaces.Discrete(3)  # 0=hold, 1=buy, 2=sell
        self.observation_space = spaces.Box(
            low=0, high=1, 
            shape=(num_pairs*3,),  # bid, ask, spread for each pair
            dtype=np.float32
        )
        
    def reset(self) -> np.ndarray:
        """Reset the environment state"""
        self.state = np.random.rand(self.num_pairs*3)
        return self.state
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:
        """Execute one environment step"""
        reward = self._calculate_reward(action)
        next_state = self._get_next_state()
        done = False  # Continuous task
        info = {}
        return next_state, reward, done, info
        
    def _calculate_reward(self, action: int) -> float:
        """Calculate reward based on action"""
        # Implement your reward logic here
        return 0.0
        
    def _get_next_state(self) -> np.ndarray:
        """Simulate market state update"""
        return np.random.rand(self.num_pairs*3)

### [FILE] ml/rl_trainer.py
import numpy as np
import logging

logger = logging.getLogger('advanced_model')

# Safe TensorFlow import
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, LSTM
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger.warning("TensorFlow not available, using fallback")

class NeuralPrecisionModel:
    """Advanced neural network for trading predictions"""
    
    def __init__(self, input_shape=(50, 1)):
        self.model = None
        self.trained = False
        
        if TENSORFLOW_AVAILABLE:
            self.model = self._build_model(input_shape)
        else:
            logger.warning("Using fallback model")
    
    def _build_model(self, input_shape):
        """Build neural network architecture"""
        if not TENSORFLOW_AVAILABLE:
            return None
            
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            LSTM(32),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train(self, X, y, epochs=100, batch_size=32):
        """Train the model"""
        if self.model is None:
            self.trained = True
            return
            
        try:
            if len(X.shape) == 2:
                X = X.reshape((X.shape[0], X.shape[1], 1))
                
            history = self.model.fit(
                X, y, epochs=epochs, batch_size=batch_size,
                validation_split=0.2, verbose=0
            )
            
            self.trained = True
            accuracy = history.history['accuracy'][-1]
            logger.info(f"✅ Model trained - Accuracy: {accuracy:.3f}")
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            self.trained = False
    
    def predict(self, X):
        """Make prediction"""
        if not self.trained or self.model is None:
            return 0.5
            
        try:
            if len(X.shape) == 2:
                X = X.reshape((1, X.shape[0], 1))
                
            prediction = self.model.predict(X, verbose=0)[0][0]
            return float(prediction)
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            return 0.5

class QuantumOverlay:
    """Quantum-inspired prediction adjustments"""
    
    def __init__(self):
        self.prediction_horizon = 5
        
    def apply_quantum_overlay(self, market_data, base_prediction):
        """Apply quantum-style adjustments"""
        try:
            volatility = np.std(market_data[-10:]) if len(market_data) >= 10 else 0.01
            trend_strength = self._calculate_trend_strength(market_data)
            
            quantum_factor = 1.0 + (volatility * trend_strength * 0.1)
            adjusted = base_prediction * quantum_factor
            
            return np.clip(adjusted, 0.1, 0.9)
            
        except Exception as e:
            logger.error(f"Quantum overlay failed: {e}")
            return base_prediction
    
    def _calculate_trend_strength(self, prices):
        """Calculate trend strength"""
        if len(prices) < 5:
            return 0.5
            
        short_ma = np.mean(prices[-5:])
        long_ma = np.mean(prices[-20:]) if len(prices) >= 20 else short_ma
        
        return (short_ma - long_ma) / long_ma

### [FILE] ml/rl_light.py
import numpy as np
from typing import Dict, Any, List

class LightRL:
    """Memory-efficient RL alternative for free tier"""
    def __init__(self, num_actions: int = 3, state_size: int = 10):
        self.weights = np.random.randn(state_size, num_actions) * 0.01
        self.bias = np.zeros(num_actions)
        
    def train(self, experiences: List[Dict[str, Any]], lr: float = 0.01) -> None:
        """Online training with stochastic gradient descent"""
        if not experiences:
            return
            
        for exp in experiences:
            state = np.array(exp['state'])
            action = exp['action']
            reward = exp['reward']
            
            # Forward pass
            logits = state @ self.weights + self.bias
            probs = np.exp(logits) / np.sum(np.exp(logits))
            
            # Backward pass (simplified)
            grad = probs
            grad[action] -= 1
            self.weights -= lr * np.outer(state, grad)
            self.bias -= lr * grad
    
    def predict(self, state: np.ndarray) -> int:
        """Prediction with quantized weights to save memory"""
        quantized_weights = np.round(self.weights, decimals=4)
        quantized_bias = np.round(self.bias, decimals=4)
        logits = state @ quantized_weights + quantized_bias
        return np.argmax(logits)

### [FILE] config/free_tier.yaml
aws_free_tier:
  max_operating_hours_per_day: 6
  supported_instance_types:
    - "t2.micro"
    - "t3.micro"
  monthly_free_hours: 750
  cost_alerts:
    monthly_threshold: 0.50
    projected_overage_alert: 1.00

scheduling:
  operational_windows:
    - start: "09:00"
      end: "12:00"
    - start: "14:00"
      end: "17:00"
  max_daily_runtime: 6
  min_uptime: 1
  max_downtime: 18
  timezone: "Asia/Singapore"
  utc_offset: "+08:00"

daily_schedule:
  start_time: "09:00"
  stop_time: "17:00"
  enable_auto_shutdown: true
  timezone: "Asia/Singapore"

free_tier_limits:
  max_daily_runtime_hours: 6
  instance_type: "t2.micro"
  shutdown_buffer_minutes: 5

performance_optimization:
  memory:
    max_utilization_percent: 85
    alert_threshold: 70
  cpu:
    baseline_performance: 10
    max_burst_utilization: 95
    sustainable_utilization: 20
  network:
    max_bandwidth_mbps: 100
    monitor_data_transfer: true

binance_api_optimization:
  requests_per_minute: 1200
  weight_limit_buffer: 0.2
  max_websocket_connections: 5
  http_pool_size: 3
  kline_intervals:
    - "1m"
    - "5m" 
    - "15m"
    - "1h"

logging_monitoring:
  log_level: "INFO"
  performance_metrics_interval: 300
  health_check_interval: 60
  cloudwatch:
    enabled: true
    custom_metrics: true
    log_retention_days: 7

ml_training:
  training_schedule:
    enabled: false
    allowed_window: "02:00-04:00"
  max_training_time_minutes: 30
  model_size_limit_mb: 50

compliance:
  enforce_usage_limits: true
  auto_shutdown: true
  usage_reporting: true
  max_monthly_cost: 0.00

instance_management:
  startup_sequence:
    - "timezone_verification"
    - "health_check"
    - "api_connectivity_test"
    - "market_data_validation"
    - "strategy_initialization"
  shutdown_sequence:
    - "close_open_orders"
    - "portfolio_balance_check"
    - "save_trading_state"
    - "performance_report"
    - "graceful_shutdown"
  timezone_verification:
    expected_timezone: "Asia/Singapore"
    fallback_timezone: "UTC"
    verification_interval: 3600

schedule_sync:
  primary_config: "config.yaml"
  sync_interval: 300
  conflict_resolution: "primary_preference"
  validation_checks: true

log_management:
  enable_rotation: true
  max_file_size_mb: 10
  backup_count: 5
  compression: true
  total_size_limit_mb: 100
  rotation_interval_minutes: 60
  cleanup_threshold_gb: 1.0
  emergency_cleanup_threshold_gb: 0.5
  auto_cleanup: true
  cleanup_cooldown_minutes: 30
  retain_days: 7
  compress_after_days: 3
  delete_after_days: 30

free_tier_optimization:
  max_memory_mb: 512
  max_cpu_percent: 80
  max_disk_usage_percent: 85
  max_daily_runtime_hours: 6
  graceful_shutdown_minutes: 15
  max_log_size_mb: 100
  temp_file_cleanup_interval_minutes: 60
  cache_cleanup_interval_minutes: 120
  disk_check_interval_seconds: 300
  log_rotation_check_interval_seconds: 600

watchdog_enhanced:
  monitoring_interval: 60
  alert_thresholds:
    cpu_percent: 85.0
    memory_percent: 90.0
    disk_percent: 95.0
    disk_free_gb: 1.0
    network_errors: 10
  enable_log_rotation_monitoring: true
  max_log_size_alert_mb: 80
  enable_disk_space_alerts: true

# >>> NEW: Load Testing Configuration Section (ADDITION ONLY)
load_testing:
  enabled: false
  traffic_multiplier: 1.0
  duration_minutes: 60
  ramp_up_time: 300
  scenarios:
    - name: "normal"
      multiplier: 1.0
      duration: 300
    - name: "2x_traffic" 
      multiplier: 2.0
      duration: 300
    - name: "5x_traffic"
      multiplier: 5.0
      duration: 300
    - name: "10x_traffic"
      multiplier: 10.0
      duration: 300
  monitoring:
    enable_detailed_metrics: true
    alert_on_degradation: true
    auto_stop_on_critical: true
  resource_allocation:
    max_memory_percent: 90
    max_cpu_percent: 95
    network_bandwidth_limit_mbps: 50
  performance_thresholds:
    max_response_time_ms: 1000
    max_error_rate_percent: 5
    min_throughput_requests_per_second: 10

# >>> NEW: High-Frequency Trading Optimizations (ADDITION ONLY)
hft_optimizations:
  enabled: false
  queue_optimizations:
    max_batch_size: 200
    processing_delay_ms: 0.5
    bulk_operations: true
    priority_queues: true
  connection_optimizations:
    connection_pool_size: 10
    max_pool_size: 30
    keepalive_interval: 30
    timeout_optimization: true
  message_processing:
    batch_processing: true
    max_batch_size: 100
    batch_timeout_ms: 10
    parallel_processing: true

# >>> NEW: 6-Hour Daily Scheduler Configuration (ADDITION ONLY)
daily_six_hour_scheduler:
  enabled: true
  daily_hours: 6
  operation_window: "00:00-06:00"
  timezone: "Asia/Singapore"
  auto_start: true
  enforce_strict_limits: true
  shutdown_buffer_minutes: 10
  monitoring:
    check_interval_seconds: 60
    status_report_interval_minutes: 30
    alert_on_limit_approach: true
  compliance:
    enforce_singapore_timezone: true
    validate_aws_region: true
    require_daily_reset: true

# >>> NEW: Enhanced Resource Management (ADDITION ONLY)
enhanced_resource_management:
  enabled: true
  disk_space_monitoring:
    check_interval_seconds: 300
    critical_threshold_gb: 0.5
    warning_threshold_gb: 1.0
    emergency_cleanup_auto: true
  log_management:
    rotation_interval_seconds: 1800
    max_total_size_mb: 100
    compression_enabled: true
    retention_days: 7
  memory_management:
    aggressive_cleanup: true
    cache_limits:
      analysis_cache_mb: 50
      market_data_cache_mb: 100
      order_cache_mb: 20

# PRESERVE: All existing sections remain unchanged
binance:
  testnet: false
  recv_window: 5000
  timeout: 10

websocket:
  reconnect_delay: 5
  max_reconnect_attempts: 10
  heartbeat_interval: 30

logging:
  level: "INFO"
  format: "detailed"
  enable_file_logging: true
  enable_console_logging: true

compatibility:
  preserve_original_config: true
  legacy_support: true
  migration_safe: true

### [FILE] optimizations/path_generator.py
import numpy as np
from numba import jit

@jit(nopython=True)
def generate_paths(symbols: np.ndarray) -> np.ndarray:
    n = len(symbols)
    paths = np.empty((n*(n-1)*(n-2), 3), dtype=np.int32)
    count = 0
    for i in range(n):
        for j in range(n):
            for k in range(n):
                if i != j and j != k and i != k:
                    paths[count] = (i, j, k)
                    count += 1
    return paths[:count]

def get_top_pairs(order_books: dict, max_pairs: int = 8) -> list:
    """Select most liquid pairs based on order book depth"""
    liquidity = []
    for sym, book in order_books.items():
        bid_depth = sum(float(b[1]) for b in book['bids'][:3])
        ask_depth = sum(float(a[1]) for a in book['asks'][:3])
        liquidity.append((sym, (bid_depth + ask_depth)/2))
    
    return [x[0] for x in sorted(liquidity, key=lambda x: -x[1])[:max_pairs]]

### [FILE] optimizations/adaptive_rate_limiter.py
from functools import wraps
import asyncio
import time
from typing import Optional
import numpy as np

class AdaptiveRateLimiter:
    def __init__(self, calls_per_second: int, burst_capacity: Optional[int] = None):
        self.base_interval = 1.0 / calls_per_second
        self.burst_capacity = burst_capacity or calls_per_second
        self.tokens = self.burst_capacity
        self.last_update = time.monotonic()
        self.lock = asyncio.Lock()
        self.response_times = []
        
    async def _update_tokens(self):
        now = time.monotonic()
        elapsed = now - self.last_update
        self.last_update = now
        self.tokens = min(
            self.burst_capacity,
            self.tokens + elapsed / self.base_interval
        )
    
    def _calculate_dynamic_interval(self):
        """Adjust rate based on recent performance"""
        if len(self.response_times) < 5:
            return self.base_interval
            
        avg_response = np.mean(self.response_times[-5:])
        return min(
            self.base_interval * 2,  # Max 2x slower
            max(
                self.base_interval * 0.5,  # Min 2x faster
                avg_response * 0.8  # Adaptive adjustment
            )
        )

    def __call__(self, func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.monotonic()
            
            async with self.lock:
                await self._update_tokens()
                if self.tokens < 1:
                    wait_time = (1 - self.tokens) * self._calculate_dynamic_interval()
                    await asyncio.sleep(wait_time)
                    await self._update_tokens()
                self.tokens -= 1
                
            result = await func(*args, **kwargs)
            
            # Record performance
            self.response_times.append(time.monotonic() - start_time)
            if len(self.response_times) > 10:
                self.response_times.pop(0)
                
            return result
        return wrapper

### [FILE] core/hedging.py
from typing import Optional, Dict
import asyncio
from decimal import Decimal

class FuturesHedger:
    def __init__(self, spot_client, futures_client, hedge_ratio: float = 0.33):
        self.spot = spot_client
        self.futures = futures_client
        self.hedge_ratio = hedge_ratio
        self.open_hedges: Dict[str, Decimal] = {}  # Track active hedges {asset: amount}
        
    async def hedge_position(self, asset: str, spot_amount: float) -> Optional[float]:
        """Lightweight futures hedge with position tracking"""
        if abs(spot_amount) < 10:  # Skip small amounts
            return None
            
        # Calculate net exposure including existing hedges
        current_hedge = self.open_hedges.get(asset, Decimal('0'))
        net_exposure = Decimal(str(spot_amount)) - current_hedge
        
        if abs(net_exposure) < Decimal('5'):  # Skip small adjustments
            return None
            
        side = 'SELL' if net_exposure > 0 else 'BUY'
        qty = abs(float(net_exposure) * self.hedge_ratio)
        qty = round(qty, 3)  # Round to 3 decimal places for free tier
        
        try:
            if self.spot.config['mode'] == 'paper':
                print(f"SIMULATED HEDGE: {side} {qty} {asset}USDT")
                hedge_amount = qty
            else:
                # Live implementation
                order = await self.futures.create_order(
                    symbol=f'{asset}USDT',
                    side=side,
                    type='MARKET',
                    quantity=qty
                )
                hedge_amount = float(order['executedQty'])
            
            # Update hedge tracking
            if asset in self.open_hedges:
                if side == 'BUY':
                    self.open_hedges[asset] += Decimal(str(hedge_amount))
                else:
                    self.open_hedges[asset] -= Decimal(str(hedge_amount))
            else:
                self.open_hedges[asset] = Decimal(str(hedge_amount)) * (1 if side == 'BUY' else -1)
                
            return hedge_amount
            
        except Exception as e:
            print(f"Hedging failed for {asset}: {e}")
            return None

    def get_net_exposure(self, asset: str) -> float:
        """Get current net hedge position"""
        return float(self.open_hedges.get(asset, Decimal('0')))

### [FILE] core/emergency.py
class EmergencyRecovery:
    async def recover_stuck_trade(self, partial_path: List[str]):
        """Example: BTC was bought but ETH trade failed"""
        if len(partial_path) == 1:  # Only first leg executed
            asset = partial_path[0].split('-')[0]
            await self.client.market_sell(f'{asset}USDC')
        
        elif len(partial_path) == 2:  # Two legs executed
            # Calculate best recovery path
            remaining = [p for p in ['BTC','ETH','USDC'] if p not in partial_path]
            recovery_path = partial_path + remaining
            await self.execute_trade(recovery_path)

### [FILE] ml/light_predictor.py
import numpy as np
import logging

logger = logging.getLogger('light_predictor')

class NanoPredictor:
    """
    Ultra-lightweight neural network for free-tier constraints
    3-2-1 architecture for minimal memory footprint
    """
    def __init__(self):
        # Tiny neural network weights (3-2-1) with float16 for memory efficiency
        self.weights = [
            np.array([[0.5, -0.3], [0.2, 0.8], [-0.4, 0.1]], dtype=np.float16),  # Input to hidden
            np.array([[0.7], [-0.2]], dtype=np.float16)  # Hidden to output
        ]
        self.biases = [
            np.array([0.1, -0.1], dtype=np.float16),  # Hidden layer bias
            np.array([0.05], dtype=np.float16)  # Output bias
        ]
        self.trained = False
    
    def predict(self, inputs: np.ndarray) -> float:
        """Make prediction with ultra-light network"""
        try:
            if inputs is None or len(inputs) == 0:
                return 0.5  # Neutral prediction
            
            # Ensure input is proper format
            x = np.array(inputs, dtype=np.float16).flatten()
            if len(x) != 3:
                # Pad or truncate to 3 features
                if len(x) > 3:
                    x = x[:3]
                else:
                    x = np.pad(x, (0, 3 - len(x)), mode='constant')
            
            # Forward pass with clipping for stability
            x = np.clip(x, -3, 3)
            
            # Hidden layer with ReLU
            z1 = np.dot(x, self.weights[0]) + self.biases[0]
            a1 = np.maximum(0, z1)  # ReLU activation
            
            # Output layer (linear)
            z2 = np.dot(a1, self.weights[1]) + self.biases[1]
            output = float(np.clip(z2, 0, 1))  # Clip to [0, 1] range
            
            return output
            
        except Exception as e:
            logger.error(f"Prediction error: {e}")
            return 0.5  # Fallback to neutral
    
    def train_online(self, features: np.ndarray, target: float, learning_rate: float = 0.01):
        """Online learning with single sample"""
        try:
            if features is None or target is None:
                return
                
            # Forward pass
            x = np.array(features, dtype=np.float16).flatten()[:3]  # Use first 3 features
            if len(x) < 3:
                x = np.pad(x, (0, 3 - len(x)), mode='constant')
            
            # Hidden layer
            z1 = np.dot(x, self.weights[0]) + self.biases[0]
            a1 = np.maximum(0, z1)
            
            # Output layer
            z2 = np.dot(a1, self.weights[1]) + self.biases[1]
            prediction = float(z2)
            
            # Backward pass (simplified)
            error = prediction - target
            
            # Update weights (simplified gradient descent)
            if abs(error) > 0.1:  # Only update on significant errors
                # Output layer update
                grad_output = error * a1
                self.weights[1] -= learning_rate * grad_output.reshape(-1, 1)
                self.biases[1] -= learning_rate * error
                
                # Hidden layer update
                grad_hidden = error * self.weights[1].flatten() * (z1 > 0).astype(np.float16)
                self.weights[0] -= learning_rate * np.outer(x, grad_hidden)
                self.biases[0] -= learning_rate * grad_hidden
            
            self.trained = True
            
        except Exception as e:
            logger.error(f"Training error: {e}")
    
    def get_weights_info(self) -> dict:
        """Get information about model weights"""
        return {
            'input_weights_shape': self.weights[0].shape,
            'output_weights_shape': self.weights[1].shape,
            'trained': self.trained,
            'memory_footprint_bytes': sum(w.nbytes for w in self.weights) + sum(b.nbytes for b in self.biases)
        }

### [FILE] utils/smart_sleep.py
import asyncio
import numpy as np

class MarketAdaptiveSleep:
    def __init__(self):
        self.last_sleep = 1.0
        self.volatility = 0.0
        
    async def sleep(self):
        # Dynamic sleep between 0.3s and 2s
        sleep_time = np.clip(1.0 / (1.0 + self.volatility * 10), 0.3, 2.0)
        await asyncio.sleep(sleep_time)
        self.last_sleep = sleep_time
        
    def update_volatility(self, new_value):
        self.volatility = 0.9 * self.volatility + 0.1 * new_value

### [FILE] monitoring/free_tier_guard.py
import time
import logging
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import psutil
import boto3
from utils.advanced_logger import AdvancedLogger

class FreeTierGuard:
    """
    Enhanced free tier guard for AWS free tier compliance
    Ensures 6-hour daily operation and resource monitoring
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = AdvancedLogger(__name__)
        
        # Free tier configuration
        free_tier_config = config.get('free_tier', {})
        self.max_daily_hours = free_tier_config.get('max_daily_hours', 6)
        self.start_hour = free_tier_config.get('start_hour', 0)  # UTC
        self.stop_hour = free_tier_config.get('stop_hour', 6)    # UTC
        self.aws_region = free_tier_config.get('aws_region', 'ap-southeast-1')
        
        # Runtime tracking
        self.start_time = time.time()
        self.daily_start_time = self._get_daily_start_time()
        self.max_runtime_seconds = self.max_daily_hours * 3600
        self.total_runtime_today = 0
        self.is_monitoring = False
        self.monitor_thread = None
        
        # Resource monitoring
        self.max_cpu_usage = 80.0  # Percentage
        self.max_memory_usage = 85.0  # Percentage
        self.max_disk_usage = 90.0  # Percentage
        
        # AWS client (optional)
        self.aws_client = None
        self.initialize_aws_client()
        
        self.logger.info(f"FreeTierGuard initialized: {self.max_daily_hours}h daily from {self.start_hour}:00-{self.stop_hour}:00 UTC")

    def _get_daily_start_time(self) -> float:
        """Calculate today's start time in UTC"""
        now = datetime.utcnow()
        today_start = datetime(now.year, now.month, now.day, self.start_hour)
        return time.mktime(today_start.timetuple())

    def initialize_aws_client(self):
        """Initialize AWS client for monitoring"""
        try:
            aws_config = self.config.get('aws', {})
            access_key = aws_config.get('access_key')
            secret_key = aws_config.get('secret_key')
            
            if access_key and secret_key:
                self.aws_client = boto3.Session(
                    aws_access_key_id=access_key,
                    aws_secret_access_key=secret_key,
                    region_name=self.aws_region
                )
                self.logger.info("AWS client initialized for free tier monitoring")
            else:
                self.logger.info("AWS credentials not provided, using basic monitoring")
                
        except Exception as e:
            self.logger.warning(f"Could not initialize AWS client: {e}")

    def should_continue_running(self) -> bool:
        """
        Check if bot should continue running based on free tier limits
        Returns False if daily limits are exceeded or outside allowed hours
        """
        current_time = time.time()
        
        # Check if we're in the allowed time window
        if not self._is_in_allowed_hours():
            self.logger.warning("Outside allowed operating hours")
            return False
            
        # Check daily runtime limit
        elapsed_today = current_time - self.daily_start_time
        self.total_runtime_today = elapsed_today
        
        if elapsed_today > self.max_runtime_seconds:
            self.logger.warning(f"Daily runtime limit exceeded: {elapsed_today/3600:.1f}h > {self.max_daily_hours}h")
            return False
            
        # Check system resource limits
        if not self._check_system_resources():
            self.logger.warning("System resource limits exceeded")
            return False
            
        return True

    def _is_in_allowed_hours(self) -> bool:
        """Check if current time is within allowed operating hours"""
        current_utc = datetime.utcnow()
        current_hour = current_utc.hour
        
        # Handle overnight window (e.g., 00:00-06:00)
        if self.start_hour < self.stop_hour:
            return self.start_hour <= current_hour < self.stop_hour
        else:
            # Handle window crossing midnight (e.g., 22:00-04:00)
            return current_hour >= self.start_hour or current_hour < self.stop_hour

    def _check_system_resources(self) -> bool:
        """Check system resources are within free tier limits"""
        try:
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            if cpu_percent > self.max_cpu_usage:
                self.logger.warning(f"CPU usage too high: {cpu_percent}%")
                return False
                
            # Memory usage
            memory = psutil.virtual_memory()
            if memory.percent > self.max_memory_usage:
                self.logger.warning(f"Memory usage too high: {memory.percent}%")
                return False
                
            # Disk usage
            disk = psutil.disk_usage('/')
            if disk.percent > self.max_disk_usage:
                self.logger.warning(f"Disk usage too high: {disk.percent}%")
                return False
                
            return True
            
        except Exception as e:
            self.logger.error(f"Error checking system resources: {e}")
            return True  # Don't stop bot on monitoring errors

    def get_remaining_time(self) -> float:
        """Get remaining runtime in seconds for today"""
        if not self._is_in_allowed_hours():
            return 0
            
        current_time = time.time()
        elapsed_today = current_time - self.daily_start_time
        remaining = max(0, self.max_runtime_seconds - elapsed_today)
        return remaining

    def get_usage_statistics(self) -> Dict[str, Any]:
        """Get comprehensive usage statistics"""
        current_time = time.time()
        elapsed_total = current_time - self.start_time
        elapsed_today = current_time - self.daily_start_time
        
        # System resources
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        stats = {
            'runtime': {
                'total_seconds': elapsed_total,
                'total_hours': elapsed_total / 3600,
                'today_seconds': min(elapsed_today, self.max_runtime_seconds),
                'today_hours': min(elapsed_today / 3600, self.max_daily_hours),
                'remaining_seconds': self.get_remaining_time(),
                'remaining_hours': self.get_remaining_time() / 3600,
                'daily_limit_hours': self.max_daily_hours
            },
            'system': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'disk_percent': disk.percent,
                'memory_used_gb': memory.used / (1024**3),
                'disk_used_gb': disk.used / (1024**3)
            },
            'status': {
                'in_allowed_hours': self._is_in_allowed_hours(),
                'within_daily_limit': elapsed_today <= self.max_runtime_seconds,
                'within_resource_limits': self._check_system_resources()
            }
        }
        
        return stats

    def start_monitoring(self, interval: int = 60):
        """Start background monitoring"""
        if self.is_monitoring:
            return
            
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        self.logger.info(f"Started free tier monitoring with {interval}s interval")

    def stop_monitoring(self):
        """Stop background monitoring"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        self.logger.info("Stopped free tier monitoring")

    def _monitor_loop(self, interval: int):
        """Background monitoring loop"""
        while self.is_monitoring:
            try:
                stats = self.get_usage_statistics()
                
                # Log warning if approaching limits
                remaining_hours = stats['runtime']['remaining_hours']
                if remaining_hours < 1:
                    self.logger.warning(f"Less than 1 hour remaining: {remaining_hours:.1f}h")
                elif remaining_hours < 0.5:
                    self.logger.warning(f"Less than 30 minutes remaining: {remaining_hours*60:.0f}m")
                    
                # Log resource warnings
                if stats['system']['cpu_percent'] > 70:
                    self.logger.warning(f"High CPU usage: {stats['system']['cpu_percent']}%")
                if stats['system']['memory_percent'] > 75:
                    self.logger.warning(f"High memory usage: {stats['system']['memory_percent']}%")
                    
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {e}")
                time.sleep(interval)

    def get_daily_schedule(self) -> Dict[str, Any]:
        """Get today's operating schedule"""
        now = datetime.utcnow()
        today_start = datetime(now.year, now.month, now.day, self.start_hour)
        
        if self.start_hour < self.stop_hour:
            today_end = datetime(now.year, now.month, now.day, self.stop_hour)
        else:
            # Schedule crosses midnight
            today_end = datetime(now.year, now.month, now.day, self.stop_hour) + timedelta(days=1)
            
        return {
            'start_time': today_start,
            'end_time': today_end,
            'duration_hours': self.max_daily_hours,
            'current_time': now,
            'is_active': self._is_in_allowed_hours()
        }

    def emergency_shutdown_check(self) -> bool:
        """
        Check if emergency shutdown is required
        Returns True if immediate shutdown is needed
        """
        # Check if way over daily limit (safety margin)
        current_time = time.time()
        elapsed_today = current_time - self.daily_start_time
        if elapsed_today > self.max_runtime_seconds * 1.1:  # 10% over limit
            self.logger.error(f"Emergency shutdown: Exceeded daily limit by 10%")
            return True
            
        # Check if critical system resources
        try:
            if psutil.cpu_percent(interval=0.5) > 95:
                self.logger.error("Emergency shutdown: CPU usage critical")
                return True
                
            if psutil.virtual_memory().percent > 95:
                self.logger.error("Emergency shutdown: Memory usage critical")
                return True
                
        except Exception as e:
            self.logger.error(f"Error in emergency shutdown check: {e}")
            
        return False

    def __enter__(self):
        """Context manager entry"""
        self.start_monitoring()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.stop_monitoring()

### [FILE] run.sh

# Set Python path
export PYTHONPATH=/app

# Set timezone to Singapore
export TZ=Asia/Singapore

# Free tier resource limits
export MEMORY_LIMIT=900000000
export CPU_LIMIT=50

# Dynamic priority adjustment based on system load
CURRENT_HOUR=$(date +%-H)
LOAD_AVG=$(cat /proc/loadavg | awk '{print $1}')

# Adjust priority based on load
PRIORITY=5
if command -v bc >/dev/null 2>&1; then
    if (( $(echo "$LOAD_AVG > 1.5" | bc -l) )); then
        PRIORITY=10
    elif [[ $CURRENT_HOUR -ge 1 && $CURRENT_HOUR -le 5 ]]; then
        PRIORITY=8
    fi
else
    if [[ $CURRENT_HOUR -ge 1 && $CURRENT_HOUR -le 5 ]]; then
        PRIORITY=8
    fi
fi

echo "[$(date)] Starting Quantum Trading Bot with nice -n $PRIORITY"
echo "[$(date)] Free Tier Mode: Enabled"
echo "[$(date)] Region: Singapore (UTC+8)"
echo "[$(date)] Mode: Paper Trading (Simulation)"

# Run with adjusted priority
exec nice -n $PRIORITY python -OO run.py


### [FILE] utils/secure_storage.py
import os
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import yaml
from datetime import datetime, timedelta

class SecureStorage:
    def __init__(self):
        self.secrets_path = "config/config_secrets.yaml"
        self.encryption_key = self._derive_key()
        self.cipher = Fernet(self.encryption_key)
        
    def _derive_key(self):
        # Use the encryption key from config
        with open(self.secrets_path, 'r') as f:
            secrets = yaml.safe_load(f)
            
        password = secrets['security']['encryption_key'].encode()
        salt = b'fixed_salt_for_free_tier'
        
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        return base64.urlsafe_b64encode(kdf.derive(password))
    
    def encrypt_data(self, data):
        """Encrypt sensitive data"""
        return self.cipher.encrypt(data.encode())
    
    def decrypt_data(self, encrypted_data):
        """Decrypt sensitive data"""
        return self.cipher.decrypt(encrypted_data).decode()
    
    def load_encrypted_secrets(self):
        """Load and decrypt secrets from file"""
        if not os.path.exists(self.secrets_path):
            raise FileNotFoundError("Secrets file not found")
            
        with open(self.secrets_path, 'r') as f:
            encrypted_data = yaml.safe_load(f)
            
        decrypted_secrets = {}
        for key, value in encrypted_data.items():
            if key == 'security':
                # Skip security section as it contains the encryption key
                decrypted_secrets[key] = value
            elif isinstance(value, dict):
                decrypted_secrets[key] = {}
                for subkey, subvalue in value.items():
                    decrypted_secrets[key][subkey] = self.decrypt_data(subvalue)
            else:
                decrypted_secrets[key] = self.decrypt_data(value)
                
        return decrypted_secrets
    
    def rotate_api_keys(self):
        """Automatically rotate API keys every 30 days"""
        secrets = self.load_encrypted_secrets()
        last_rotation = datetime.fromisoformat(secrets.get('last_rotation', '2000-01-01'))
        
        if datetime.now() - last_rotation > timedelta(days=30):
            # Generate new API keys (this would call Binance API in production)
            new_api_key = "new_key_from_binance_api"
            new_api_secret = "new_secret_from_binance_api"
            
            # Update secrets
            secrets['binance']['api_key'] = new_api_key
            secrets['binance']['api_secret'] = new_api_secret
            secrets['last_rotation'] = datetime.now().isoformat()
            
            # Save encrypted
            self.save_encrypted_secrets(secrets)
            
    def save_encrypted_secrets(self, secrets):
        """Encrypt and save secrets to file"""
        encrypted_data = {}
        for key, value in secrets.items():
            if key == 'security':
                # Don't encrypt the security section
                encrypted_data[key] = value
            elif isinstance(value, dict):
                encrypted_data[key] = {}
                for subkey, subvalue in value.items():
                    encrypted_data[key][subkey] = self.encrypt_data(subvalue)
            else:
                encrypted_data[key] = self.encrypt_data(value)
                
        with open(self.secrets_path, 'w') as f:
            yaml.dump(encrypted_data, f)


### [FILE] scripts/schedule_manager.py

import schedule
import time
import asyncio
import yaml
from datetime import datetime, timedelta
import psutil
import logging
import sys
import os
from typing import List, Tuple, Optional, Dict
import pytz
from dateutil import tz
import threading
import traceback
import requests

logger = logging.getLogger('schedule_manager')

class AtomicTimezoneEngine:
    """💀 ULTIMATE: Thread-safe timezone handling with atomic operations"""
    
    def __init__(self):
        self._lock = threading.RLock()
        self.aws_region = self._detect_aws_region_atomic()
        self.singapore_tz = pytz.timezone('Asia/Singapore')
        self.utc_tz = pytz.UTC
        self._last_sync = time.time()
        self._timezone_cache = {}
        
    def _detect_aws_region_atomic(self) -> str:
        """Atomic AWS region detection with brutal error handling"""
        with self._lock:
            region_sources = [
                lambda: os.getenv('AWS_REGION'),
                lambda: os.getenv('AWS_DEFAULT_REGION'),
                lambda: self._get_aws_metadata_atomic('placement/region'),
                lambda: 'ap-southeast-1'  # Singapore default
            ]
            
            for source in region_sources:
                try:
                    region = source()
                    if region and isinstance(region, str) and len(region) > 0:
                        logger.info(f"🌍 Detected AWS region: {region}")
                        return region
                except Exception as e:
                    logger.debug(f"Region detection source failed: {e}")
                    continue
                    
            logger.warning("⚠️ Using default region: ap-southeast-1")
            return 'ap-southeast-1'
            
    def _get_aws_metadata_atomic(self, path: str) -> Optional[str]:
        """Atomic AWS metadata retrieval with timeout"""
        try:
            response = requests.get(
                f'http://169.254.169.254/latest/meta-data/{path}',
                timeout=2.0
            )
            if response.status_code == 200:
                return response.text
        except Exception as e:
            logger.debug(f"AWS metadata unavailable: {e}")
        return None
        
    def get_current_time_info_atomic(self) -> Dict:
        """💀 Atomic time information retrieval"""
        with self._lock:
            now_utc = datetime.now(pytz.UTC)
            now_singapore = now_utc.astimezone(self.singapore_tz)
            
            return {
                'utc': {
                    'hour': now_utc.hour,
                    'minute': now_utc.minute,
                    'date': now_utc.strftime('%Y-%m-%d'),
                    'timestamp': now_utc.timestamp()
                },
                'singapore': {
                    'hour': now_singapore.hour,
                    'minute': now_singapore.minute,
                    'date': now_singapore.strftime('%Y-%m-%d')
                },
                'aws_region': self.aws_region
            }
            
    def convert_singapore_to_utc_atomic(self, singapore_hour: int) -> int:
        """💀 Atomic Singapore to UTC conversion"""
        with self._lock:
            try:
                now = datetime.now()
                # Create Singapore time
                sg_time = self.singapore_tz.localize(
                    datetime(now.year, now.month, now.day, singapore_hour, 0, 0)
                )
                # Convert to UTC
                utc_time = sg_time.astimezone(self.utc_tz)
                return utc_time.hour
            except Exception as e:
                logger.error(f"Timezone conversion failed: {e}")
                # Fallback: Singapore is UTC+8
                return (singapore_hour - 8) % 24

class AtomicScheduleManager:
    """💀 ULTIMATE: Brutally tested schedule manager with atomic operations"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        self.config_path = config_path
        self.config = self._load_config_atomic()
        self.timezone_engine = AtomicTimezoneEngine()
        self.trading_windows = self._parse_trading_windows_atomic()
        
        # Atomic state management
        self._state_lock = asyncio.Lock()
        self.daily_hours_used = 0.0
        self.max_daily_hours = 6.0
        self.trading_enabled = False
        self.last_status_check = datetime.now(pytz.UTC)
        self._shutdown_event = asyncio.Event()
        
        # Statistics with atomic operations
        self._stats_lock = threading.Lock()
        self.stats = {
            'trading_cycles': 0,
            'uptime_minutes': 0,
            'window_checks': 0,
            'timezone_adjustments': 0,
            'errors': 0
        }
        
        logger.info("✅ Atomic schedule manager initialized")
        
    def _load_config_atomic(self, path: str = None) -> Dict:
        """Atomic configuration loading"""
        path = path or self.config_path
        default_config = {
            'schedule': {
                'trading_hours': [
                    {'start': '02:00', 'end': '05:00'},
                    {'start': '14:00', 'end': '17:00'}
                ],
                'check_interval': 300
            }
        }
        
        try:
            if not os.path.exists(path):
                logger.warning(f"Config file {path} not found, using defaults")
                return default_config
                
            with open(path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            if not config or not isinstance(config, dict):
                logger.error("Invalid configuration file, using defaults")
                return default_config
                
            return config
            
        except Exception as e:
            logger.error(f"Config loading failed: {e}, using defaults")
            return default_config
            
    def _parse_trading_windows_atomic(self) -> List[Tuple[int, int]]:
        """Atomic trading window parsing"""
        windows = []
        try:
            schedule_config = self.config.get('schedule', {})
            trading_hours = schedule_config.get('trading_hours', [])
            
            if not trading_hours:
                logger.warning("No trading hours configured, using defaults")
                trading_hours = [
                    {'start': '02:00', 'end': '05:00'},
                    {'start': '14:00', 'end': '17:00'}
                ]
            
            for i, window in enumerate(trading_hours):
                start_str = window.get('start', '02:00')
                end_str = window.get('end', '05:00')
                
                # Parse Singapore time
                start_hour_sg = int(start_str.split(':')[0])
                end_hour_sg = int(end_str.split(':')[0])
                
                # Convert to UTC
                start_hour_utc = self.timezone_engine.convert_singapore_to_utc_atomic(start_hour_sg)
                end_hour_utc = self.timezone_engine.convert_singapore_to_utc_atomic(end_hour_sg)
                
                windows.append((start_hour_utc, end_hour_utc))
                
                logger.info(f"📅 Window {i+1}: {start_hour_sg:02d}:00-{end_hour_sg:02d}:00 SGT -> "
                           f"UTC {start_hour_utc:02d}:00-{end_hour_utc:02d}:00")
                
        except Exception as e:
            logger.error(f"Window parsing failed: {e}")
            # Fallback windows in UTC (Singapore time 02:00-05:00 = UTC 18:00-21:00 previous day)
            windows = [(18, 21), (6, 9)]  # UTC times
            logger.info("📅 Using fallback trading windows")
            
        return windows
        
    def _is_in_trading_window_atomic(self, current_hour_utc: int) -> bool:
        """Atomic trading window check"""
        for start, end in self.trading_windows:
            if start <= current_hour_utc < end:
                return True
        return False
        
    async def should_run_atomic(self) -> bool:
        """💀 Atomic schedule checking with brutal error handling"""
        async with self._state_lock:
            try:
                with self._stats_lock:
                    self.stats['window_checks'] += 1
                
                # Get current time information atomically
                time_info = self.timezone_engine.get_current_time_info_atomic()
                now_utc = datetime.fromtimestamp(time_info['utc']['timestamp'], pytz.UTC)
                current_hour_utc = time_info['utc']['hour']
                
                # Reset daily counter at midnight UTC
                if now_utc.date() != self.last_status_check.date():
                    self.daily_hours_used = 0.0
                    self.last_status_check = now_utc
                    logger.info("🔄 Daily usage counter reset at UTC midnight")
                
                # Check daily limit
                if self.daily_hours_used >= self.max_daily_hours:
                    logger.info(f"⏰ Daily limit reached: {self.daily_hours_used:.2f}/{self.max_daily_hours} hours")
                    self.trading_enabled = False
                    return False
                    
                # Check trading window
                self.trading_enabled = self._is_in_trading_window_atomic(current_hour_utc)
                
                # Log status
                if self.trading_enabled:
                    current_window = next(((s,e) for s,e in self.trading_windows if s <= current_hour_utc < e), None)
                    if current_window:
                        start, end = current_window
                        remaining = end - current_hour_utc
                        logger.info(f"🎯 Trading ACTIVE | Window: {start:02d}:00-{end:02d}:00 UTC | "
                                   f"Remaining: {remaining}h")
                else:
                    # Find next window
                    all_windows = sorted(self.trading_windows)
                    next_window = next(((s,e) for s,e in all_windows if s > current_hour_utc), None)
                    if not next_window and all_windows:
                        next_window = all_windows[0]  # Wrap to next day
                    
                    if next_window:
                        start, end = next_window
                        hours_until = (start - current_hour_utc) % 24
                        logger.info(f"😴 Trading PAUSED | Next: {start:02d}:00 UTC (in {hours_until}h)")
                    
                return self.trading_enabled
                
            except Exception as e:
                with self._stats_lock:
                    self.stats['errors'] += 1
                logger.error(f"Schedule check failed: {e}")
                self.trading_enabled = False
                return False
                
    async def update_usage_atomic(self, minutes: int = 1):
        """💀 Atomic usage update"""
        async with self._state_lock:
            self.daily_hours_used += minutes / 60.0
            with self._stats_lock:
                self.stats['uptime_minutes'] += minutes
                self.stats['trading_cycles'] += 1
            
    def get_uptime_estimate_atomic(self) -> Dict:
        """💀 Atomic uptime estimation"""
        time_info = self.timezone_engine.get_current_time_info_atomic()
        current_hour_utc = time_info['utc']['hour']
        
        # Calculate remaining time
        current_window_remaining = 0
        for start, end in self.trading_windows:
            if start <= current_hour_utc < end:
                current_window_remaining = end - current_hour_utc
                break
                
        # Total remaining trading time today
        total_remaining = 0
        for start, end in self.trading_windows:
            if end > current_hour_utc:
                window_start = max(start, current_hour_utc)
                total_remaining += end - window_start
                
        with self._stats_lock:
            stats_copy = self.stats.copy()
            
        return {
            'daily_used': round(self.daily_hours_used, 3),
            'daily_remaining': round(self.max_daily_hours - self.daily_hours_used, 3),
            'current_window_remaining': current_window_remaining,
            'total_remaining_trading': total_remaining,
            'trading_enabled': self.trading_enabled,
            'current_utc_hour': current_hour_utc,
            'current_sgt_hour': time_info['singapore']['hour'],
            'aws_region': self.timezone_engine.aws_region,
            'stats': stats_copy
        }
        
    async def run_scheduler_atomic(self):
        """💀 Main scheduling loop with atomic operations"""
        logger.info("⏰ Starting atomic scheduler")
        
        # Log trading windows
        for i, (start, end) in enumerate(self.trading_windows):
            logger.info(f"📅 Window {i+1}: {start:02d}:00-{end:02d}:00 UTC")
            
        while not self._shutdown_event.is_set():
            try:
                # Update usage if trading is active
                if self.trading_enabled:
                    await self.update_usage_atomic(1)
                    
                await asyncio.sleep(60)  # Check every minute
                
            except Exception as e:
                with self._stats_lock:
                    self.stats['errors'] += 1
                logger.error(f"Scheduler error: {e}")
                await asyncio.sleep(30)
                
    async def shutdown_atomic(self):
        """💀 Atomic shutdown"""
        self._shutdown_event.set()
        logger.info("🛑 Atomic scheduler shutdown")

# Global singleton with thread safety
_scheduler_instance = None
_scheduler_lock = threading.Lock()

def get_scheduler() -> AtomicScheduleManager:
    """Get atomic scheduler instance"""
    global _scheduler_instance
    with _scheduler_lock:
        if _scheduler_instance is None:
            _scheduler_instance = AtomicScheduleManager()
    return _scheduler_instance

async def test_timezone_handling():
    """Test timezone handling thoroughly"""
    scheduler = get_scheduler()
    
    # Test multiple timezone scenarios
    test_cases = [
        (0, "Midnight UTC"),
        (8, "Morning UTC"), 
        (18, "Evening UTC"),
        (22, "Night UTC")
    ]
    
    for hour, description in test_cases:
        # Mock current hour for testing
        original_method = scheduler.timezone_engine.get_current_time_info_atomic
        scheduler.timezone_engine.get_current_time_info_atomic = lambda: {
            'utc': {'hour': hour, 'minute': 0, 'timestamp': time.time()},
            'singapore': {'hour': (hour + 8) % 24, 'minute': 0},
            'aws_region': 'ap-southeast-1'
        }
        
        should_run = await scheduler.should_run_atomic()
        print(f"⏰ {description} (UTC {hour:02d}:00) - Should run: {should_run}")
        
        # Restore original method
        scheduler.timezone_engine.get_current_time_info_atomic = original_method
        
    await scheduler.shutdown_atomic()

if __name__ == "__main__":
    asyncio.run(test_timezone_handling())

### [FILE] scripts/free_tier_optimizer.py

import psutil
import asyncio
import time
from typing import Dict, Any
import yaml
from pathlib import Path

class FreeTierOptimizer:
    def __init__(self, config_path: str = "config/config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.memory_limit = 900  # MB (leaving 100MB for system)
        self.cpu_limit = 15      # % max CPU usage
        self.network_limit = 50  # MB/hour
        self.last_optimization = time.time()
        self.optimization_interval = 300  # 5 minutes
        
    def _load_config(self) -> Dict[str, Any]:
        """Load the current configuration"""
        try:
            with open(self.config_path, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"Config file {self.config_path} not found")
            return {}
            
    def _save_config(self, config: Dict[str, Any]):
        """Save the updated configuration"""
        try:
            with open(self.config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
        except Exception as e:
            print(f"Error saving config: {e}")
    
    async def monitor_resources(self):
        """Continuously monitor system resources and optimize"""
        while True:
            try:
                current_time = time.time()
                if current_time - self.last_optimization >= self.optimization_interval:
                    await self.optimize()
                    self.last_optimization = current_time
                
                # Check for critical resource conditions more frequently
                memory_percent = psutil.virtual_memory().percent
                cpu_percent = psutil.cpu_percent(interval=1)
                
                if memory_percent > 85 or cpu_percent > 80:
                    await self.emergency_optimize()
                    
                await asyncio.sleep(30)  # Check every 30 seconds
                
            except Exception as e:
                print(f"Monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def optimize(self):
        """Apply free tier optimizations based on current resource usage"""
        print("[OPTIMIZER] Running optimization cycle")
        
        # Get current resource usage
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = memory.percent
        
        # Load current config
        current_config = self._load_config()
        
        # Memory-based optimizations
        if memory_percent > 70:
            new_active_pairs = max(2, current_config['performance']['max_active_pairs'] - 1)
            current_config['performance']['max_active_pairs'] = new_active_pairs
            print(f"[OPTIMIZER] Reduced active pairs to {new_active_pairs} due to memory pressure")
        
        # CPU-based optimizations
        if cpu_percent > self.cpu_limit:
            new_interval = min(5.0, current_config['performance']['polling_interval'] * 1.2)
            current_config['performance']['polling_interval'] = new_interval
            print(f"[OPTIMIZER] Increased polling interval to {new_interval}s due to CPU pressure")
        
        # Network optimization (if we had metrics)
        current_config['websocket']['enabled'] = memory_percent < 75
        
        # Save optimized config
        self._save_config(current_config)
        self.config = current_config
        
        print(f"[OPTIMIZER] Optimization complete. Memory: {memory_percent}%, CPU: {cpu_percent}%")
        
        return current_config
    
    async def emergency_optimize(self):
        """Emergency optimization for critical resource conditions"""
        print("[OPTIMIZER] EMERGENCY: Critical resource usage detected")
        
        current_config = self._load_config()
        
        # Drastic measures for emergency
        current_config['performance']['max_active_pairs'] = 2
        current_config['performance']['polling_interval'] = 10.0
        current_config['websocket']['enabled'] = False
        
        self._save_config(current_config)
        self.config = current_config
        
        print("[OPTIMIZER] Emergency measures applied")
        
        # Force garbage collection
        import gc
        gc.collect()
        
        return current_config

    def get_optimization_advice(self) -> Dict[str, str]:
        """Get advice for manual optimization"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        advice = {}
        
        if memory.percent > 70:
            advice['memory'] = "Consider reducing symbol count or disabling ML features"
        
        if cpu_percent > 20:
            advice['cpu'] = "Increase polling intervals or reduce computational complexity"
        
        return advice

# Singleton instance
_optimizer_instance = None

def get_optimizer():
    """Get the singleton optimizer instance"""
    global _optimizer_instance
    if _optimizer_instance is None:
        _optimizer_instance = FreeTierOptimizer()
    return _optimizer_instance

async def main():
    """Main function for standalone optimization"""
    optimizer = FreeTierOptimizer()
    print("Starting Free Tier Optimizer...")
    await optimizer.monitor_resources()

if __name__ == "__main__":
    asyncio.run(main())

### [FILE] monitor.sh

# Optimized for AWS Free Tier (t2.micro, Singapore region)

# Configuration
LOG_FILE="/app/logs/monitor.log"
MAX_MEMORY=900000  # 900MB in KB (Free Tier limit)
MAX_CPU=50         # 50% CPU usage threshold
BOT_PROCESS="python.*run.py"

# Function to log messages with timestamp
log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# Function to check resource usage
check_resources() {
    # Get memory and CPU usage of the bot process
    local PID=$(pgrep -f "$BOT_PROCESS")
    if [ -z "$PID" ]; then
        log_message "ERROR: Bot process not found!"
        return 1
    fi

    local MEM_USAGE=$(ps -o rss= -p $PID | awk '{print $1}')
    local CPU_USAGE=$(ps -o %cpu= -p $PID | awk '{print $1}')

    # Log current usage
    log_message "CPU: ${CPU_USAGE}%, Memory: ${MEM_USAGE} KB"

    # Check thresholds
    if [ -n "$MEM_USAGE" ] && [ "$MEM_USAGE" -gt "$MAX_MEMORY" ]; then
        log_message "WARNING: Memory usage exceeded ($MEM_USAGE KB > $MAX_MEMORY KB)"
        # Restart with higher polling interval
        pkill -f "$BOT_PROCESS"
        sleep 5
        export PERFORMANCE_POLLING_INTERVAL=10
        nice -n 15 python -OO run.py &
    fi

    if [ -n "$CPU_USAGE" ] && [ $(echo "$CPU_USAGE > $MAX_CPU" | bc) -eq 1 ]; then
        log_message "WARNING: CPU usage exceeded ($CPU_USAGE% > $MAX_CPU%)"
        # Restart with higher polling interval
        pkill -f "$BOT_PROCESS"
        sleep 5
        export PERFORMANCE_POLLING_INTERVAL=8
        nice -n 15 python -OO run.py &
    fi
}

# Function to check disk space
check_disk() {
    local DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$DISK_USAGE" -gt 90 ]; then
        log_message "CRITICAL: Disk usage exceeded ($DISK_USAGE%)"
        # Clean up old logs
        find /app/logs -name "*.log" -mtime +7 -delete
    fi
}

# Create log directory if it doesn't exist
mkdir -p /app/logs

log_message "Starting resource monitor for Quantum Trading Bot"

# Main loop
while true; do
    check_resources
    check_disk
    sleep 60  # Check every 60 seconds
done

### [FILE] custom_types.py

from typing import Any, Dict, List, Tuple, Optional, Union, Callable
from decimal import Decimal
import numpy as np

# Type aliases for better readability
Symbol = str
Price = float
Quantity = float
Timestamp = float

# Trading specific types
OrderBook = Dict[str, Any]
KlineData = List[Any]
TradeSignal = Dict[str, Any]
MarketData = Dict[Symbol, Dict[str, Any]]

# ML specific types
FeatureVector = np.ndarray
Prediction = Union[int, float, np.ndarray]

# Configuration types
Config = Dict[str, Any]
Secrets = Dict[str, str]

# Async types
AsyncFunction = Callable[..., Any]

def validate_config(config: Config) -> bool:
    """Basic configuration validation"""
    required_sections = ['binance', 'trading', 'risk_management']
    return all(section in config for section in required_sections)

class TradingError(Exception):
    """Base exception for trading-related errors"""
    pass

class InsufficientFundsError(TradingError):
    """Raised when there are insufficient funds for a trade"""
    pass

class NetworkError(TradingError):
    """Raised for network-related issues"""
    pass


### [FILE] ml/advanced_model.py
import numpy as np
import logging

logger = logging.getLogger('advanced_model')

# Safe TensorFlow import
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, LSTM
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger.warning("TensorFlow not available, using fallback")

class NeuralPrecisionModel:
    """Advanced neural network for trading predictions"""
    
    def __init__(self, input_shape=(50, 1)):
        self.model = None
        self.trained = False
        
        if TENSORFLOW_AVAILABLE:
            self.model = self._build_model(input_shape)
        else:
            logger.warning("Using fallback model")
    
    def _build_model(self, input_shape):
        """Build neural network architecture"""
        if not TENSORFLOW_AVAILABLE:
            return None
            
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            LSTM(32),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train(self, X, y, epochs=100, batch_size=32):
        """Train the model"""
        if self.model is None:
            self.trained = True
            return
            
        try:
            if len(X.shape) == 2:
                X = X.reshape((X.shape[0], X.shape[1], 1))
                
            history = self.model.fit(
                X, y, epochs=epochs, batch_size=batch_size,
                validation_split=0.2, verbose=0
            )
            
            self.trained = True
            accuracy = history.history['accuracy'][-1]
            logger.info(f"✅ Model trained - Accuracy: {accuracy:.3f}")
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            self.trained = False
    
    def predict(self, X):
        """Make prediction"""
        if not self.trained or self.model is None:
            return 0.5
            
        try:
            if len(X.shape) == 2:
                X = X.reshape((1, X.shape[0], 1))
                
            prediction = self.model.predict(X, verbose=0)[0][0]
            return float(prediction)
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            return 0.5

class QuantumOverlay:
    """Quantum-inspired prediction adjustments"""
    
    def __init__(self):
        self.prediction_horizon = 5
        
    def apply_quantum_overlay(self, market_data, base_prediction):
        """Apply quantum-style adjustments"""
        try:
            volatility = np.std(market_data[-10:]) if len(market_data) >= 10 else 0.01
            trend_strength = self._calculate_trend_strength(market_data)
            
            quantum_factor = 1.0 + (volatility * trend_strength * 0.1)
            adjusted = base_prediction * quantum_factor
            
            return np.clip(adjusted, 0.1, 0.9)
            
        except Exception as e:
            logger.error(f"Quantum overlay failed: {e}")
            return base_prediction
    
    def _calculate_trend_strength(self, prices):
        """Calculate trend strength"""
        if len(prices) < 5:
            return 0.5
            
        short_ma = np.mean(prices[-5:])
        long_ma = np.mean(prices[-20:]) if len(prices) >= 20 else short_ma
        
        return (short_ma - long_ma) / long_ma


### [FILE] monitoring/security_monitor.py
import logging
import time
import psutil
from typing import Dict, List
from dataclasses import dataclass
import asyncio

@dataclass
class SecurityEvent:
    timestamp: float
    level: str
    component: str
    message: str
    details: Dict

class SecurityMonitor:
    """🔒 Real-time security monitoring and alerting"""
    
    def __init__(self):
        self.logger = logging.getLogger('security')
        self.events: List[SecurityEvent] = []
        self.metrics = {
            'failed_logins': 0,
            'api_key_exposures': 0,
            'malformed_requests': 0,
            'memory_violations': 0,
            'anomaly_detections': 0
        }
        
    async def log_security_event(self, level: str, component: str, message: str, details: Dict = None):
        """Log security event with automatic alerting"""
        event = SecurityEvent(
            timestamp=time.time(),
            level=level,
            component=component,
            message=message,
            details=details or {}
        )
        
        self.events.append(event)
        
        # Keep only last 1000 events
        if len(self.events) > 1000:
            self.events = self.events[-1000:]
            
        # Log based on severity
        if level == 'CRITICAL':
            self.logger.critical(f"[{component}] {message}")
            # Trigger immediate alert
            await self._trigger_alert(event)
        elif level == 'WARNING':
            self.logger.warning(f"[{component}] {message}")
        else:
            self.logger.info(f"[{component}] {message}")
            
    async def _trigger_alert(self, event: SecurityEvent):
        """Trigger security alert"""
        # Implement alerting logic (email, SMS, etc.)
        pass
        
    def get_security_report(self) -> Dict:
        """Generate security report"""
        return {
            'total_events': len(self.events),
            'critical_events': len([e for e in self.events if e.level == 'CRITICAL']),
            'metrics': self.metrics.copy(),
            'recent_events': [
                {
                    'timestamp': e.timestamp,
                    'level': e.level,
                    'component': e.component,
                    'message': e.message
                }
                for e in self.events[-10:]  # Last 10 events
            ]
        }

### [FILE] monitoring/hft_performance.py

import asyncio
import time
import psutil
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger('hft_monitor')

@dataclass
class HFTMetrics:
    """HFT performance metrics container"""
    message_rate: float
    processing_latency: float
    queue_depth: int
    memory_usage: float
    cpu_usage: float
    network_latency: float
    error_rate: float
    clock_skew: float

class HFTPerformanceMonitor:
    """High-Frequency Trading Performance Monitor"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.running = False
        self.metrics_history: List[HFTMetrics] = []
        self.performance_thresholds = {
            'max_message_rate': 1000,  # messages/second
            'max_processing_latency': 0.01,  # 10ms
            'max_queue_depth': 500,
            'max_memory_mb': 800,
            'max_cpu_percent': 80,
            'max_network_latency': 0.1,  # 100ms
            'max_error_rate': 0.01,  # 1%
            'max_clock_skew': 0.1  # 100ms
        }
        
        # HFT optimization state
        self.optimization_level = 0
        self.last_optimization = time.time()
        
    async def start_monitoring(self):
        """Start HFT performance monitoring"""
        self.running = True
        logger.info("🚀 Starting HFT Performance Monitor")
        
        while self.running:
            try:
                metrics = await self._collect_metrics()
                self.metrics_history.append(metrics)
                
                # Keep only recent history for HFT
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                
                # Check performance thresholds
                await self._check_performance_thresholds(metrics)
                
                # Optimize based on metrics
                await self._optimize_performance(metrics)
                
                await asyncio.sleep(1)  # High-frequency monitoring
                
            except Exception as e:
                logger.error(f"HFT monitoring error: {e}")
                await asyncio.sleep(5)
    
    async def _collect_metrics(self) -> HFTMetrics:
        """Collect comprehensive HFT performance metrics"""
        # System metrics
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=0.1)  # Shorter interval for HFT
        
        # Application-specific metrics (would be populated from actual components)
        message_rate = self._calculate_message_rate()
        processing_latency = self._calculate_processing_latency()
        queue_depth = self._get_queue_depths()
        
        return HFTMetrics(
            message_rate=message_rate,
            processing_latency=processing_latency,
            queue_depth=queue_depth,
            memory_usage=memory.percent,
            cpu_usage=cpu_percent,
            network_latency=await self._measure_network_latency(),
            error_rate=self._calculate_error_rate(),
            clock_skew=self._get_clock_skew()
        )
    
    async def _check_performance_thresholds(self, metrics: HFTMetrics):
        """Check if performance metrics exceed thresholds"""
        alerts = []
        
        if metrics.message_rate > self.performance_thresholds['max_message_rate']:
            alerts.append(f"High message rate: {metrics.message_rate:.1f}/s")
            
        if metrics.processing_latency > self.performance_thresholds['max_processing_latency']:
            alerts.append(f"High processing latency: {metrics.processing_latency*1000:.1f}ms")
            
        if metrics.queue_depth > self.performance_thresholds['max_queue_depth']:
            alerts.append(f"High queue depth: {metrics.queue_depth}")
            
        if metrics.memory_usage > self.performance_thresholds['max_memory_mb']:
            alerts.append(f"High memory usage: {metrics.memory_usage}%")
            
        if metrics.cpu_usage > self.performance_thresholds['max_cpu_percent']:
            alerts.append(f"High CPU usage: {metrics.cpu_usage}%")
            
        if metrics.network_latency > self.performance_thresholds['max_network_latency']:
            alerts.append(f"High network latency: {metrics.network_latency*1000:.1f}ms")
            
        if metrics.error_rate > self.performance_thresholds['max_error_rate']:
            alerts.append(f"High error rate: {metrics.error_rate*100:.1f}%")
            
        if metrics.clock_skew > self.performance_thresholds['max_clock_skew']:
            alerts.append(f"High clock skew: {metrics.clock_skew*1000:.1f}ms")
        
        if alerts:
            logger.warning(f"🚨 HFT Performance Alerts: {', '.join(alerts)}")
    
    async def _optimize_performance(self, metrics: HFTMetrics):
        """Apply HFT performance optimizations based on current metrics"""
        current_time = time.time()
        
        # Only optimize every 30 seconds to prevent thrashing
        if current_time - self.last_optimization < 30:
            return
            
        new_optimization_level = 0
        
        # Level 1: Moderate optimizations
        if (metrics.cpu_usage > 70 or metrics.memory_usage > 70 or 
            metrics.processing_latency > 0.005):
            new_optimization_level = 1
            
        # Level 2: Aggressive optimizations  
        if (metrics.cpu_usage > 85 or metrics.memory_usage > 85 or
            metrics.processing_latency > 0.01 or metrics.error_rate > 0.05):
            new_optimization_level = 2
            
        # Level 3: Emergency optimizations
        if (metrics.cpu_usage > 95 or metrics.memory_usage > 95 or
            metrics.processing_latency > 0.02 or metrics.error_rate > 0.1):
            new_optimization_level = 3
        
        if new_optimization_level != self.optimization_level:
            await self._apply_optimizations(new_optimization_level)
            self.optimization_level = new_optimization_level
            self.last_optimization = current_time
    
    async def _apply_optimizations(self, level: int):
        """Apply specific optimizations based on level"""
        if level == 0:
            logger.info("✅ HFT Performance: Normal mode")
            # Reset to default settings
        elif level == 1:
            logger.info("🔧 HFT Performance: Level 1 optimizations")
            # Reduce polling frequency, increase timeouts slightly
        elif level == 2:
            logger.info("⚡ HFT Performance: Level 2 optimizations") 
            # Aggressive: Reduce symbol count, increase polling intervals
        elif level == 3:
            logger.info("🚨 HFT Performance: Level 3 emergency optimizations")
            # Emergency: Minimal operation, maximum timeouts
    
    def _calculate_message_rate(self) -> float:
        """Calculate current message processing rate"""
        if len(self.metrics_history) < 2:
            return 0
        recent_metrics = self.metrics_history[-10:]
        return np.mean([m.message_rate for m in recent_metrics])
    
    def _calculate_processing_latency(self) -> float:
        """Calculate average processing latency"""
        if len(self.metrics_history) < 2:
            return 0
        recent_metrics = self.metrics_history[-10:]
        return np.mean([m.processing_latency for m in recent_metrics])
    
    def _get_queue_depths(self) -> int:
        """Get current queue depths from system components"""
        # This would integrate with actual queue monitoring
        return 0
    
    async def _measure_network_latency(self) -> float:
        """Measure current network latency"""
        try:
            # Simplified latency measurement
            return 0.05  # 50ms default
        except:
            return 0.1
    
    def _calculate_error_rate(self) -> float:
        """Calculate current error rate"""
        if len(self.metrics_history) < 10:
            return 0
        recent_metrics = self.metrics_history[-10:]
        return np.mean([m.error_rate for m in recent_metrics])
    
    def _get_clock_skew(self) -> float:
        """Get current clock skew estimate"""
        # This would integrate with Binance client clock sync
        return 0.0
    
    def get_performance_report(self) -> Dict:
        """Generate HFT performance report"""
        if not self.metrics_history:
            return {}
            
        recent_metrics = self.metrics_history[-10:]
        
        return {
            'optimization_level': self.optimization_level,
            'avg_message_rate': np.mean([m.message_rate for m in recent_metrics]),
            'avg_processing_latency_ms': np.mean([m.processing_latency * 1000 for m in recent_metrics]),
            'max_memory_usage': max([m.memory_usage for m in recent_metrics]),
            'max_cpu_usage': max([m.cpu_usage for m in recent_metrics]),
            'avg_network_latency_ms': np.mean([m.network_latency * 1000 for m in recent_metrics]),
            'avg_error_rate_percent': np.mean([m.error_rate * 100 for m in recent_metrics]),
            'monitoring_duration': len(self.metrics_history)
        }


### [FILE] core/quantum_overlays.py

import numpy as np
from ml.model import QuantumModel

class QuantumOverlay:
    def __init__(self, client, symbols, model: QuantumModel):
        self.client = client
        self.symbols = symbols
        self.model = model
        self.prediction_window = 20
        self.signal_threshold = 0.7

    def simulate_quantum_prediction(self, prices, chaos_label=None):
        if len(prices) < 3:
            return 0.5  # Neutral

        features = np.array(prices[-3:]).reshape(1, -1)

        if chaos_label is not None:
            chaos_numeric = 1.0 if chaos_label == "chaotic" else 0.0
            features = np.append(features, [[chaos_numeric]], axis=1)

        prediction = self.model.predict(features)[0]
        return prediction

### [FILE] utils/fast_math.py

import numpy as np

def calculate_path_profit(path, bid_ask_cache):
    """Hand-optimized profit calculation"""
    try:
        a, b, c = path
        # Pre-allocate numpy arrays for vector math
        bids = np.array([
            bid_ask_cache[a]['bids'][0][0],
            bid_ask_cache[b]['asks'][0][0],
            bid_ask_cache[c]['asks'][0][0]
        ], dtype=np.float64)
        return np.prod(bids) - 1
    except:
        return -1.0

def vectorized_volatility(prices):
    """Faster alternative to pandas rolling stats"""
    prices = np.asarray(prices, dtype=np.float64)
    returns = np.diff(np.log(prices))
    return np.std(returns) * np.sqrt(len(returns))


### [FILE] utils/watchdog.py
import json
import os
import asyncio
from datetime import datetime
import sys
import time
import requests
import psutil
import gc
import zlib
from typing import Dict, Any

class RecoveryWatchdog:
    def __init__(self, bot):
        self.bot = bot
        self.last_save = datetime.now()
        self.running = True
        
    async def run(self):
        """Main monitoring loop with secure state serialization"""
        while self.running:
            try:
                await self._monitor()
            except Exception as e:
                print(f"Watchdog error: {e}")
                await asyncio.sleep(60)

    async def _monitor(self):
        """Check both system health and spot instance status"""
        # 1. Auto-save compressed state every 5 min
        if (datetime.now() - self.last_save).seconds >= 300:
            await self._save_secure_state()

        # 2. Check for frozen bot
        if time.time() - self.bot.last_activity > 300:
            print("[WATCHDOG] Bot frozen - restarting")
            os.execv(sys.executable, [sys.executable] + sys.argv)

        # 3. Check for spot termination notice
        await self._check_spot_termination()

    async def _check_spot_termination(self):
        """Check for AWS spot termination with timeout"""
        try:
            resp = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None, 
                    lambda: requests.get(
                        'http://169.254.169.254/latest/meta-data/spot/instance-action',
                        timeout=2
                    )
                ),
                timeout=3.0
            )
            if resp.status_code == 200:
                print("[WATCHDOG] Spot termination detected! Saving state...")
                await self._save_secure_state()
                os._exit(1)  # Force restart
        except (requests.RequestException, asyncio.TimeoutError):
            pass  # Expected if not on spot instance

    async def _save_secure_state(self):
        """Save state with secure JSON serialization and compression"""
        try:
            state = {
                'active_trades': [
                    {
                        'symbol': trade.get('symbol'),
                        'timestamp': trade.get('timestamp'),
                        'profit': float(trade.get('profit', 0)),
                        # Only include safe, serializable data
                    }
                    for trade in list(self.bot.arb_engine.trade_history)
                    if self._is_serializable_trade(trade)
                ],
                'threshold': float(self.bot.arb_engine.current_threshold),
                'hedges': {
                    k: float(v) for k, v in self.bot.hedger.open_hedges.items()
                },
                'save_timestamp': datetime.utcnow().isoformat()
            }
            
            # Validate state before serialization
            if not self._validate_state(state):
                raise ValueError("State validation failed")
            
            # Serialize with JSON and compress
            json_data = json.dumps(state, separators=(',', ':'), ensure_ascii=False)
            compressed = zlib.compress(json_data.encode('utf-8'))
            
            # Secure file rotation
            await self._rotate_secure_state_files(compressed)
                
            self.last_save = datetime.now()
            
        except Exception as e:
            print(f"[WATCHDOG] Secure state save failed: {e}")

    def _is_serializable_trade(self, trade: Any) -> bool:
        """Validate trade data is safe for serialization"""
        try:
            if not isinstance(trade, dict):
                return False
                
            # Check for required fields and types
            required = ['symbol', 'timestamp']
            if not all(field in trade for field in required):
                return False
                
            # Validate data types
            if not isinstance(trade['symbol'], str):
                return False
            if not isinstance(trade['timestamp'], (int, float)):
                return False
                
            return True
        except:
            return False

    def _validate_state(self, state: Dict) -> bool:
        """Validate state before serialization"""
        try:
            # Check structure
            if not isinstance(state, dict):
                return False
                
            required_keys = ['active_trades', 'threshold', 'hedges', 'save_timestamp']
            if not all(key in state for key in required_keys):
                return False
                
            # Validate data types
            if not isinstance(state['active_trades'], list):
                return False
            if not isinstance(state['threshold'], float):
                return False
            if not isinstance(state['hedges'], dict):
                return False
                
            return True
        except:
            return False

    async def _rotate_secure_state_files(self, compressed_data: bytes):
        """Rotate state files with secure permissions"""
        try:
            # Rotate state files (keep last 3)
            for i in range(2, -1, -1):
                filename = f'state_{i}.json.gz'
                if os.path.exists(filename):
                    if i == 2:
                        os.remove(filename)
                    else:
                        os.rename(filename, f'state_{i+1}.json.gz')
                        
            # Write new state with secure permissions
            with open('state_0.json.gz', 'wb') as f:
                f.write(compressed_data)
            os.chmod('state_0.json.gz', 0o600)  # Read/write owner only
                
        except Exception as e:
            print(f"[WATCHDOG] File rotation failed: {e}")

    def load_state(self):
        """Load most recent secure state"""
        for i in range(3):
            try:
                filename = f'state_{i}.json.gz'
                if os.path.exists(filename):
                    with open(filename, 'rb') as f:
                        decompressed = zlib.decompress(f.read())
                        state = json.loads(decompressed.decode('utf-8'))
                        
                    # Validate loaded state
                    if self._validate_state(state):
                        return state
            except (FileNotFoundError, EOFError, zlib.error, json.JSONDecodeError) as e:
                continue
        return None

### [FILE] utils/parallel.py

import threading
from queue import Queue

class ThreadPool:
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.task_queue = Queue()
        self.results = []

    def _worker(self):
        while True:
            func, args = self.task_queue.get()
            try:
                self.results.append(func(*args))
            except Exception as e:
                print(f"Thread error: {e}")
            finally:
                self.task_queue.task_done()

    def map(self, func, iterable):
        for item in iterable:
            self.task_queue.put((func, (item,)))
        
        for _ in range(self.max_workers):
            t = threading.Thread(target=self._worker, daemon=True)
            t.start()
        
        self.task_queue.join()
        return self.results

### [FILE] utils/log_uploader.py
import boto3
from datetime import datetime
import os

class LogUploader:
    def __init__(self):
        self.client = boto3.client('logs')
        self.log_group = "arbitrage-bot"
        self._ensure_log_group()

    def _ensure_log_group(self):
        try:
            self.client.create_log_group(logGroupName=self.log_group)
        except self.client.exceptions.ResourceAlreadyExistsException:
            pass

    def upload(self, log_type="operations"):
        """Upload logs to CloudWatch"""
        log_stream = f"{log_type}-{datetime.now().strftime('%Y%m%d')}"
        
        try:
            self.client.create_log_stream(
                logGroupName=self.log_group,
                logStreamName=log_stream
            )
        except self.client.exceptions.ResourceAlreadyExistsException:
            pass

        with open(f"logs/{'bot_operations.log' if log_type == 'operations' else 'trades.json'}") as f:
            self.client.put_log_events(
                logGroupName=self.log_group,
                logStreamName=log_stream,
                logEvents=[{
                    'timestamp': int(time.time() * 1000),
                    'message': line.strip()
                } for line in f.readlines() if line.strip()]
            )



### [FILE] utils/config_loader.py

import yaml
import os
import logging
import time
from typing import Dict, Any
import threading
import hashlib
import asyncio

logger = logging.getLogger('config_loader')

class ConfigLoader:
    """✅ FIXED: Thread-safe configuration loader with caching, robust validation, and enhanced fallback logic"""
    
    _instance = None
    _lock = asyncio.Lock()  # ✅ FIXED: Changed to asyncio lock
    _config_cache = None
    _last_load_time = 0
    _cache_duration = 30  # Cache config for 30 seconds
    
    def __new__(cls):
        with threading.Lock():
            if cls._instance is None:
                cls._instance = super(ConfigLoader, cls).__new__(cls)
            return cls._instance
            
    def __init__(self):
        self._local_cache = {}
        self._config_hash = None
        # ✅ FIXED: Enhanced thread safety
        self._load_lock = asyncio.Lock()
        self._validation_lock = asyncio.Lock()
        self._fallback_used = False
        self._fallback_reason = ""
        
    async def load_config(self, config_path: str = "config/config.yaml") -> Dict[str, Any]:
        """✅ FIXED: Async configuration loading with caching, race condition protection, and robust fallback"""
        current_time = time.time()
        
        # Check cache first
        if (self._config_cache and 
            current_time - self._last_load_time < self._cache_duration and
            not self._fallback_used):
            return self._config_cache.copy()
        
        async with self._load_lock:  # ✅ FIXED: Single loader at a time
            # Double-check cache after acquiring lock
            if (self._config_cache and 
                current_time - self._last_load_time < self._cache_duration and
                not self._fallback_used):
                return self._config_cache.copy()
                
            try:
                # Ensure config directory exists
                config_dir = os.path.dirname(config_path)
                if config_dir and not os.path.exists(config_dir):
                    try:
                        os.makedirs(config_dir, exist_ok=True)
                        logger.info(f"Created config directory: {config_dir}")
                    except (OSError, PermissionError) as e:
                        self._fallback_used = True
                        self._fallback_reason = f"Config directory creation failed: {e}"
                        logger.warning(f"⚠️ {self._fallback_reason}, using defaults")
                        return await self.get_safe_default_config()
                
                # Load configuration file
                if not os.path.exists(config_path):
                    self._fallback_used = True
                    self._fallback_reason = f"Config file not found at {config_path}"
                    logger.warning(f"⚠️ {self._fallback_reason}, using safe defaults")
                    return await self.get_safe_default_config()
                
                try:
                    with open(config_path, "r", encoding='utf-8') as f:
                        config_content = f.read()
                        config = yaml.safe_load(config_content)
                        
                    if not config or not isinstance(config, dict):
                        self._fallback_used = True
                        self._fallback_reason = "Invalid configuration file format (empty or not dict)"
                        logger.error(f"❌ {self._fallback_reason}")
                        return await self.get_safe_default_config()
                        
                    # Validate and optimize config
                    config = await self._validate_and_optimize_config(config)
                    
                    # Reset fallback flag on successful load
                    self._fallback_used = False
                    self._fallback_reason = ""
                    
                except (yaml.YAMLError, UnicodeDecodeError) as e:
                    self._fallback_used = True
                    self._fallback_reason = f"Config file parsing failed: {e}"
                    logger.error(f"❌ {self._fallback_reason}, using defaults")
                    return await self.get_safe_default_config()
                except (IOError, OSError, PermissionError) as e:
                    self._fallback_used = True
                    self._fallback_reason = f"Config file access failed: {e}"
                    logger.error(f"❌ {self._fallback_reason}, using defaults")
                    return await self.get_safe_default_config()
                
                # Cache the config
                self._config_cache = config.copy()
                self._last_load_time = current_time
                self._config_hash = await self._calculate_config_hash(config)
                
                logger.info(f"✅ Config loaded: {len(config.get('symbols', []))} symbols")
                return config
                
            except Exception as e:
                self._fallback_used = True
                self._fallback_reason = f"Unexpected config loading error: {e}"
                logger.error(f"❌ {self._fallback_reason}, using defaults")
                return await self.get_safe_default_config()
    
    async def _validate_and_optimize_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """✅ FIXED: Async config validation and optimization with thread safety and enhanced error handling"""
        async with self._validation_lock:
            try:
                # Ensure all required sections exist
                required_sections = ['binance', 'trading', 'performance', 'risk_management']
                for section in required_sections:
                    if section not in config:
                        config[section] = {}
                        logger.warning(f"Missing section {section}, created with defaults")
                
                # API key validation with environment fallback
                api_key = (config['binance'].get('api_key') or 
                          os.getenv('BINANCE_API_KEY') or 
                          os.getenv('BINANCE_API_KEY_TEST', ''))
                
                api_secret = (config['binance'].get('api_secret') or 
                             os.getenv('BINANCE_API_SECRET') or 
                             os.getenv('BINANCE_API_SECRET_TEST', ''))
                
                # Security validation with graceful degradation
                if (not api_key or not api_secret or 
                    len(api_key) < 20 or len(api_secret) < 20):
                    
                    logger.error("❌ Invalid API credentials - using testnet with safe defaults")
                    safe_config = await self.get_safe_default_config()
                    # Preserve any valid settings from the original config
                    for key in safe_config:
                        if key in config and isinstance(config[key], dict):
                            safe_config[key].update(config[key])
                    safe_config['binance']['testnet'] = True
                    safe_config['trading']['live_trading'] = False
                    return safe_config
                
                config['binance']['api_key'] = api_key.strip()
                config['binance']['api_secret'] = api_secret.strip()
                
                # Trading mode determination with safety checks
                live_trading = config['trading'].get('live_trading', False)
                dry_run = config['trading'].get('dry_run', True)
                
                # ✅ FIXED: Enhanced safety - never allow live trading with test credentials
                if "test" in api_key.lower() or "test" in api_secret.lower():
                    live_trading = False
                    dry_run = True
                    logger.warning("🔒 Test credentials detected, forcing paper trading mode")
                
                if (not live_trading or dry_run or not api_key or not api_secret):
                    config['binance']['testnet'] = True
                    config['trading']['live_trading'] = False
                    config['trading']['dry_run'] = True
                    logger.info("📄 PAPER TRADING MODE (safe default)")
                else:
                    config['binance']['testnet'] = False
                    config['trading']['dry_run'] = False
                    logger.critical("🚀 LIVE TRADING MODE ACTIVATED")
                
                # Symbol validation and optimization with bounds checking
                symbols = config.get('symbols', [])
                if not symbols or not isinstance(symbols, list):
                    symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC', 'SOLUSDC', 'XRPUSDC', 'ADAUSDC', 'DOGEUSDC', 'DOTUSDC']
                    logger.warning("⚠️ Invalid symbols list, using defaults")
                
                # Free Tier symbol limit enforcement with validation
                valid_symbols = []
                for symbol in symbols:
                    if isinstance(symbol, str) and len(symbol) <= 20 and symbol.upper() == symbol:
                        valid_symbols.append(symbol)
                    if len(valid_symbols) >= 8:  # Hard limit for free tier
                        break
                
                if len(valid_symbols) < 3:
                    # Emergency fallback if too few valid symbols
                    valid_symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC', 'SOLUSDC']
                    logger.warning("🆘 Insufficient valid symbols, using emergency defaults")
                
                config['symbols'] = valid_symbols
                
                # Numeric validation with bounds checking and fallbacks
                try:
                    trading = config['trading']
                    
                    # Safe numeric conversions with bounds
                    trading['min_order_size'] = max(10.0, float(trading.get('min_order_size', 10)))
                    trading['max_order_size'] = max(100.0, float(trading.get('max_order_size', 20000)))
                    trading['position_size_percent'] = max(0.01, min(1.0, float(trading.get('position_size_percent', 0.15))))
                    trading['max_position_size'] = max(100.0, float(trading.get('max_position_size', 15000)))
                    trading['fee'] = max(0.0001, min(0.1, float(trading.get('fee', 0.001))))
                    
                    risk = config['risk_management']
                    risk['max_risk_per_trade'] = max(0.01, min(1.0, float(risk.get('max_risk_per_trade', 0.15))))
                    risk['min_profit_threshold'] = max(0.001, min(0.1, float(risk.get('min_profit_threshold', 0.005))))
                    risk['daily_loss_limit'] = min(-10.0, float(risk.get('daily_loss_limit', -100)))
                    
                    # Free Tier performance limits with safety margins
                    performance = config['performance']
                    polling_interval = max(2.0, min(60.0, float(performance.get('polling_interval', 5.0))))
                    if polling_interval < 2.0:
                        logger.warning("Free Tier: Increasing polling interval to 2.0s minimum")
                        polling_interval = 2.0
                    performance['polling_interval'] = polling_interval
                    
                    performance['max_active_pairs'] = max(1, min(8, int(performance.get('max_active_pairs', 8))))
                    performance['cache_size'] = max(10, min(500, int(performance.get('cache_size', 50))))
                    
                    config.setdefault('chaos', {})
                    config['chaos']['window_size'] = max(10, min(500, int(config['chaos'].get('window_size', 20))))
                    
                except (ValueError, TypeError) as e:
                    logger.error(f"Invalid numeric config: {e}, using safe defaults")
                    # Apply safe defaults for corrupted numeric values
                    safe_defaults = await self.get_safe_default_config()
                    for key in ['trading', 'risk_management', 'performance']:
                        if key in safe_defaults:
                            config[key] = {**safe_defaults[key], **config.get(key, {})}
                
                # Final validation with fallback
                if not await self.validate_config(config):
                    logger.error("Config validation failed, using safe defaults")
                    return await self.get_safe_default_config()
                    
                return config
                
            except Exception as e:
                logger.error(f"Config validation error: {e}, using safe defaults")
                return await self.get_safe_default_config()
    
    async def _calculate_config_hash(self, config: Dict[str, Any]) -> str:
        """✅ FIXED: Async config hash calculation"""
        try:
            config_str = yaml.dump(config, sort_keys=True)
            return hashlib.md5(config_str.encode()).hexdigest()
        except Exception as e:
            logger.error(f"Config hash calculation failed: {e}")
            return "error_hash"
    
    async def has_config_changed(self) -> bool:
        """✅ FIXED: Async config change detection with fallback handling"""
        if not self._config_hash or self._fallback_used:
            return True
            
        try:
            async with self._load_lock:
                if not os.path.exists("config/config.yaml"):
                    return True
                    
                with open("config/config.yaml", "r", encoding='utf-8') as f:
                    current_config = yaml.safe_load(f)
                current_hash = await self._calculate_config_hash(current_config)
                return current_hash != self._config_hash
        except Exception as e:
            logger.error(f"Config change detection failed: {e}")
            return True  # Assume changed on error
    
    async def get_safe_default_config(self) -> Dict[str, Any]:
        """✅ FIXED: Enhanced safe default configuration with comprehensive settings"""
        return {
            'mode': 'paper',
            'free_tier': True,
            'binance': {
                'testnet': True, 
                'rate_limit': 2,
                'api_key': 'test_key_fallback',
                'api_secret': 'test_secret_fallback'
            },
            'symbols': ['BTCUSDC', 'ETHUSDC', 'BNBUSDC', 'SOLUSDC', 'XRPUSDC', 'ADAUSDC', 'DOGEUSDC', 'DOTUSDC'],
            'trading': {
                'live_trading': False, 
                'dry_run': True,
                'min_order_size': 10, 
                'max_order_size': 20000,
                'position_size_percent': 0.15, 
                'max_position_size': 15000, 
                'fee': 0.001,
                'emergency_stop': True
            },
            'risk_management': {
                'max_risk_per_trade': 0.15, 
                'min_profit_threshold': 0.005, 
                'daily_loss_limit': -100,
                'max_drawdown': 0.25,
                'circuit_breaker': True
            },
            'performance': {
                'polling_interval': 5.0, 
                'max_active_pairs': 8, 
                'cache_size': 50,
                'memory_limit_mb': 800,
                'cpu_threshold': 0.7
            },
            'chaos': {
                'window_size': 20,
                'fallback_mode': True
            },
            'dependency_recovery': {
                'enabled': True,
                'max_retries': 3,
                'backoff_multiplier': 2.0,
                'degraded_mode_timeout': 300
            }
        }
    
    async def validate_config(self, config: Dict[str, Any]) -> bool:
        """✅ FIXED: Enhanced config validation with comprehensive checks"""
        try:
            required_sections = ['binance', 'trading', 'performance', 'risk_management']
            if not all(section in config for section in required_sections):
                logger.error("Missing required config sections")
                return False
            
            symbols = config.get('symbols', [])
            if len(symbols) == 0 or len(symbols) > 10:
                logger.error(f"Invalid symbols count: {len(symbols)}")
                return False
                
            trading = config['trading']
            if (trading.get('min_order_size', 0) <= 0 or 
                trading.get('max_order_size', 0) <= trading.get('min_order_size', 1)):
                logger.error("Invalid order size configuration")
                return False
                
            performance = config['performance']
            if (performance.get('polling_interval', 0) <= 0 or
                performance.get('max_active_pairs', 0) <= 0):
                logger.error("Invalid performance configuration")
                return False
                
            # Additional safety checks
            if config.get('trading', {}).get('live_trading', False):
                api_key = config.get('binance', {}).get('api_key', '')
                if 'test' in api_key.lower():
                    logger.error("Live trading with test credentials detected")
                    return False
                    
            return True
            
        except Exception as e:
            logger.error(f"Config validation error: {e}")
            return False

    def get_loader_status(self) -> Dict[str, Any]:
        """Get loader status including fallback information"""
        return {
            'fallback_used': self._fallback_used,
            'fallback_reason': self._fallback_reason,
            'cache_valid': self._config_cache is not None,
            'last_load_time': self._last_load_time,
            'cache_age_seconds': time.time() - self._last_load_time if self._last_load_time else 0
        }

# Global instance
_config_loader = ConfigLoader()

async def load_config(config_path: str = "config/config.yaml") -> Dict[str, Any]:
    """✅ FIXED: Async public interface for loading configuration"""
    return await _config_loader.load_config(config_path)

async def has_config_changed() -> bool:
    """✅ FIXED: Async config change check"""
    return await _config_loader.has_config_changed()

def get_config_loader_status() -> Dict[str, Any]:
    """Get the current config loader status"""
    return _config_loader.get_loader_status()


### [FILE] utils/cache.py
# Time-based caching system
import time

class Cache:
    def __init__(self, ttl=2.0, max_size=32):
        self.ttl = ttl
        self.max_size = max_size
        self._cache = {}
        self._timestamps = {}

    def get(self, key):
        now = time.time()
        if key in self._cache and (now - self._timestamps[key]) < self.ttl:
            return self._cache[key]
        return None

    def set(self, key, value):
        if len(self._cache) >= self.max_size:
            oldest = min(self._timestamps, key=self._timestamps.get)
            del self._cache[oldest]
            del self._timestamps[oldest]
        self._cache[key] = value
        self._timestamps[key] = time.time()

### [FILE] utils/advanced_logger.py
import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import json
import gzip
import os
from datetime import datetime
import psutil
import threading
import time
import asyncio
from typing import Dict, Any
from collections import deque

class MemoryAwareTradingLogger:
    """💀 ULTIMATE: Fixed log rotation with memory-aware management"""
    
    def __init__(self, free_tier=True, live_trading=False):
        self.free_tier = free_tier
        self.live_trading = live_trading
        self.trade_log_path = "logs/trades.json.gz"
        self.operations_log_path = "logs/bot_operations.log"
        self.daily_log_path = "logs/bot_daily.log"
        
        # Create logs directory
        os.makedirs("logs", exist_ok=True)
        
        # Memory management
        self._log_mutex = threading.RLock()
        self._async_mutex = asyncio.Lock()
        self._last_cleanup = time.time()
        self._log_count = 0
        self._total_log_size = 0
        self._cleanup_interval = 1800  # 30 minutes
        
        # Memory monitoring
        self._memory_check_interval = 300  # 5 minutes
        self._last_memory_check = time.time()
        
        # Log rotation limits
        self._max_trade_log_size = 50 * 1024 * 1024  # 50MB max for trade logs
        self._max_operations_log_size = 20 * 1024 * 1024  # 20MB max for operations
        
        # Trade history with size limits
        self._trade_history = deque(maxlen=1000)  # Keep only last 1000 trades
        
        self._setup_operation_logs()
        
        # Live trading warning
        if self.live_trading:
            self.logger.warning("🚀 LIVE TRADING MODE ACTIVATED - REAL MONEY AT RISK")

    def _setup_operation_logs(self):
        """Setup logging with proper rotation and memory awareness"""
        # Remove existing handlers to avoid duplicates
        logger = logging.getLogger('quantum_bot_live')
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
            handler.close()
        
        logger.setLevel(logging.INFO)

        # Enhanced formatter with memory info
        formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | [MEM:%(memory)dMB|CPU:%(cpu).1f%%] | [LIVE] | %(message)s' 
            if self.live_trading else 
            '%(asctime)s | %(levelname)-8s | [MEM:%(memory)dMB|CPU:%(cpu).1f%%] | [PAPER] | %(message)s'
        )

        # ✅ FIXED: Proper log rotation with strict size limits
        file_handler = RotatingFileHandler(
            filename=self.operations_log_path,
            maxBytes=10 * 1024 * 1024,  # 10MB max
            backupCount=3,  # Keep only 3 backup files for free tier
            encoding='utf-8'
        )
        file_handler.setFormatter(formatter)

        # Time-based rotation for daily logs
        time_handler = TimedRotatingFileHandler(
            filename=self.daily_log_path,
            when='midnight',
            interval=1,
            backupCount=3,  # Keep 3 days of logs
            encoding='utf-8'
        )
        time_handler.setFormatter(formatter)

        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(time_handler)
        logger.addHandler(console_handler)

        # Add memory and CPU filter
        class SystemMetricsFilter(logging.Filter):
            def filter(self, record):
                try:
                    process = psutil.Process()
                    memory_info = process.memory_info()
                    record.memory = memory_info.rss // 1024 // 1024
                    record.cpu = psutil.cpu_percent(interval=0.1)
                except (psutil.NoSuchProcess, AttributeError):
                    record.memory = 0
                    record.cpu = 0.0
                return True
        
        for handler in logger.handlers:
            handler.addFilter(SystemMetricsFilter())
            
        self.logger = logger

    async def log_trade_async(self, trade_data: Dict[str, Any]):
        """✅ FIXED: Async trade logging with memory-aware rotation"""
        async with self._async_mutex:
            await self._perform_async_cleanup_check()
            await self._log_trade_internal(trade_data)

    def log_trade(self, trade_data: Dict[str, Any]):
        """✅ FIXED: Synchronous trade logging with memory limits"""
        with self._log_mutex:
            self._perform_cleanup_check()
            self._log_trade_internal_sync(trade_data)

    async def _log_trade_internal(self, trade_data: Dict[str, Any]):
        """Internal async trade logging with memory limits"""
        try:
            self._log_count += 1
            
            trade = {
                "timestamp": datetime.utcnow().isoformat(),
                "live_trade": self.live_trading,
                "status": "win" if trade_data.get("profit", 0) >= 0 else "loss",
                **trade_data
            }

            # Add to in-memory history (size-limited)
            self._trade_history.append(trade)
            
            # Check if we need to write to disk
            if len(self._trade_history) >= 100 or self._should_rotate_logs():
                await self._write_trade_history_to_disk()
                
            # Live trading alerts
            if self.live_trading:
                await self._log_live_trade_alert_async(trade)
                
        except Exception as e:
            self.logger.error(f"Async trade log failed: {e}")

    def _log_trade_internal_sync(self, trade_data: Dict[str, Any]):
        """Internal sync trade logging with memory limits"""
        try:
            self._log_count += 1
            
            trade = {
                "timestamp": datetime.utcnow().isoformat(),
                "live_trade": self.live_trading,
                "status": "win" if trade_data.get("profit", 0) >= 0 else "loss",
                **trade_data
            }

            # Add to in-memory history
            self._trade_history.append(trade)
            
            # Check if we need to write to disk
            if len(self._trade_history) >= 100 or self._should_rotate_logs():
                self._write_trade_history_to_disk_sync()
                
            # Live trading alerts
            if self.live_trading:
                self._log_live_trade_alert_sync(trade)
                
        except Exception as e:
            self.logger.error(f"Sync trade log failed: {e}")

    async def _write_trade_history_to_disk(self):
        """Write trade history to disk with compression and rotation"""
        if not self._trade_history:
            return
            
        try:
            # Check current log size
            current_size = 0
            if os.path.exists(self.trade_log_path):
                current_size = os.path.getsize(self.trade_log_path)
                
            # Rotate if too large
            if current_size > self._max_trade_log_size:
                await self._rotate_trade_logs_async()
                
            # Write compressed log
            trades_list = list(self._trade_history)
            with gzip.open(self.trade_log_path, 'wt') as f:
                json.dump(trades_list, f, separators=(',', ':'))
                
            # Clear in-memory history after successful write
            self._trade_history.clear()
            
            # Update size tracking
            self._total_log_size = os.path.getsize(self.trade_log_path) if os.path.exists(self.trade_log_path) else 0
                
        except Exception as e:
            self.logger.error(f"Trade history write failed: {e}")

    def _write_trade_history_to_disk_sync(self):
        """Synchronous version of trade history writing"""
        if not self._trade_history:
            return
            
        try:
            current_size = 0
            if os.path.exists(self.trade_log_path):
                current_size = os.path.getsize(self.trade_log_path)
                
            if current_size > self._max_trade_log_size:
                self._rotate_trade_logs_sync()
                
            trades_list = list(self._trade_history)
            with gzip.open(self.trade_log_path, 'wt') as f:
                json.dump(trades_list, f, separators=(',', ':'))
                
            self._trade_history.clear()
            self._total_log_size = os.path.getsize(self.trade_log_path) if os.path.exists(self.trade_log_path) else 0
                
        except Exception as e:
            self.logger.error(f"Sync trade history write failed: {e}")

    def _should_rotate_logs(self) -> bool:
        """Check if logs should be rotated"""
        try:
            # Check operations log size
            if os.path.exists(self.operations_log_path):
                op_size = os.path.getsize(self.operations_log_path)
                if op_size > self._max_operations_log_size:
                    return True
                    
            # Check trade log size  
            if os.path.exists(self.trade_log_path):
                trade_size = os.path.getsize(self.trade_log_path)
                if trade_size > self._max_trade_log_size:
                    return True
                    
            return False
        except:
            return False

    async def _rotate_trade_logs_async(self):
        """Rotate trade logs asynchronously"""
        try:
            if os.path.exists(self.trade_log_path):
                # Create backup with timestamp
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = f"logs/trades_backup_{timestamp}.json.gz"
                os.rename(self.trade_log_path, backup_path)
                self.logger.info(f"🔄 Rotated trade log to {backup_path}")
                
        except Exception as e:
            self.logger.error(f"Trade log rotation failed: {e}")

    def _rotate_trade_logs_sync(self):
        """Synchronous trade log rotation"""
        try:
            if os.path.exists(self.trade_log_path):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = f"logs/trades_backup_{timestamp}.json.gz"
                os.rename(self.trade_log_path, backup_path)
                self.logger.info(f"🔄 Rotated trade log to {backup_path}")
                
        except Exception as e:
            self.logger.error(f"Sync trade log rotation failed: {e}")

    async def _perform_async_cleanup_check(self):
        """Async cleanup check"""
        current_time = time.time()
        if current_time - self._last_cleanup > self._cleanup_interval:
            await self._cleanup_old_logs_async()
            self._last_cleanup = current_time
            
        if current_time - self._last_memory_check > self._memory_check_interval:
            await self._check_memory_usage_async()
            self._last_memory_check = current_time

    def _perform_cleanup_check(self):
        """Synchronous cleanup check"""
        current_time = time.time()
        if current_time - self._last_cleanup > self._cleanup_interval:
            self._cleanup_old_logs_sync()
            self._last_cleanup = current_time
            
        if current_time - self._last_memory_check > self._memory_check_interval:
            self._check_memory_usage_sync()
            self._last_memory_check = current_time

    async def _cleanup_old_logs_async(self):
        """Cleanup old logs asynchronously"""
        try:
            current_time = time.time()
            log_dir = "logs"
            
            for filename in os.listdir(log_dir):
                if filename.startswith('trades_backup_') or filename.endswith('.log'):
                    filepath = os.path.join(log_dir, filename)
                    
                    # Delete files older than 7 days
                    if os.path.isfile(filepath):
                        file_age = current_time - os.path.getmtime(filepath)
                        if file_age > 7 * 24 * 3600:  # 7 days
                            os.remove(filepath)
                            self.logger.info(f"🧹 Cleaned up old log file: {filename}")
                            
        except Exception as e:
            self.logger.error(f"Async log cleanup failed: {e}")

    def _cleanup_old_logs_sync(self):
        """Synchronous old log cleanup"""
        try:
            current_time = time.time()
            log_dir = "logs"
            
            for filename in os.listdir(log_dir):
                if filename.startswith('trades_backup_') or filename.endswith('.log'):
                    filepath = os.path.join(log_dir, filename)
                    
                    if os.path.isfile(filepath):
                        file_age = current_time - os.path.getmtime(filepath)
                        if file_age > 7 * 24 * 3600:
                            os.remove(filepath)
                            self.logger.info(f"🧹 Cleaned up old log file: {filename}")
                            
        except Exception as e:
            self.logger.error(f"Sync log cleanup failed: {e}")

    async def _check_memory_usage_async(self):
        """Async memory usage check"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
            
            if memory_mb > 800:
                self.logger.warning(f"💾 High memory usage: {memory_mb}MB")
                # Force garbage collection
                import gc
                gc.collect()
                
        except Exception as e:
            self.logger.debug(f"Async memory check failed: {e}")

    def _check_memory_usage_sync(self):
        """Synchronous memory usage check"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
            
            if memory_mb > 800:
                self.logger.warning(f"💾 High memory usage: {memory_mb}MB")
                import gc
                gc.collect()
                
        except Exception as e:
            self.logger.debug(f"Sync memory check failed: {e}")

    async def _log_live_trade_alert_async(self, trade):
        """Special async logging for live trades with memory awareness"""
        identifier = trade.get("symbol") or "→".join(trade.get("path", []))
        
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
        except (psutil.NoSuchProcess, AttributeError):
            memory_mb = 0
            
        self.logger.info(
            f"{'🚀 LIVE' if trade.get('live_trade') else '📄 PAPER'} "
            f"TRADE {trade['status'].upper()} | "
            f"{identifier.ljust(12)} | "
            f"Profit: ${trade.get('profit', 0):.2f} | "
            f"Memory: {memory_mb}MB"
        )

    def _log_live_trade_alert_sync(self, trade):
        """Special sync logging for live trades"""
        identifier = trade.get("symbol") or "→".join(trade.get("path", []))
        
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
        except (psutil.NoSuchProcess, AttributeError):
            memory_mb = 0
            
        self.logger.info(
            f"{'🚀 LIVE' if trade.get('live_trade') else '📄 PAPER'} "
            f"TRADE {trade['status'].upper()} | "
            f"{identifier.ljust(12)} | "
            f"Profit: ${trade.get('profit', 0):.2f} | "
            f"Memory: {memory_mb}MB"
        )

    def log_order_execution(self, symbol, side, quantity, price, order_id):
        """Log order execution with memory monitoring"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss // 1024 // 1024
        except (psutil.NoSuchProcess, AttributeError):
            memory_mb = 0
        
        if self.live_trading:
            self.logger.warning(
                f"🎯 LIVE ORDER EXECUTED | {side} {quantity:.6f} {symbol} @ ${price:,.2f} | "
                f"ID: {order_id} | Memory: {memory_mb}MB"
            )
        else:
            self.logger.info(
                f"📄 PAPER ORDER | {side} {quantity:.6f} {symbol} @ ${price:,.2f} | "
                f"Memory: {memory_mb}MB"
            )

    async def get_log_stats_async(self) -> Dict[str, Any]:
        """Get async log statistics"""
        async with self._async_mutex:
            return await self._get_log_stats_internal()

    def get_log_stats(self) -> Dict[str, Any]:
        """Get sync log statistics"""
        with self._log_mutex:
            return self._get_log_stats_internal_sync()

    async def _get_log_stats_internal(self) -> Dict[str, Any]:
        """Internal async stats implementation"""
        try:
            trade_count = 0
            if os.path.exists(self.trade_log_path):
                with gzip.open(self.trade_log_path, 'rt') as f:
                    trades = json.load(f)
                    trade_count = len(trades)
            
            return {
                'trade_count': trade_count,
                'log_count': self._log_count,
                'total_log_size_bytes': self._total_log_size,
                'free_tier': self.free_tier,
                'live_trading': self.live_trading,
                'last_cleanup': self._last_cleanup,
                'in_memory_trades': len(self._trade_history),
                'log_files': await self._get_log_file_info_async()
            }
        except Exception as e:
            self.logger.error(f"Async stats failed: {e}")
            return {'error': str(e)}

    def _get_log_stats_internal_sync(self) -> Dict[str, Any]:
        """Internal sync stats implementation"""
        try:
            trade_count = 0
            if os.path.exists(self.trade_log_path):
                with gzip.open(self.trade_log_path, 'rt') as f:
                    trades = json.load(f)
                    trade_count = len(trades)
            
            return {
                'trade_count': trade_count,
                'log_count': self._log_count,
                'total_log_size_bytes': self._total_log_size,
                'free_tier': self.free_tier,
                'live_trading': self.live_trading,
                'last_cleanup': self._last_cleanup,
                'in_memory_trades': len(self._trade_history),
                'log_files': self._get_log_file_info_sync()
            }
        except Exception as e:
            self.logger.error(f"Sync stats failed: {e}")
            return {'error': str(e)}

    async def _get_log_file_info_async(self) -> Dict[str, Any]:
        """Get async log file information"""
        log_dir = "logs"
        file_info = {}
        
        for filename in os.listdir(log_dir):
            filepath = os.path.join(log_dir, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                file_info[filename] = {
                    'size_bytes': stat.st_size,
                    'modified_time': stat.st_mtime,
                    'age_seconds': time.time() - stat.st_mtime
                }
                
        return file_info

    def _get_log_file_info_sync(self) -> Dict[str, Any]:
        """Get sync log file information"""
        log_dir = "logs"
        file_info = {}
        
        for filename in os.listdir(log_dir):
            filepath = os.path.join(log_dir, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                file_info[filename] = {
                    'size_bytes': stat.st_size,
                    'modified_time': stat.st_mtime,
                    'age_seconds': time.time() - stat.st_mtime
                }
                
        return file_info

    def error(self, message):
        self.logger.error(message)
        
    def info(self, message):
        self.logger.info(message)
        
    def warning(self, message):
        self.logger.warning(message)
        
    def critical(self, message):
        self.logger.critical(message)

    async def cleanup_async(self):
        """Async cleanup"""
        async with self._async_mutex:
            await self._cleanup_old_logs_async()
            self._trade_history.clear()

    def cleanup_sync(self):
        """Synchronous cleanup"""
        with self._log_mutex:
            self._cleanup_old_logs_sync()
            self._trade_history.clear()

# Backward compatibility
TradingLogger = MemoryAwareTradingLogger

### [FILE] sklearn/linear_model.py
import numpy as np

class SGDRegressor:
    """Ultra-lightweight SGD implementation for free-tier"""
    def __init__(self, loss='huber', penalty='l2', max_iter=1000, tol=1e-3):
        self.loss = loss
        self.penalty = penalty
        self.max_iter = max_iter
        self.tol = tol
        self.coef_ = None
        
    def fit(self, X, y):
        # Initialize coefficients
        n_features = X.shape[1] if len(X.shape) > 1 else 1
        self.coef_ = np.zeros(n_features)
        
        # Stochastic Gradient Descent
        for _ in range(self.max_iter):
            for i in range(len(X)):
                pred = np.dot(X[i], self.coef_)
                error = pred - y[i]
                
                # Huber loss gradient
                if self.loss == 'huber':
                    grad = np.where(np.abs(error) <= 1.0,
                                  error,
                                  1.0 * np.sign(error))
                
                # Update weights
                l2_penalty = 0.1 * self.coef_ if self.penalty == 'l2' else 0
                self.coef_ -= 0.01 * (grad * X[i] + l2_penalty)
                
            # Early stopping
            if np.mean(np.abs(error)) < self.tol:
                break
                
        return self
    
    def predict(self, X):
        return np.dot(X, self.coef_)

### [FILE] ml/trainer.py

import time
import numpy as np
from binance import Client
from binance.exceptions import BinanceAPIException
from ml.model import QuantumModel
from abc import ABC, abstractmethod
import logging

logger = logging.getLogger('trainer')

class BaseTrainer(ABC):
    """✅ FIXED: Proper abstract base class with abstract methods"""
    @abstractmethod
    def record_trade(self, trade_data: dict): ...
    
    @abstractmethod 
    def get_testnet_symbols(self) -> list: ...
    
    @abstractmethod
    def get_order_book_safe(self, symbol: str): ...
    
    @abstractmethod
    def evaluate_strategy(self, symbol: str): ...
    
    @abstractmethod
    def run(self): ...

class Trainer(BaseTrainer):  # ✅ Now properly implements the interface
    def __init__(self):
        # ✅ Binance API Config (testnet)
        self.API_KEY = 'test_api_key'  # Will be overridden by environment
        self.API_SECRET = 'test_api_secret'  # Will be overridden by environment
        self.BASE_URL = 'https://testnet.binance.vision'
        self.trades = []
        self.model = QuantumModel()
        self.train_every_n_trades = 20  # retrain after this many trades

        # ✅ Initialize Binance client
        try:
            self.client = Client(self.API_KEY, self.API_SECRET)
            self.client.API_URL = self.BASE_URL
        except Exception as e:
            logger.warning(f"Binance client init warning: {e}")
            self.client = None

        # ✅ Trade history
        self.trades = []

        # ✅ Fetch supported symbols from testnet
        self.TESTNET_SUPPORTED_SYMBOLS = self.get_testnet_symbols()
        logger.info("[INFO] Starting bot with testnet-supported pairs:")
        logger.info(f"Supported symbols: {self.TESTNET_SUPPORTED_SYMBOLS}")

    def record_trade(self, trade, **kwargs):
        """✅ Implement abstract method"""
        trade.update(kwargs)
        self.trades.append(trade)

        trade_type = trade.get("type", "UNKNOWN")
        features = trade.get("features", {})
        outcome = trade.get("outcome", None)

        if features and outcome is not None:
            try:
                X = np.array([list(features.values())])
                y = np.array([int(outcome)])  # 1 or 0
                self.model.train(X, y)
            except Exception as e:
                logger.error(f"Model training error: {e}")

        # Retrain logic every N trades
        if len(self.trades) % self.train_every_n_trades == 0:
            self._retrain_model()

        if trade_type == "TRIANGULAR_ARBITRAGE":
            logger.info(f"[Trainer] ✅ Arbitrage trade recorded: {trade}")
        elif trade_type == "SCALPER":
            logger.info(f"[Trainer] 📈 Scalper trade recorded: {trade}")
        else:
            logger.info(f"[Trainer] 📄 Trade recorded: {trade}")

    def _retrain_model(self):
        """Retrain model on accumulated trade data"""
        X, y = [], []
        for t in self.trades:
            features = t.get("features")
            outcome = t.get("outcome")
            if features and outcome is not None:
                try:
                    X.append(list(features.values()))
                    y.append(int(outcome))
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid training sample: {e}")
                    continue

        if X and y:
            try:
                self.model.train(np.array(X), np.array(y))
                logger.info(f"[ML] ✅ Retrained model on {len(X)} samples.")
            except Exception as e:
                logger.error(f"[ML ERROR] Retraining failed: {e}")

    def get_testnet_symbols(self) -> list:
        """✅ Implement abstract method - Get supported testnet symbols"""
        try:
            if self.client is None:
                return ['BTCUSDC', 'ETHUSDC', 'BNBUSDC']  # Fallback
                
            exchange_info = self.client.get_exchange_info()
            symbols = [s['symbol'] for s in exchange_info['symbols'] if 'USDC' in s['symbol']]
            return symbols[:10]  # Limit to top 10
        except Exception as e:
            logger.error(f"[WARNING] Failed to fetch exchange info: {e}")
            return ['BTCUSDC', 'ETHUSDC', 'BNBUSDC']  # fallback list

    def get_order_book_safe(self, symbol):
        """✅ Implement abstract method - Safe order book retrieval"""
        if symbol not in self.TESTNET_SUPPORTED_SYMBOLS:
            logger.info(f"[SKIP] {symbol} not supported on Binance Testnet")
            return None

        try:
            if self.client is None:
                return None
            return self.client.get_order_book(symbol=symbol, limit=5)
        except BinanceAPIException as e:
            logger.error(f"[ERROR] Binance API error: {e}")
        except Exception as e:
            logger.error(f"[ERROR] Failed to get order book for {symbol}: {e}")
        return None

    def evaluate_strategy(self, symbol):
        """✅ Implement abstract method - Strategy evaluation"""
        depth = self.get_order_book_safe(symbol)
        if not depth or 'bids' not in depth or 'asks' not in depth:
            logger.info(f"[INFO] Skipping {symbol}, no valid order book data.")
            return

        try:
            bid = float(depth['bids'][0][0])
            ask = float(depth['asks'][0][0])
            spread = ask - bid
            logger.info(f"[SCALPER] {symbol} Spread: {spread:.4f}")

            if spread > 0.001:  # Minimum spread threshold
                logger.info(f"[SCALPER] {symbol} Spread: {spread:.4f} — signal triggered ✅")
                self.record_trade({
                    "type": "SCALPER",
                    "symbol": symbol,
                    "side": "SIMULATED_BUY",
                    "price": bid,
                    "qty": 1,
                    "timestamp": time.time()
                },
                features={"spread": spread, "volatility": spread/bid},
                outcome=True  # Simulated outcome
                )
        except Exception as e:
            logger.error(f"Strategy evaluation error for {symbol}: {e}")

    def run(self):
        """✅ Implement abstract method - Main training loop"""
        logger.info("Starting trainer main loop")
        while True:
            try:
                for symbol in self.TESTNET_SUPPORTED_SYMBOLS:
                    self.evaluate_strategy(symbol)
                    time.sleep(1)
                time.sleep(10)
            except KeyboardInterrupt:
                logger.info("Trainer stopped by user")
                break
            except Exception as e:
                logger.error(f"Trainer error: {e}")
                time.sleep(30)

# ✅ Optional quick test
if __name__ == "__main__":
    trainer = Trainer()
    symbol = "BTCUSDC"
    spread = 0.0012
    trainer.record_trade(
        {
            "type": "SCALPER",
            "symbol": symbol,
            "timestamp": time.time()
        },
        features={"spread": spread, "volatility": 0.02},
        outcome=True
    )


### [FILE] ml/rl_wrapper.py
import numpy as np
from ml.rl_light import LightRL
from gymnasium import Env

class TradingEnv(Env):
    def __init__(self, chaos_module):
        self.chaos = chaos_module
        self.action_space = spaces.Discrete(3)  # HOLD, BUY, SELL
        self.observation_space = spaces.Box(
            low=0, high=1, shape=(6,), dtype=np.float32)
    
    def step(self, action):
        # ... implement RL logic ...
        return obs, reward, done, info

class RLWrapper:
    def __init__(self, chaos_module):
        self.env = TradingEnv(chaos_module)
        self.model = PPO(
            "MlpPolicy", 
            self.env, 
            device="cpu",  # Critical for free-tier
            policy_kwargs={"net_arch": [64, 64]}  # Smaller network
        )
    
    def predict(self, market_state):
        return self.model.predict(market_state)

### [FILE] ml/model.py
import os
import numpy as np
from sklearn.linear_model import LogisticRegression
import logging
from datetime import datetime

logger = logging.getLogger('quantum_bot')

class QuantumModel:
    def __init__(self):
        self.model = LogisticRegression(max_iter=1000, random_state=42)
        self.is_trained = False
        self.log_dir = "data/logs"
        os.makedirs(self.log_dir, exist_ok=True)
        logger.info("QuantumModel initialized successfully")

    def train(self, X, y):
        """Train model with proper error handling"""
        try:
            if len(X) == 0 or len(y) == 0:
                logger.warning("No training data provided")
                return
                
            self.model.fit(X, y)
            self.is_trained = True
            logger.info(f"Model trained on {len(X)} samples")
        except Exception as e:
            logger.error(f"Training failed: {e}")
            self.is_trained = False

    def predict(self, X):
        """Predict with fallback to rule-based logic"""
        if not self.is_trained or len(X) == 0:
            return self._rule_based_predict(X)
        
        try:
            predictions = self.model.predict(X)
            self._log_predictions(X, predictions)
            return predictions
        except Exception as e:
            logger.error(f"Prediction failed, using fallback: {e}")
            return self._rule_based_predict(X)

    def _rule_based_predict(self, X):
        """Simple rule-based prediction fallback"""
        if len(X) == 0:
            return np.array([0])
        
        if len(X.shape) > 1 and X.shape[1] >= 3:
            recent_avg = np.mean(X[:, -3:], axis=1)
            earlier_avg = np.mean(X[:, :-3], axis=1) if X.shape[1] > 3 else recent_avg
            return (recent_avg > earlier_avg).astype(int)
        else:
            return np.zeros(len(X) if len(X.shape) > 1 else 1)

    def _log_predictions(self, X, predictions):
        """Log prediction results"""
        try:
            log_file = os.path.join(self.log_dir, "predictions.log")
            with open(log_file, "a") as f:
                timestamp = datetime.now().isoformat()
                for i, (features, pred) in enumerate(zip(X, predictions)):
                    f.write(f"{timestamp} | Sample {i}: {features} => {pred}\n")
        except Exception as e:
            logger.error(f"Failed to log predictions: {e}")


### [FILE] ml/chaos_module.py
import numpy as np
import time
import logging
from typing import List, Union, Optional, Dict
from decimal import Decimal, InvalidOperation, ROUND_HALF_UP
import gc
import asyncio
import math
from dataclasses import dataclass
from enum import Enum
from collections import deque
import re

logger = logging.getLogger('chaos_module')

class SecurityViolationException(Exception):
    """Exception for security violations in market data"""
    pass

class MalformedMarketDataException(Exception):
    """Exception for malformed market data"""
    pass

class MarketDataValidator:
    """🔒 SECURITY HARDENED: Enhanced market data validation with comprehensive protection"""
    
    # Security thresholds
    MAX_PRICE_VALUE = 1e12  # $1 trillion
    MAX_ARRAY_SIZE = 10000
    MAX_STRING_LENGTH = 100
    MIN_REASONABLE_PRICE = 1e-10  # Essentially zero
    
    # Price change sanity limits (50% change between consecutive prices)
    MAX_SANE_PRICE_CHANGE = 0.5
    
    @staticmethod
    def validate_price_data(prices: any) -> List[float]:
        """Comprehensive price data validation with enhanced security checks"""
        if prices is None:
            raise MalformedMarketDataException("Price data is None")
            
        # Check for suspiciously large datasets
        if hasattr(prices, '__len__') and len(prices) > MarketDataValidator.MAX_ARRAY_SIZE:
            raise SecurityViolationException(f"Price array too large: {len(prices)}")
            
        # Convert to list if single value
        if isinstance(prices, (int, float)):
            prices = [prices]
            
        if not isinstance(prices, (list, tuple, np.ndarray)):
            raise MalformedMarketDataException(f"Invalid price data type: {type(prices)}")
            
        if len(prices) == 0:
            raise MalformedMarketDataException("Empty price data")
            
        # Security: Check for reasonable array size
        if len(prices) > MarketDataValidator.MAX_ARRAY_SIZE:
            raise SecurityViolationException("Price array exceeds security limits")
            
        validated_prices = []
        previous_price = None
        
        for i, price in enumerate(prices):
            try:
                # Enhanced security validation
                price_float = MarketDataValidator._validate_and_sanitize_price(price, i)
                
                # ✅ ADDED: Consecutive price change validation
                if previous_price is not None:
                    price_change = abs(price_float - previous_price) / previous_price
                    if price_change > MarketDataValidator.MAX_SANE_PRICE_CHANGE:
                        raise SecurityViolationException(
                            f"Suspicious price change detected: {price_change:.2%} at index {i}"
                        )
                
                validated_prices.append(price_float)
                previous_price = price_float
                
            except (MalformedMarketDataException, SecurityViolationException) as e:
                logger.warning(f"Invalid price at index {i}: {e}")
                continue
                
        if len(validated_prices) < 5:
            raise MalformedMarketDataException("Insufficient valid price data")
            
        return validated_prices
    
    @staticmethod
    def _validate_and_sanitize_price(price: any, index: int) -> float:
        """Validate and sanitize a single price value with enhanced security checks"""
        if price is None:
            raise MalformedMarketDataException("Price is None")
            
        # Check for dangerous types
        if isinstance(price, (dict, list, tuple)):
            raise SecurityViolationException(f"Container type not allowed for price: {type(price)}")
            
        try:
            # String validation with enhanced security checks
            if isinstance(price, str):
                # Check for suspicious patterns in string
                if len(price) > MarketDataValidator.MAX_STRING_LENGTH:
                    raise SecurityViolationException("Price string too long")
                    
                # Enhanced malicious character detection
                suspicious_chars = re.findall(r'[^\d\.\-eE]', price)
                if suspicious_chars:
                    raise SecurityViolationException(f"Suspicious characters in price: {suspicious_chars}")
                    
                # Remove any potential malicious characters (defense in depth)
                cleaned_price = re.sub(r'[^\d\.\-eE]', '', price)
                if cleaned_price != price:
                    logger.warning(f"Price string sanitized at index {index}")
                    
                price = cleaned_price
                
            # Convert to float with security bounds
            price_float = float(price)
            
            # Security: Check for special float values
            if not math.isfinite(price_float):
                raise SecurityViolationException("Price is not finite")
                
            # Security: Check for reasonable price range
            if price_float <= 0:
                raise SecurityViolationException("Price must be positive")
                
            if price_float > MarketDataValidator.MAX_PRICE_VALUE:
                raise SecurityViolationException("Price unrealistically high")
                
            # Check for extreme values that might indicate data corruption
            if price_float < MarketDataValidator.MIN_REASONABLE_PRICE:
                raise SecurityViolationException("Price suspiciously low")
                
            return price_float
            
        except (ValueError, TypeError) as e:
            raise MalformedMarketDataException(f"Price conversion failed: {e}")
    
    @staticmethod
    def detect_anomalies(prices: List[float]) -> Dict[str, any]:
        """Detect statistical anomalies with enhanced security considerations"""
        if len(prices) < 10:
            return {'anomalies_detected': False, 'reasons': []}
            
        prices_array = np.array(prices)
        anomalies = []
        
        # Security: Check for NaN or infinity
        if not np.all(np.isfinite(prices_array)):
            anomalies.append("Non-finite values detected")
            
        # Calculate basic statistics with security bounds
        mean = np.mean(prices_array)
        std = np.std(prices_array)
        
        # Security: Check for statistical anomalies that might indicate attack
        if std == 0:  # All prices identical
            anomalies.append("Zero standard deviation (all prices identical)")
            
        # Detect outliers (beyond 6 sigma)
        z_scores = np.abs((prices_array - mean) / (std + 1e-8))
        extreme_outliers = np.sum(z_scores > 6)
        if extreme_outliers > 0:
            anomalies.append(f"Extreme outliers detected: {extreme_outliers}")
            
        # Detect sudden jumps (>50% change between consecutive prices)
        if len(prices) >= 2:
            changes = np.abs(np.diff(prices_array) / (prices_array[:-1] + 1e-8))
            large_jumps = np.sum(changes > MarketDataValidator.MAX_SANE_PRICE_CHANGE)
            if large_jumps > 0:
                anomalies.append(f"Large price jumps detected: {large_jumps}")
                
        # Check for monotonic sequences (potential manipulation)
        increasing = all(x <= y for x, y in zip(prices_array, prices_array[1:]))
        decreasing = all(x >= y for x, y in zip(prices_array, prices_array[1:]))
        if increasing or decreasing:
            anomalies.append("Suspicious monotonic sequence detected")
            
        return {
            'anomalies_detected': len(anomalies) > 0,
            'reasons': anomalies,
            'statistics': {
                'mean': mean,
                'std': std,
                'min': np.min(prices_array),
                'max': np.max(prices_array),
                'outlier_count': extreme_outliers
            }
        }

# Enhanced LightChaos class with security monitoring
class SecureLightChaos:
    """🔒 SECURITY HARDENED: Chaos module with comprehensive security monitoring"""
    
    def __init__(self, config: dict):
        self.config = config
        self._security_logger = logging.getLogger('security.chaos')
        self._security_stats = {
            'validation_failures': 0,
            'anomaly_detections': 0,
            'security_rejections': 0,
            'successful_analyses': 0
        }
        self._price_history = deque(maxlen=100)  # Keep recent prices for trend analysis
        
    def update(self, prices: Union[List[float], float]) -> str:
        """🔒 SECURE: Market analysis with enhanced security and anomaly detection"""
        try:
            # Enhanced security validation
            validated_prices = MarketDataValidator.validate_price_data(prices)
            
            # Enhanced anomaly detection
            anomaly_report = MarketDataValidator.detect_anomalies(validated_prices)
            
            if anomaly_report['anomalies_detected']:
                self._security_stats['anomaly_detections'] += 1
                self._security_logger.warning(
                    f"Statistical anomalies detected: {anomaly_report['reasons']}"
                )
                # Continue processing but with caution flag
                return self._analyze_with_caution(validated_prices, anomaly_report)
                
            # Normal analysis
            self._security_stats['successful_analyses'] += 1
            return self._analyze_market(validated_prices)
            
        except SecurityViolationException as e:
            self._security_stats['security_rejections'] += 1
            self._security_logger.critical(f"SECURITY REJECTION: {e}")
            return "SECURITY_VIOLATION"
        except MalformedMarketDataException as e:
            self._security_stats['validation_failures'] += 1
            logger.warning(f"Data validation failed: {e}")
            return "MALFORMED_DATA"
        except Exception as e:
            self._security_logger.error(f"Unexpected analysis error: {e}")
            return "ANALYSIS_ERROR"
    
    def _analyze_with_caution(self, prices: List[float], anomaly_report: Dict) -> str:
        """Analyze market data with enhanced caution due to detected anomalies"""
        # Use simpler, more robust analysis when anomalies are detected
        if len(prices) < 2:
            return "INSUFFICIENT_DATA"
            
        current_price = prices[-1]
        short_avg = np.mean(prices[-5:]) if len(prices) >= 5 else current_price
        long_avg = np.mean(prices[-20:]) if len(prices) >= 20 else short_avg
        
        # Conservative analysis
        if current_price > short_avg * 1.02 and current_price > long_avg * 1.05:
            return "CAUTIOUS_BULLISH"
        elif current_price < short_avg * 0.98 and current_price < long_avg * 0.95:
            return "CAUTIOUS_BEARISH"
        else:
            return "STABLE_CONSERVATIVE"
    
    def _analyze_market(self, prices: List[float]) -> str:
        """Normal market analysis"""
        if len(prices) < 2:
            return "INSUFFICIENT_DATA"
            
        current_price = prices[-1]
        short_avg = np.mean(prices[-5:]) if len(prices) >= 5 else current_price
        long_avg = np.mean(prices[-20:]) if len(prices) >= 20 else short_avg
        
        # Basic trend analysis
        if current_price > short_avg * 1.05 and current_price > long_avg * 1.1:
            return "STRONG_BULLISH"
        elif current_price > short_avg * 1.02 and current_price > long_avg * 1.05:
            return "BULLISH"
        elif current_price < short_avg * 0.95 and current_price < long_avg * 0.9:
            return "STRONG_BEARISH"
        elif current_price < short_avg * 0.98 and current_price < long_avg * 0.95:
            return "BEARISH"
        else:
            return "STABLE"
    
    def get_security_stats(self) -> Dict:
        """Get security monitoring statistics"""
        return self._security_stats.copy()
    
    def reset_security_stats(self):
        """Reset security statistics (for testing)"""
        self._security_stats = {
            'validation_failures': 0,
            'anomaly_detections': 0,
            'security_rejections': 0,
            'successful_analyses': 0
        }

# Backward compatibility
LightChaos = SecureLightChaos

### [FILE] docker/Dockerfile
FROM amazonlinux:2023

WORKDIR /app

# Install system dependencies
RUN dnf update -y && \
    dnf install -y python3.11 python3.11-pip python3.11-devel gcc make git curl && \
    dnf clean all

# Set Python 3.11 as default
RUN alternatives --set python /usr/bin/python3.11

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip3.11 install --no-cache-dir --upgrade pip && \
    pip3.11 install --no-cache-dir -r requirements.txt

# Create necessary directories
RUN mkdir -p /app/config /app/data /app/logs

# Copy application code
COPY . .

# Set timezone to Singapore time
RUN ln -sf /usr/share/zoneinfo/Asia/Singapore /etc/localtime

# Set environment variables
ENV AWS_REGION=ap-southeast-1 \
    PYTHONUNBUFFERED=1 \
    PYTHONOPTIMIZE=1 \
    MODE=paper \
    FREE_TIER=true

# Set proper permissions
RUN chmod +x run.sh && \
    chmod +x monitor.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8080/health', timeout=2)" || exit 1

CMD ["./run.sh"]


### [FILE] docker/docker-compose.yml

version: '3.8'

services:
  quantum-bot:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    container_name: quantum_trading_bot
    restart: unless-stopped
    mem_limit: 650m  # ✅ FIXED: Strict memory limit for free tier
    cpus: 0.5        # ✅ FIXED: CPU limit for free tier
    volumes:
      - ../config:/app/config:ro  # ✅ FIXED: Read-only config
      - ../data:/app/data
      - ../logs:/app/logs
    environment:
      - MODE=paper
      - FREE_TIER=true
      - AWS_REGION=ap-southeast-1
      - TZ=Asia/Singapore
      - PYTHONUNBUFFERED=1
    working_dir: /app
    command: python -OO bootstrap.py
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8080/health', timeout=2) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ✅ ADDED: Resource monitor for free tier
  resource-monitor:
    image: alpine:latest
    container_name: resource_monitor
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: |
      sh -c "
      while true; do
        echo '=== RESOURCE USAGE ==='
        docker stats quantum_trading_bot --no-stream --format 'table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}'
        sleep 60
      done"

### [FILE] core/type-checker.py
# Lightweight interface validation
class TypeChecker:
    @staticmethod
    def enforce_chaos_module(obj):
        required = ['health_score', 'update', 'detect_regime']
        if not all(hasattr(obj, attr) for attr in required):
            raise ValueError("Invalid chaos module - missing required methods")

    @staticmethod
    def enforce_trainer(obj):
        required = ['record_trade', 'get_testnet_symbols']
        if not all(hasattr(obj, attr) for attr in required):
            raise ValueError("Invalid trainer - missing required methods")

### [FILE] core/strategy_engine.py
import asyncio
from typing import List, Optional
from binance_api.client import BinanceClient
from utils.queue import Queue as ThreadSafeQueue

# ✅ FIXED: Correct import paths
try:
    from core.turbo_arbitrage import TurboArbitrage
    from core.quantum_overlays import QuantumOverlay
except ImportError:
    # Fallback for module structure
    from .turbo_arbitrage import TurboArbitrage
    from .quantum_overlays import QuantumOverlay

class StrategyEngine:
    def __init__(self, client: BinanceClient, symbols: Optional[List[str]] = None):
        self.client = client
        self.symbols = symbols or self._get_default_symbols()
        
        # Using your custom thread-safe queue implementation
        self.trade_queue = ThreadSafeQueue()
        self.active = False
        
        # ✅ FIXED: Initialize strategies with error handling
        try:
            self.arbitrage = TurboArbitrage(client, symbols=self.symbols)
            self.quantum = QuantumOverlay(client, symbols=self.symbols)
        except Exception as e:
            print(f"Strategy initialization warning: {e}")
            # Create placeholder strategies that won't crash
            self.arbitrage = self._create_dummy_strategy()
            self.quantum = self._create_dummy_strategy()

    def _create_dummy_strategy(self):
        """Create a dummy strategy that won't crash on method calls"""
        class DummyStrategy:
            async def execute(self, trade): 
                print(f"Dummy strategy executed: {trade}")
            async def shutdown(self): 
                pass
        return DummyStrategy()

    def start(self):
        """Start the strategy engine and trade processor"""
        self.active = True
        asyncio.create_task(self._process_trades())

    async def _process_trades(self):
        """Process trades from the queue with shutdown handling"""
        while self.active or not self.trade_queue.empty():
            try:
                # Use asyncio.to_thread for blocking queue operations
                trade = await asyncio.get_event_loop().run_in_executor(
                    None,
                    self.trade_queue.get
                )
                
                if trade is None:  # Shutdown signal
                    break
                    
                await self._execute_trade(trade)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"Trade processing error: {e}")
                await asyncio.sleep(1)  # Prevent tight loop on errors

    async def shutdown(self):
        """Graceful shutdown using custom queue's termination"""
        self.active = False
        
        if hasattr(self.trade_queue, 'shutdown'):
            self.trade_queue.shutdown()  # Custom method that unblocks all threads
        
        # Clean up strategies with error handling
        try:
            await self.arbitrage.shutdown()
        except Exception as e:
            print(f"Arbitrage shutdown error: {e}")
            
        try:
            await self.quantum.shutdown()
        except Exception as e:
            print(f"Quantum overlay shutdown error: {e}")

    def _get_default_symbols(self) -> List[str]:
        """Get default symbols to monitor for live trading"""
        return ['BTCUSDC', 'ETHUSDC', 'BNBUSDC']  # ✅ USDC pairs for live trading

    async def _execute_trade(self, trade: dict):
        """Execute trade through appropriate strategy with error handling"""
        try:
            trade_type = trade.get('type', 'quantum')
            
            if trade_type == 'arbitrage':
                await self.arbitrage.execute(trade)
            else:
                await self.quantum.execute(trade)
                
        except Exception as e:
            print(f"Trade execution failed: {e}")
            # Implement fallback logic or alert system here

    async def analyze_market(self, symbol: str) -> dict:
        """Analyze market conditions for a symbol"""
        try:
            # Get current market data
            klines = await self.client.get_klines(symbol, interval="1m", limit=50)
            order_book = await self.client.get_order_book(symbol, limit=10)
            
            if not klines or not order_book:
                return {"error": "Failed to get market data"}
            
            # Basic market analysis
            prices = [float(k[4]) for k in klines]  # Closing prices
            current_price = float(order_book['bids'][0][0])
            spread = float(order_book['asks'][0][0]) - float(order_book['bids'][0][0])
            
            return {
                "symbol": symbol,
                "current_price": current_price,
                "spread_percent": (spread / current_price) * 100,
                "volatility": self._calculate_volatility(prices),
                "liquidity": sum(float(b[1]) for b in order_book['bids'][:5]),
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {"error": f"Analysis failed: {str(e)}"}

    def _calculate_volatility(self, prices: List[float]) -> float:
        """Calculate price volatility"""
        if len(prices) < 2:
            return 0.0
            
        returns = [(prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices))]
        return np.std(returns) if returns else 0.0

### [FILE] core/self_evolution.py
class SelfEvolution:
    def __init__(self, strategy_engine, chaos_module):
        self.engine = strategy_engine
        self.chaos = chaos_module
        self.health_history = []
        
    def adapt_strategy(self):
        """Dynamically adjust parameters based on market health"""
        health_status = self.chaos.health_score
        
        # Store health metrics
        self.health_history.append({
            'timestamp': time.time(),
            'score': health_status,
            'parameters': self._current_params()
        })
        
        # Adaptive logic
        if health_status < 0.3:  # Critical
            self.engine.arbitrage.min_profit *= 1.5  # Higher profit threshold
            self.engine.arbitrage.pause(300)  # 5min cool-off
        elif health_status < 0.6:  # Unstable
            self.engine.arbitrage.min_profit *= 1.2
            self.engine.arbitrage.trade_amount *= 0.5  # Reduce position size
            
    def _current_params(self):
        return {
            'min_profit': self.engine.arbitrage.min_profit,
            'trade_amount': self.engine.arbitrage.trade_amount,
            'max_slippage': self.engine.arbitrage.max_slippage
        }

### [FILE] core/quantum_overlay.py

import numpy as np
from ml.model import QuantumModel

class QuantumOverlay:
    def __init__(self, client, symbols, model: QuantumModel):
        self.client = client
        self.symbols = symbols
        self.model = model
        self.prediction_window = 20
        self.signal_threshold = 0.7

    def simulate_quantum_prediction(self, prices, chaos_label=None):
        if len(prices) < 3:
            return 0.5  # Neutral

        features = np.array(prices[-3:]).reshape(1, -1)

        if chaos_label is not None:
            chaos_numeric = 1.0 if chaos_label == "chaotic" else 0.0
            features = np.append(features, [[chaos_numeric]], axis=1)

        prediction = self.model.predict(features)[0]
        return prediction


### [FILE] core/portfolio_manager.py

import logging
import time
from typing import Dict, List, Optional, Any, Tuple
from decimal import Decimal, ROUND_DOWN
import numpy as np
from utils.advanced_logger import AdvancedLogger
from binance_api.client import BinanceAPIClient

class PortfolioManager:
    """
    Enhanced portfolio manager for USDC-based trading
    Comprehensive risk management and position tracking
    """
    
    def __init__(self, api_client: BinanceAPIClient, config: Dict[str, Any]):
        self.api_client = api_client
        self.config = config
        self.logger = AdvancedLogger(__name__)
        
        # === CRITICAL: Use configured bridge currency ===
        trading_config = config.get('trading', {})
        self.bridge_currency = trading_config.get('bridge_currency', 'USDC')
        self.max_position_size = trading_config.get('max_position_size', 100.0)
        self.min_trade_amount = trading_config.get('min_trade_amount', 10.0)
        self.max_drawdown = trading_config.get('max_drawdown', 0.05)
        
        # Risk management
        risk_config = config.get('risk', {})
        self.max_daily_loss = risk_config.get('max_daily_loss', 0.1)
        self.max_position_risk = risk_config.get('max_position_risk', 0.02)
        
        # Portfolio state
        self.positions: Dict[str, Dict] = {}
        self.balances: Dict[str, Dict] = {}
        self.cash_balance = 0.0
        self.initial_balance = 0.0
        self.current_balance = 0.0
        self.daily_pnl = 0.0
        self.total_pnl = 0.0
        self.last_update = 0
        self.update_interval = 30  # seconds
        
        # Performance tracking
        self.performance_history = []
        self.trade_history = []
        
        self.logger.info("PortfolioManager initialized with USDC base currency")

    def update_balances(self, force: bool = False) -> bool:
        """
        Update portfolio balances from Binance
        Returns True if successful
        """
        current_time = time.time()
        if not force and current_time - self.last_update < self.update_interval:
            return True
            
        try:
            # Get account info from Binance
            account_info = self.api_client.get_account_info()
            self.last_update = current_time
            
            # Clear previous balances
            self.balances = {}
            self.cash_balance = 0.0
            
            # Process balances
            for asset_info in account_info['balances']:
                asset = asset_info['asset']
                free = float(asset_info['free'])
                locked = float(asset_info['locked'])
                total = free + locked
                
                self.balances[asset] = {
                    'free': free,
                    'locked': locked,
                    'total': total
                }
                
                # Track USDC cash balance separately
                if asset == self.bridge_currency:
                    self.cash_balance = free
                    
            # Update positions
            self._update_positions()
            
            # Update portfolio value
            self._update_portfolio_value()
            
            self.logger.debug(f"Balances updated - {self.bridge_currency}: {self.cash_balance:.2f}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to update balances: {e}")
            return False

    def _update_positions(self):
        """Update positions from balances"""
        self.positions = {}
        
        for asset, balance_info in self.balances.items():
            if asset != self.bridge_currency and balance_info['total'] > 0:
                symbol = f"{asset}{self.bridge_currency}"
                self.positions[symbol] = {
                    'asset': asset,
                    'quantity': balance_info['total'],
                    'symbol': symbol,
                    'current_value': 0.0,
                    'unrealized_pnl': 0.0
                }

    def _update_portfolio_value(self):
        """Update total portfolio value"""
        try:
            total_value = self.cash_balance
            
            # Add value of all positions
            for symbol, position in self.positions.items():
                try:
                    price_data = self.api_client.get_symbol_price(symbol)
                    current_price = float(price_data['price'])
                    position_value = position['quantity'] * current_price
                    position['current_value'] = position_value
                    position['current_price'] = current_price
                    total_value += position_value
                    
                except Exception as e:
                    self.logger.warning(f"Could not get price for {symbol}: {e}")
                    
            self.current_balance = total_value
            
            # Initialize initial balance if not set
            if self.initial_balance == 0:
                self.initial_balance = total_value
                
            # Update PnL
            self.total_pnl = total_value - self.initial_balance
            
            # Record performance
            self.performance_history.append({
                'timestamp': time.time(),
                'portfolio_value': total_value,
                'cash_balance': self.cash_balance,
                'positions_value': total_value - self.cash_balance,
                'total_pnl': self.total_pnl
            })
            
            # Keep only last 1000 records
            if len(self.performance_history) > 1000:
                self.performance_history.pop(0)
                
        except Exception as e:
            self.logger.error(f"Error updating portfolio value: {e}")

    def get_total_value(self) -> float:
        """Get total portfolio value in USDC"""
        if time.time() - self.last_update > self.update_interval:
            self.update_balances(force=True)
        return self.current_balance

    def get_cash_balance(self) -> float:
        """Get available USDC balance"""
        return self.cash_balance

    def get_position_size(self, symbol: str) -> float:
        """Get current position size for symbol"""
        position = self.positions.get(symbol)
        return position['current_value'] if position else 0.0

    def calculate_position_size(self, symbol: str, risk_per_trade: float = None) -> float:
        """
        Calculate appropriate position size based on risk management
        """
        if risk_per_trade is None:
            risk_per_trade = self.max_position_risk
            
        # Update balances if stale
        if time.time() - self.last_update > self.update_interval:
            self.update_balances()
            
        # Calculate based on portfolio value and risk
        portfolio_value = self.get_total_value()
        risk_amount = portfolio_value * risk_per_trade
        
        # Apply position size limits
        position_size = min(risk_amount, self.max_position_size)
        position_size = max(position_size, self.min_trade_amount)
        
        # Ensure we have enough cash
        available_cash = self.get_cash_balance()
        position_size = min(position_size, available_cash * 0.95)  # Leave 5% buffer
        
        self.logger.debug(f"Calculated position size for {symbol}: {position_size:.2f} {self.bridge_currency}")
        return position_size

    def can_open_position(self, symbol: str, amount: float) -> Tuple[bool, str]:
        """
        Check if a new position can be opened
        Returns (can_open, reason)
        """
        # Check minimum trade amount
        if amount < self.min_trade_amount:
            return False, f"Amount below minimum {self.min_trade_amount}"
            
        # Check maximum position size
        if amount > self.max_position_size:
            return False, f"Amount exceeds maximum position size {self.max_position_size}"
            
        # Check available cash
        if amount > self.cash_balance * 0.95:  # 5% buffer
            return False, "Insufficient cash balance"
            
        # Check daily loss limit
        if self.daily_pnl < -self.initial_balance * self.max_daily_loss:
            return False, "Daily loss limit exceeded"
            
        # Check if symbol already has position
        current_position = self.get_position_size(symbol)
        if current_position > 0:
            return False, f"Position already exists for {symbol}"
            
        return True, "OK"

    def record_trade(self, symbol: str, side: str, quantity: float, 
                    price: float, fee: float = 0.0):
        """
        Record a trade in portfolio history
        """
        trade_value = quantity * price
        trade_record = {
            'timestamp': time.time(),
            'symbol': symbol,
            'side': side,
            'quantity': quantity,
            'price': price,
            'value': trade_value,
            'fee': fee,
            'portfolio_value': self.get_total_value()
        }
        
        self.trade_history.append(trade_record)
        
        # Keep only last 500 trades
        if len(self.trade_history) > 500:
            self.trade_history.pop(0)
            
        self.logger.info(f"Recorded trade: {side} {quantity} {symbol} at {price}")

    def get_portfolio_allocation(self) -> Dict[str, float]:
        """Get current portfolio allocation by asset"""
        total_value = self.get_total_value()
        if total_value == 0:
            return {}
            
        allocation = {}
        
        # Cash allocation
        allocation[self.bridge_currency] = self.cash_balance / total_value
        
        # Position allocations
        for symbol, position in self.positions.items():
            allocation[symbol] = position['current_value'] / total_value
            
        return allocation

    def get_risk_metrics(self) -> Dict[str, Any]:
        """Calculate portfolio risk metrics"""
        total_value = self.get_total_value()
        
        # Concentration risk (Herfindahl index)
        allocation = self.get_portfolio_allocation()
        concentration = sum(weight ** 2 for weight in allocation.values())
        
        # Drawdown calculation
        if self.performance_history:
            peak_value = max(entry['portfolio_value'] for entry in self.performance_history)
            current_drawdown = (peak_value - total_value) / peak_value if peak_value > 0 else 0
        else:
            current_drawdown = 0
            
        # VAR-like metric (simplified)
        recent_returns = []
        for i in range(1, min(21, len(self.performance_history))):
            current = self.performance_history[-i]['portfolio_value']
            previous = self.performance_history[-i-1]['portfolio_value'] if i < len(self.performance_history) - 1 else current
            if previous > 0:
                ret = (current - previous) / previous
                recent_returns.append(ret)
                
        volatility = np.std(recent_returns) if recent_returns else 0
        var_95 = np.percentile(recent_returns, 5) if recent_returns else 0
        
        metrics = {
            'total_value': total_value,
            'cash_balance': self.cash_balance,
            'total_pnl': self.total_pnl,
            'daily_pnl': self.daily_pnl,
            'current_drawdown': current_drawdown,
            'concentration_index': concentration,
            'volatility': volatility,
            'value_at_risk_95': var_95,
            'position_count': len(self.positions),
            'cash_ratio': self.cash_balance / total_value if total_value > 0 else 1.0
        }
        
        return metrics

    def rebalance_portfolio(self, target_allocation: Dict[str, float]) -> List[Dict]:
        """
        Rebalance portfolio towards target allocation
        Returns list of trades needed
        """
        current_value = self.get_total_value()
        current_allocation = self.get_portfolio_allocation()
        proposed_trades = []
        
        for asset, target_weight in target_allocation.items():
            current_weight = current_allocation.get(asset, 0)
            weight_diff = target_weight - current_weight
            
            if abs(weight_diff) < 0.01:  # 1% threshold
                continue
                
            trade_value = weight_diff * current_value
            
            if asset == self.bridge_currency:
                # Adjust cash position (not typically traded)
                continue
                
            # Determine trade side and quantity
            if trade_value > 0:
                # Buy
                side = 'BUY'
                # Get current price for quantity calculation
                try:
                    price_data = self.api_client.get_symbol_price(asset)
                    price = float(price_data['price'])
                    quantity = trade_value / price
                except:
                    continue
            else:
                # Sell
                side = 'SELL'
                # Use current position for quantity
                current_position = self.positions.get(asset, {}).get('quantity', 0)
                quantity = min(abs(trade_value) / current_value * current_position, current_position)
                
            if quantity > 0:
                proposed_trades.append({
                    'symbol': asset,
                    'side': side,
                    'quantity': quantity,
                    'value': abs(trade_value),
                    'reason': f"Rebalance to {target_weight*100:.1f}%"
                })
                
        self.logger.info(f"Rebalancing proposed {len(proposed_trades)} trades")
        return proposed_trades

    def get_performance_report(self, period: str = 'daily') -> Dict[str, Any]:
        """Generate performance report for specified period"""
        if not self.performance_history:
            return {}
            
        # Filter data based on period
        now = time.time()
        if period == 'daily':
            cutoff = now - 24 * 3600
        elif period == 'weekly':
            cutoff = now - 7 * 24 * 3600
        elif period == 'monthly':
            cutoff = now - 30 * 24 * 3600
        else:
            cutoff = 0
            
        period_data = [entry for entry in self.performance_history if entry['timestamp'] >= cutoff]
        
        if not period_data:
            return {}
            
        start_value = period_data[0]['portfolio_value']
        end_value = period_data[-1]['portfolio_value']
        returns = (end_value - start_value) / start_value if start_value > 0 else 0
        
        # Calculate volatility
        values = [entry['portfolio_value'] for entry in period_data]
        returns_series = []
        for i in range(1, len(values)):
            if values[i-1] > 0:
                ret = (values[i] - values[i-1]) / values[i-1]
                returns_series.append(ret)
                
        volatility = np.std(returns_series) if returns_series else 0
        
        # Sharpe ratio (simplified, assuming 0 risk-free rate)
        sharpe = returns / volatility if volatility > 0 else 0
        
        # Max drawdown
        peak = values[0]
        max_drawdown = 0
        for value in values:
            if value > peak:
                peak = value
            drawdown = (peak - value) / peak
            if drawdown > max_drawdown:
                max_drawdown = drawdown
                
        report = {
            'period': period,
            'start_value': start_value,
            'end_value': end_value,
            'return': returns,
            'volatility': volatility,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_drawdown,
            'trade_count': len([t for t in self.trade_history if t['timestamp'] >= cutoff]),
            'win_rate': self._calculate_win_rate(cutoff)
        }
        
        return report

    def _calculate_win_rate(self, since_timestamp: float) -> float:
        """Calculate win rate of trades since timestamp"""
        recent_trades = [t for t in self.trade_history if t['timestamp'] >= since_timestamp]
        if not recent_trades:
            return 0.0
            
        # Simplified win rate calculation
        # In a real implementation, you'd track trade outcomes
        winning_trades = 0
        for trade in recent_trades:
            # This is a placeholder - actual implementation would require tracking trade outcomes
            if trade['side'] == 'BUY':
                # Check if current price is above entry
                pass
            elif trade['side'] == 'SELL':
                # Check if current price is below entry
                pass
                
        return winning_trades / len(recent_trades) if recent_trades else 0.0

    def validate_risk_limits(self) -> Tuple[bool, List[str]]:
        """
        Validate portfolio against all risk limits
        Returns (is_valid, violations)
        """
        violations = []
        metrics = self.get_risk_metrics()
        
        # Check drawdown limit
        if metrics['current_drawdown'] > self.max_drawdown:
            violations.append(f"Drawdown {metrics['current_drawdown']:.1%} exceeds limit {self.max_drawdown:.1%}")
            
        # Check daily loss limit
        if self.daily_pnl < -self.initial_balance * self.max_daily_loss:
            violations.append(f"Daily loss {self.daily_pnl:.2f} exceeds limit")
            
        # Check concentration risk
        if metrics['concentration_index'] > 0.3:  # Arbitrary threshold
            violations.append("Portfolio too concentrated")
            
        # Check cash ratio
        if metrics['cash_ratio'] < 0.05:  # At least 5% cash
            violations.append("Insufficient cash balance")
            
        is_valid = len(violations) == 0
        return is_valid, violations

### [FILE] core/turbo_arbitrage.py
import numpy as np
import asyncio
import psutil
from numba import njit
from typing import List, Tuple, Dict
from dataclasses import dataclass
from collections import deque
import time
import gc
import logging
from decimal import Decimal, ROUND_HALF_UP  # ✅ ADDED: Decimal precision

logger = logging.getLogger('turbo_arbitrage')

@dataclass
class TradeMetrics:
    __slots__ = ['execution_time', 'profit', 'path', 'position_size']
    execution_time: float
    profit: float
    path: Tuple[str, str, str]
    position_size: float

@njit(fastmath=True, cache=True)
def calculate_profits(prices: np.ndarray, fee: float) -> np.ndarray:
    """Optimized profit calculation with Numba - FIXED PRECISION"""
    n = prices.shape[0]
    profits = np.zeros((n, n, n))
    fee_factor = (1 - fee) ** 3
    
    for i in range(n):
        for j in range(n):
            for k in range(n):
                if i != j and j != k and i != k:
                    # ✅ FIXED: Decimal-like precision using scaled integers
                    # Scale prices to avoid floating point errors
                    scale_factor = 1e8  # 8 decimal places precision
                    
                    price1_scaled = int(prices[i, 1] * scale_factor)
                    price2_scaled = int(prices[j, 0] * scale_factor) 
                    price3_scaled = int(prices[k, 1] * scale_factor)
                    price4_scaled = int(prices[i, 0] * scale_factor)
                    
                    if price1_scaled > 0 and price3_scaled > 0:
                        # Calculate with integer math for precision
                        result_scaled = (price2_scaled * price4_scaled * scale_factor) // (price1_scaled * price3_scaled)
                        result = (result_scaled / scale_factor) * fee_factor - 1
                        
                        # Filter out unrealistic profits (>100% or < -50%)
                        if -0.5 <= result <= 1.0:
                            profits[i,j,k] = result
    return profits

class TurboArbitrage:
    __slots__ = [
        'client', 'config', 'trade_history', 'price_matrix', 
        'fee', 'base_threshold', 'current_threshold', 'start_time',
        'orderbook_limiter', '_cleanup_interval', '_last_cleanup',
        '_last_mem_check', '_logger', '_loop', '_lock', '_precision_cache'
    ]
    
    def __init__(self, client, config):
        self.client = client
        self.config = config
        self.trade_history = deque(maxlen=100)
        self.price_matrix = np.zeros((8, 2), dtype=np.float64)  # ✅ FIXED: float64 for precision
        self.fee = Decimal(str(config['trading']['fee'])).quantize(Decimal('0.00001'), ROUND_HALF_UP)
        self.base_threshold = Decimal(str(config['risk_management']['min_profit_threshold'])).quantize(Decimal('0.00001'), ROUND_HALF_UP)
        self.current_threshold = float(self.base_threshold)
        self.start_time = time.monotonic()
        self.orderbook_limiter = asyncio.Semaphore(2)
        self._cleanup_interval = 300
        self._last_cleanup = time.time()
        self._last_mem_check = time.time()
        self._logger = logging.getLogger('arbitrage')
        self._logger.setLevel(logging.INFO)
        self._precision_cache = {}  # ✅ ADDED: Cache for precision calculations
        
    async def find_opportunities(self, symbols: List[str]) -> List[Tuple[Tuple[str, str, str], float]]:
        """Find profitable arbitrage opportunities with precision fixes"""
        # Free-tier resource management
        try:
            if psutil.virtual_memory().percent > 85:
                self._reduce_active_pairs(6)
                
            if time.time() - self._last_mem_check > 60:
                if psutil.virtual_memory().percent > 70:
                    self._cleanup_memory()
                self._last_mem_check = time.time()

            if time.time() - self._last_cleanup > self._cleanup_interval:
                self._cleanup_memory()
                self._last_cleanup = time.time()

            # Only process top pairs if memory constrained
            if psutil.virtual_memory().percent > 75:
                symbols = symbols[:6]
                
            books = await self._get_order_books(symbols[:6])
            if not books:
                return []

            # ✅ FIXED: Proper price matrix construction with precision
            valid_symbols = list(books.keys())
            n = len(valid_symbols)
            self.price_matrix = np.zeros((n, 2), dtype=np.float64)
            
            spread_ratios = []
            for i, symbol in enumerate(valid_symbols):
                book = books[symbol]
                if not book or 'bids' not in book or 'asks' not in book:
                    continue
                    
                bid = self._safe_float_conversion(book['bids'][0][0]) if book['bids'] else 0
                ask = self._safe_float_conversion(book['asks'][0][0]) if book['asks'] else 0
                
                if bid > 0 and ask > 0 and ask >= bid:  # ✅ FIXED: Validation
                    self.price_matrix[i, 0] = bid
                    self.price_matrix[i, 1] = ask
                    spread_ratios.append(ask/bid)
                else:
                    continue
            
            if len(spread_ratios) < 3:
                return []  # Need at least 3 valid symbols
            
            # Dynamic threshold adjustment with precision
            volatility = np.std(spread_ratios) if len(spread_ratios) > 1 else 0
            self.current_threshold = max(
                float(self.base_threshold),
                float(self.base_threshold) * (1 + volatility * 1.5)
            )
            
            # Calculate profits only on valid data
            valid_matrix = self.price_matrix[:len(valid_symbols)]
            profits = calculate_profits(valid_matrix, float(self.fee))
            
            # Get top opportunities with realistic profit thresholds
            profitable_indices = np.argwhere(profits > self.current_threshold)
            opportunities = []
            
            for idx in profitable_indices[:3]:  # Limit to top 3
                i, j, k = idx
                if i < len(valid_symbols) and j < len(valid_symbols) and k < len(valid_symbols):
                    path = (valid_symbols[i], valid_symbols[j], valid_symbols[k])
                    profit = float(Decimal(str(profits[i, j, k])).quantize(Decimal('0.0001'), ROUND_HALF_UP))
                    
                    # ✅ FIXED: Realistic profit validation
                    if 0.001 <= profit <= 0.5:  # 0.1% to 50% realistic range
                        opportunities.append((path, profit))
            
            self._log_metrics(opportunities)
            return sorted(opportunities, key=lambda x: -x[1])
            
        except Exception as e:
            self._logger.error(f"Opportunity finding failed: {str(e)}")
            return []
            
    def _safe_float_conversion(self, value):
        """Safe float conversion with error handling"""
        try:
            if isinstance(value, (int, float)):
                return float(value)
            elif isinstance(value, str):
                # Remove any non-numeric characters except decimal point and minus
                cleaned = ''.join(c for c in value if c.isdigit() or c in '.-')
                return float(cleaned) if cleaned else 0.0
            else:
                return 0.0
        except (ValueError, TypeError):
            return 0.0

    def _reduce_active_pairs(self, min_pairs: int):
        """Reduce active pairs for resource management"""
        self._logger.info(f"Reducing active pairs to {min_pairs}")

    def clear_caches(self):
        """Clear internal caches"""
        self._logger.info("Clearing arbitrage caches")
        self.price_matrix.fill(0)
        self._precision_cache.clear()
        gc.collect()

### [FILE] config/presets/high_volatility.yaml

preset: "high_volatility"
description: "Optimized for high volatility market conditions with USDC pairs"

trading:
  symbols: ["BTCUSDC", "ETHUSDC", "BNBUSDC", "SOLUSDC", "XRPUSDC", "ADAUSDC", "DOGEUSDC", "DOTUSDC"]
  quote_currency: "USDC"
  position_size_percent: 0.15  # Reduced from 0.20 for higher volatility
  max_position_size: 15000     # Reduced from 20000
  min_order_size: 15           # Increased from 10

risk_management:
  max_risk_per_trade: 0.10     # Reduced from 0.15
  min_profit_threshold: 0.005  # Increased from 0.003 for better risk-reward
  daily_loss_limit: -30        # Tighter from -40
  volatility_multiplier: 1.5
  max_drawdown: 0.25

indicators:
  atr_period: 14
  rsi_period: 14
  bb_period: 20
  bb_std: 2.5                  # Wider bands for volatility
  ema_fast: 9
  ema_slow: 21
  volume_ma_period: 20

entry_conditions:
  volatility_threshold: 0.025   # Minimum ATR percentage
  volume_multiplier: 1.3        # Minimum volume boost
  trend_alignment: true
  momentum_confirmation: true

exit_conditions:
  trailing_stop_atr: 2.0        # Wider stops for volatility
  profit_target_atr: 3.5
  time_exit_hours: 48           # Shorter time exits

arbitrage:
  min_profit_threshold: 0.006   # Increased from 0.004 for volatility
  max_slippage: 0.002           # Increased slippage tolerance
  simultaneous_trades: 2         # Reduced from 3

performance:
  polling_interval: 1.0         # Faster polling for volatile markets
  cache_size: 150               # Larger cache
  data_retention_hours: 72      # Longer retention for analysis

# PRESERVED: All existing high volatility strategies and ML configurations
ml_volatility:
  enabled: true
  prediction_confidence: 0.75
  adaptive_learning: true
  market_regime_detection: true

# NEW: Enhanced volatility monitoring
volatility_monitoring:
  realtime_atr_tracking: true
  volume_spike_detection: true
  correlation_breakdown_alert: true
  regime_change_sensitivity: "high"

### [FILE] config/config_secrets.yaml

# Binance API Configuration - ENDPOINTS ONLY (no keys)
BINANCE_BASE_ENDPOINT: "https://api.binance.com"
BINANCE_TESTNET: false
BINANCE_TIMEOUT: 30
BINANCE_RECV_WINDOW: 60000

# Trading Configuration for USDC Pairs
TRADING_ENABLED: true
TRADING_MODE: "live"
MAX_LEVERAGE: 1  # Changed to 1 for spot trading
RISK_PER_TRADE: 0.02  # 2% risk per trade
MAX_POSITION_SIZE: 0.2  # 20% of portfolio per position (as you requested)

# USDC Trading Pairs
TRADING_PAIRS: 
  - "BTCUSDC"
  - "ETHUSDC" 
  - "BNBUSDC"
  - "SOLUSDC"
  - "XRPUSDC"
  - "ADAUSDC"
  - "DOGEUSDC"
  - "DOTUSDC"

# AWS Configuration - REGION ONLY (no credentials)
AWS_REGION: "ap-southeast-1"

# AWS Free Tier Optimization
AWS_FREE_TIER_ENABLED: true
AWS_DAILY_RUN_HOURS: 6
AWS_STOP_INSTANCE_WHEN_IDLE: true
AWS_COST_OPTIMIZATION: true

# Database Configuration - URI ONLY (no credentials)
MONGODB_URI: "mongodb://localhost:27017/binance_bot"
MONGODB_DB_NAME: "binance_bot"

# Email Configuration - SERVER SETTINGS ONLY (no passwords)
EMAIL_SMTP_SERVER: "smtp.gmail.com"
EMAIL_SMTP_PORT: 587
EMAIL_USE_TLS: true
EMAIL_USERNAME: "paulbigbrothers@gmail.com"
EMAIL_FROM: "paulbigbrothers@gmail.com"
EMAIL_TO: "paulbigbrothers@gmail.com"

# Monitoring Configuration
HEARTBEAT_INTERVAL: 60
HEALTH_CHECK_TIMEOUT: 300
PERFORMANCE_MONITORING: true
SECURITY_MONITORING: true

# Rate Limiting Configuration for Binance API
RATE_LIMIT_TIER: "free_tier"
MAX_REQUESTS_PER_MINUTE: 600
MAX_ORDERS_PER_MINUTE: 120
MAX_WEIGHT_PER_MINUTE: 2400

# Strategy Configuration
STRATEGY_ENABLED: true
STRATEGY_NAME: "quantum_enhanced"
STRATEGY_VERSION: "1.0.0"

# Machine Learning Configuration
ML_ENABLED: true
ML_MODEL_PATH: "./ml/models/"
ML_TRAINING_ENABLED: false

# Emergency Settings
EMERGENCY_STOP_ENABLED: true
MAX_DRAWDOWN: 0.1
MAX_DAILY_LOSS: 0.05

# Logging Configuration
LOG_LEVEL: "INFO"
LOG_FILE: "logs/binance_bot.log"
LOG_UPLOAD_ENABLED: true

# Backup Configuration
BACKUP_ENABLED: true
BACKUP_INTERVAL: 3600
BACKUP_PATH: "./backups/"

# Environment-specific overrides
development:
  BINANCE_TESTNET: true
  TRADING_ENABLED: false
  LOG_LEVEL: "DEBUG"

production:
  BINANCE_TESTNET: false
  TRADING_ENABLED: true
  LOG_LEVEL: "INFO"

# Feature flags
features:
  advanced_analytics: true
  realtime_monitoring: true
  automated_backups: true
  cost_optimization: true
  security_scanning: true

security:
  encryption_key: "16008c892c29749c2ddc7bfd64a321d5676389eab11dc0c39055e699ded41f8060"
  api_key_iv: "35f0a2224d6e8bf482591c0380b3c4d0"

### [FILE] config/config.yaml

mode: "advanced"
free_tier: true

# ADVANCED FEATURES ENABLED
advanced_features:
  quantum_overlays: true
  reinforcement_learning: true  
  advanced_ml_predictions: true
  triangular_arbitrage: true
  neural_precision: true
  adaptive_ml: true

security:
  enabled: true
  max_memory_mb: 800
  max_queue_size: 1000
  api_key_mask: true
  input_validation: true
  secure_serialization: true
  
monitoring:
  security_events: true
  anomaly_detection: true
  memory_guard: true

aws_free_tier:
  enabled: true
  region: "ap-southeast-1"
  resource_limits:
    memory_mb: 900
    cpu_percent: 15
    storage_mb: 30
    monthly_bandwidth_gb: 1

binance:
  api_key: ${BINANCE_API_KEY}
  api_secret: ${BINANCE_API_SECRET}
  testnet: false
  rate_limit: 2

trading:
  quote_currency: USDC
  symbols: ["BTCUSDC", "ETHUSDC", "BNBUSDC", "SOLUSDC", "XRPUSDC", "ADAUSDC", "DOGEUSDC", "DOTUSDC"]
  live_trading: true
  min_order_size: 10
  max_order_size: 20000
  position_size_percent: 0.20  # 20% as per your projection
  max_position_size: 20000
  fee: 0.0015
  test_mode: false
  dry_run: false

risk_management:
  max_risk_per_trade: 0.15
  min_profit_threshold: 0.003
  daily_loss_limit: -40

performance:
  polling_interval: 2.0  # Faster polling for arbitrage
  min_active_pairs: 2
  max_active_pairs: 8
  cache_size: 100
  jit_prewarm: true

# ADVANCED ML CONFIGURATION
ml_advanced:
  neural_precision:
    enabled: true
    hidden_layers: [128, 64, 32]
    dropout_rate: 0.2
    learning_rate: 0.001
    
  reinforcement_learning:
    enabled: true
    state_size: 50
    action_space: 10
    memory_size: 10000
    batch_size: 32
    
  quantum_overlays:
    enabled: true
    prediction_horizon: 5
    confidence_threshold: 0.7

arbitrage:
  triangular_enabled: true
  min_profit_threshold: 0.004  # 0.4% as per 2025 conditions
  max_slippage: 0.001
  simultaneous_trades: 3

schedule:
  enabled: true
  # FIXED: Updated trading_hours to match aws_schedule operation_windows in Singapore time
  trading_hours:
    - start: 09:00  # Singapore time (UTC+8) - matches operation_windows
      end: 12:00
    - start: 14:00  # Singapore time (UTC+8) - matches operation_windows
      end: 17:00
  check_interval: 300

# PRESERVED EXISTING SECTIONS - NO MODIFICATIONS
# Only fixing schedule consistency

# NEW: Added for AWS Free Tier schedule management (additions only)
aws_schedule:
  daily_run_hours: 6
  enabled: true
  operation_windows:
    - start: "09:00"  # Singapore time (UTC+8) - matches your 02:00 UTC
      end: "12:00"
    - start: "14:00"  # Singapore time (UTC+8) - matches your 06:00 UTC  
      end: "17:00"

# NEW: Added for free tier resource monitoring (additions only)
free_tier_limits:
  monthly_hours: 750
  daily_budget_hours: 6
  cpu_credits: 144
  data_transfer_gb: 100

# NEW: Added for Binance API rate limiting (additions only)
api_limits:
  requests_per_minute: 1100
  orders_per_second: 8
  weight_limit: 1150

# PRESERVE ALL EXISTING COMPATIBILITY SETTINGS
compatibility:
  preserve_existing: true
  legacy_support: true
  migration_mode: false

# NEW: Added schedule consistency validation (additions only)
schedule_consistency:
  enabled: true
  primary_timezone: "Asia/Singapore"
  validate_on_startup: true
  auto_correct_mismatches: true

load_testing:
  enabled: true
  traffic_multiplier: 10
  duration_hours: 24
  max_concurrent_connections: 1000
  resource_monitoring_interval: 30

# NEW: Enhanced rate limiter configuration for state persistence
rate_limiter:
  state_persistence: true
  state_file: "rate_limiter_state.json"
  auto_recovery: true
  max_retry_attempts: 5
  recovery_delay: 30

# NEW: Clock skew protection configuration
clock_skew:
  auto_sync: true
  sync_interval: 3600  # 1 hour
  max_allowed_skew: 5000  # 5 seconds in milliseconds
  fallback_to_local: true

# NEW: Enhanced monitoring for AWS Free Tier
aws_enhanced_monitoring:
  cpu_credit_balance: true
  network_throughput: true
  disk_iops: true
  memory_utilization: true
  alert_threshold: 80

# NEW: Binance API optimization for USDC pairs
binance_optimization:
  usdc_markets: true
  fee_optimization: true  # Without BNB fee discount
  recv_window: 5000
  testnet_first: false

# NEW: Performance optimization for t2.micro
performance_optimization:
  max_memory_usage: 85%
  cpu_throttle_prevention: true
  background_cleanup: true
  cache_optimization: true
  connection_pooling: true

### [FILE] binance_api/websocket.py

import asyncio
import websockets
import json
import logging
from typing import Callable, List
from collections import defaultdict

class BinanceWebSocket:
    def __init__(self, symbols: List[str], on_message_callback: Callable, testnet: bool = False):
        self.symbols = [s.lower() for s in symbols]
        self.on_message_callback = on_message_callback
        self.base_url = "wss://testnet.binance.vision" if testnet else "wss://stream.binance.com:9443"
        self.order_books = defaultdict(dict)
        self._connect_task = None
        self._running = False
        self._logger = logging.getLogger('websocket')
        self._logger.setLevel(logging.INFO)

    def _build_stream_url(self):
        # Only subscribe to top 6 symbols instead of all 30 for free tier
        top_symbols = self.symbols[:6]
        streams = [f"{symbol}@bookTicker" for symbol in top_symbols]
        return f"{self.base_url}/stream?streams={'/'.join(streams)}"

    async def _connect(self):
        url = self._build_stream_url()
        
        while self._running:
            try:
                async with websockets.connect(url, ping_interval=30) as ws:
                    self._logger.info(f"Connected to WebSocket: {url}")
                    
                    async for message in ws:
                        if not self._running:
                            break
                            
                        try:
                            data = json.loads(message)
                            if 'stream' in data and 'data' in data:
                                stream_name = data['stream']
                                symbol = stream_name.split('@')[0].upper()
                                self.order_books[symbol] = data['data']
                                await self.on_message_callback(data['data'])
                        except json.JSONDecodeError as e:
                            self._logger.error(f"Failed to parse message: {e}")
                        except Exception as e:
                            self._logger.error(f"Error processing message: {e}")
                            
            except Exception as e:
                self._logger.error(f"WebSocket error: {e}, reconnecting in 5 seconds...")
                await asyncio.sleep(5)

    def start(self):
        self._running = True
        self._connect_task = asyncio.create_task(self._connect())

    def stop(self):
        self._running = False
        if self._connect_task:
            self._connect_task.cancel()

    def get_order_book(self, symbol: str):
        return self.order_books.get(symbol.upper())


### [FILE] binance_api/utils.py
import time
import hashlib
import hmac
import requests

def sign_request(secret, query_string):
    return hmac.new(secret.encode(), query_string.encode(), hashlib.sha256).hexdigest()

def retry_request(func, retries=3, delay=1):
    for attempt in range(retries):
        try:
            return func()
        except Exception as e:
            print(f"[RETRY] Attempt {attempt+1} failed: {e}")
            time.sleep(delay)
    raise Exception("[RETRY] All attempts failed.")

